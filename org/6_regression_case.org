#+TITLE: Multiple linear regression - Case study
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: Case Study - Predicting medical expenses
#+STARTUP: overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* README
#+attr_latex: :width 400px
#+caption: Bed-ridden wounded, knitting (1918-19), US Nat'l Archives
[[../img/6_hospital.jpg]]

- This lecture and practice follows the case developed by Lantz' book
  Machine Learning with R , 3rd edition (2019). In the updated 4th
  edition (2023), this case has been exchanged by an automobile
  industry case.

- To code along with the lecture, download ~6_regression_practice.org~
  from GitHub, complete the file and upload it to Canvas by the
  deadline.

* Rationale
#+attr_latex: :width 400px
#+caption: Source: Peter G Peterson foundation (01/30/2023)
[[../img/6_medical.jpg]] [[../img/6_cost.jpg]]

#+attr_latex: :width 400px
#+caption: Source: US Health Insurance Industry Analysis report (NAIC)
[[../img/6_profits.png]]

- Health insurance companies only make money if they collect more in
  fees than they spend on medical care to its beneficiaries.

- What do you think are profit margins in other industries?
  #+begin_quote
  Profit margins in other industries (sources below):
  - aerospace (2022: 8.28%)
  - retail (Amazon 2022: 43%)
  - cars (2020: 7.5%)
  - pharma (2023: 71%)
  #+end_quote

- Medical expenses are difficult to estimate because the conditions
  that are the most costly to treat are rare and seem random.

- Analysis goal: use patient data to forecast average medical expense
  for at-risk segments of the population (like smokers or obese).

- Image source: [[https://content.naic.org/sites/default/files/2021-Annual-Health-Insurance-Industry-Analysis-Report.pdf][US Health Insurance Industry Analysis Report 2021]]

* ML workflow
#+attr_latex: :width 400px
#+caption: Source: blog.bigml.com (2019)
[[../img/6_workflow.png]]

1) Collecting data: US Census bureau (modified)
2) Exploring the data: correlation matrix and scatterplot matrix
3) Training a linear model on the data with ~lm~
4) Evaluating model performance with ~predict~
5) Improving model performance: nonlinear effects/transformation

* Getting the data
#+attr_latex: :width 400px
#+caption: US Census Bureau HQ in Maryland
[[../img/6_census.jpg]]

- Fun fact: the firm that designed the USCB HQ also designed the Burj
  Khalifa (Dubai), the Sears Tower (Chicago) and One World Trade
  Center (NYC)

- The dataset contains 1,338 examples of beneficiaries enrolled in an
  insurance plan with patient features and total medical expenses
  charged to the insurance plan for the calendar year:
  1) ~age~: An integer indicating the age of the primary beneficiary
     (excluding those above 64 years, as they are generally covered by
     the government).
  2) ~sex~: The policy holder's gender: either ~male~ or ~female~.
  3) ~bmi~: The body mass index (BMI), which provides a sense of how over
     or underweight a person is relative to their height. BMI is equal
     to weight (in kilograms) divided by height (in meters) squared. An
     ideal BMI is within the range of 18.5 to 24.9.
  4) ~children~: An integer indicating the number of children/dependents
     covered by the insurance plan.
  5) ~smoker~: A "yes" or "no" categorical variable that indicates
     whether the insured regularly smokes tobacco.
  6) ~region~: The beneficiary's place of residence in the US, divided
     into four geographic regions: ~northeast~, ~southeast~, ~southwest~, or
     ~northwest~.

- Import the data from ~insurance.csv~ after checking the file online in
  GitHub: [[https://bit.ly/ml_insurance][bit.ly/ml_insurance]].

- You can check the dataset in Emacs with ~M-x eww~ followed by the
  URL. eww is the Emacs World-wide Web browser (good on text/images).

- You can write the text file right away with ~C-x C-w~ to
  ~insurance.csv~!

- Try ~google.com~ in eww.

- Import the data with ~read.csv~ and save them to ~insurance~:
  #+begin_src R :results silent
    insurance <- read.csv("../data/insurance.csv")
  #+end_src

* Exploring the data: variables and distribution

- Exploring the data follows the old adage: data structure,
  statistical summary, overview visualization (numeric data),
  frequency check (categorical data).

- But this exploration is not an activity for its own sake: especially
  in the case of linear regression we need to check if the data
  conform to the minimum criteria (or else we can stop):
  1) *missing* data? (We may have to get a different sample)
  2) *categorical* features? (We may have to transform the data)
  3) *linearity* a reasonable assumption? (May have to resample/rescale)

- Display the dataframe structure:
  #+begin_src R
    str(insurance)
  #+end_src

  #+RESULTS:
  : 'data.frame':       1338 obs. of  7 variables:
  :  $ age     : int  19 18 28 33 32 31 46 37 37 60 ...
  :  $ sex     : chr  "female" "male" "male" "male" ...
  :  $ bmi     : num  27.9 33.8 33 22.7 28.9 25.7 33.4 27.7 29.8 25.8 ...
  :  $ children: int  0 1 3 0 0 0 1 3 2 0 ...
  :  $ smoker  : chr  "yes" "no" "no" "no" ...
  :  $ region  : chr  "southwest" "southeast" "southeast" "northwest" ...
  :  $ expenses: num  16885 1726 4449 21984 3867 ...

- What is the model's dependent variable?
  #+begin_quote
  Answer: ~insurance$expenses~, which measure the medical costs each
  person charged to the insurance plan for the year, and which the
  insurance company wants to minimize.
  #+end_quote

- Linear regression does not require a normally distributed dependent
  variable but the model often fits better when this is true (why?[fn:1])

- To check distribution qualities quickly, we can summarize the stats:
  #+begin_src R
    summary(insurance$expenses)
    summary(insurance)
  #+end_src

  #+RESULTS:
  #+begin_example
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
     1122    4740    9382   13270   16640   63770
        age            sex                 bmi           children
   Min.   :18.00   Length:1338        Min.   :16.00   Min.   :0.000
   1st Qu.:27.00   Class :character   1st Qu.:26.30   1st Qu.:0.000
   Median :39.00   Mode  :character   Median :30.40   Median :1.000
   Mean   :39.21                      Mean   :30.67   Mean   :1.095
   3rd Qu.:51.00                      3rd Qu.:34.70   3rd Qu.:2.000
   Max.   :64.00                      Max.   :53.10   Max.   :5.000
      smoker             region             expenses
   Length:1338        Length:1338        Min.   : 1122
   Class :character   Class :character   1st Qu.: 4740
   Mode  :character   Mode  :character   Median : 9382
                                         Mean   :13270
                                         3rd Qu.:16640
                                         Max.   :63770
  #+end_example

- What do you observe?
  #+begin_quote
  1) The mean is greater than the median (the middle magnitude is left
     of the average), which means the distribution is *right*
     skewed[fn:2].
  2) The spread is significant (minimum vs. maximum values).
  #+end_quote

- We visualize the distribution (what's the best graph for that?):
  #+begin_src R :results graphics file :file ../img/6_hist.png
    ## Visualize numerical distributions = frequencies with a histogram
    hist(insurance$expenses)
  #+end_src

  #+RESULTS:
  [[file:../img/6_hist.png]]

- The graph shows that the majority of people have annual medical
  expenses below US$15,000. Knowing the graphs structural weakness
  ahead of time will help us improve the linear model later on.

* Exploring the data: correlation matrix

- The *correlation matrix* gives an overview of how the variables relate
  to one another: given a set of variables, it provides a correlation
  for each pairwise relationship.

- To create a correlation matrix, use the ~cor~ command - take a look at
  its arguments first:
  #+begin_src R
    args(cor)
  #+end_src

  #+RESULTS:
  : function (x, y = NULL, use = "everything", method = c("pearson",
  :     "kendall", "spearman"))
  : NULL

- Let's build this up slowly: the default for ~y~ is only relevant if ~x~
  is a matrix: how is the dependent variable correlated *with itself*?
  #+begin_src R
    ## Just the dependent variable - formatted as matrix
    x <- as.matrix(insurance$expenses)
    head(x)
    cor(x)
  #+end_src

  #+RESULTS:
  :          [,1]
  : [1,] 16884.92
  : [2,]  1725.55
  : [3,]  4449.46
  : [4,] 21984.47
  : [5,]  3866.86
  : [6,]  3756.62
  :      [,1]
  : [1,]    1

- This makes sense because:
  #+begin_src R
    var(x,x)/(sd(x)*sd(x)) ## sd^2 = var
  #+end_src

- Now for all ~numeric~ variables:
  #+begin_src R
    str(insurance)
    ins_num <- c("age","bmi","children","expenses")
    cor(insurance[ins_num]) # only numerical features
  #+end_src

  #+RESULTS:
  #+begin_example
  'data.frame': 1338 obs. of  7 variables:
   $ age     : int  19 18 28 33 32 31 46 37 37 60 ...
   $ sex     : chr  "female" "male" "male" "male" ...
   $ bmi     : num  27.9 33.8 33 22.7 28.9 25.7 33.4 27.7 29.8 25.8 ...
   $ children: int  0 1 3 0 0 0 1 3 2 0 ...
   $ smoker  : chr  "yes" "no" "no" "no" ...
   $ region  : chr  "southwest" "southeast" "southeast" "northwest" ...
   $ expenses: num  16885 1726 4449 21984 3867 ...
                 age        bmi   children   expenses
  age      1.0000000 0.10934101 0.04246900 0.29900819
  bmi      0.1093410 1.00000000 0.01264471 0.19857626
  children 0.0424690 0.01264471 1.00000000 0.06799823
  expenses 0.2990082 0.19857626 0.06799823 1.00000000
  #+end_example

- What do we learn?
  #+begin_quote
  1) the diagonal of the correlation matrix is always 1 (a variable
     is always perfectly correlated with itself: ~cor(x,x) = 1~).
  2) the matrix transpose is identical to itself (correlation is
     symmetrical: ~cor(x,y) = cor(y,x)~).
  3) None of the correlations is strong (i.e. we need them all).
  4) ~age~ and ~bmi~ are weakly positively correlated: as you age, your
     BMI slightly increases.
  5) Expenses go up with age, body mass, and number of children.
  #+end_quote

* Exploring the data: scatterplot matrix

- A /scatterplot matrix/ or /pair plot/ shows the relationship of each
  variable with every other as a graph.

- You can feed the whole dataframe into the generic ~plot~ function:
  #+begin_src R :results graphics file :file ../img/6_plot.png
    plot(insurance)
  #+end_src

  #+RESULTS:
  [[file:../img/6_plot.png]]

- However, ~plot~ does not distinguish between numeric and categorical
  variables, and a scatterplot is meaningless for the latter.

- An alternative is ~graphics::pairs~[fn:3]:
  #+begin_src R :results graphics file :file ../img/6_pairs.png
    pairs(insurance[ins_num]) ## ins_num <- c("age","children","bmi","expenses")
  #+end_src

  #+RESULTS:
  [[file:../img/6_pairs.png]]

- The intersection of each row and column holds the scatterplot of the
  variables indicated by the row and column pair: e.g. the plot in the
  2nd row and 2nd column shows ~age ~ bmi~ or "age" as a function of
  "bmi" - its transpose value shows ~bmi ~ age~.

- Do you notice any patterns in these plots?
  #+begin_quote
  1) Visible nearly straight lines in ~age ~ expenses~
  2) Two point clusters in ~bmi ~ expenses~
  3) Invisible structure in the ~age ~ bmi~ plot
  #+end_quote

- The ~pairs.panels~ function in the ~psych~ package contains more
  information:
  #+begin_src R :results graphics file :file ../img/pairs_panels.png
    library(psych)
    pairs.panels(insurance[ins_num])
  #+end_src

  #+RESULTS:
  [[file:../img/pairs_panels.png]]

- What do you see?
  #+begin_quote
  1) The scatterplots above the diagonal are now a correlation matrix
  2) The diagonal shows histograms for the feature distributions with
     a density estimate (smoothing) to more clearly show profile.
  3) Each scatterplot shows a /correlation ellipse/ indicating spread:
     the more it is stretched, the stronger the correlation -
     e.g. ~children ~ bmi~ is almost round indicating that the number of
     children is largely independent of the BMI (and vice versa) = 0.01.
  4) The correlation ellipse for ~expenses ~ age~ is much more
     stretched: these features are more correlated = 0.30.
  5) The red dot at the center of the ellipsis is the mean value.
  6) The red curve drawn on the scatterplot is a ~loess curve~: the
     curves for ~children ~ age~ peaks around middle age: the oldest and
     youngest people in the sample have fewer children.
  #+end_quote

- The ~age ~ children~ trend is non-linear and cannot be seen in the
  correlations! (Unlike e.g. the ~age ~ bmi~ loess curve.)

* IN PROGRESS Training a model on the data

      - We use the generic ~lm~ function from ~stats~- check arguments:
        #+begin_src R
          args(lm)
          environment(lm)
        #+end_src

        #+RESULTS:
        : function (formula, data, subset, weights, na.action, method = "qr",
        :     model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,
        :     contrasts = NULL, offset, ...)
        : NULL
        : <environment: namespace:stats>

      - Here's a syntax overview (Lantz, 2019):
        #+attr_latex: :width 400px
        [[../img/6_lm.png]]

      - Uses the "formula" syntax - the independent variables can *all* be
        included with the ~.~ operator: ~lm(dep ~ ., data)~ or individually with
        the ~+~ operator.

      - Just like seen in the ~glm~ example (logistic regression), you can
        include /interactions/ between independent variables with the ~*~
        operator to model the combined effect of two or more features.

      - The following model relates the six independent variables to the
        total medical ~expenses~:
        #+name: insurance_model
        #+begin_src R :results silent
          ins_model <- lm(expenses ~ . ,data = insurance)
        #+end_src

      - To see the estimated \beta coefficients, print the model:
        #+begin_src R
          ins_model
        #+end_src

        #+RESULTS:
        :
        : Call:
        : lm(formula = expenses ~ ., data = insurance)
        :
        : Coefficients:
        :     (Intercept)              age          sexmale              bmi         children
        :        -11941.6            256.8           -131.4            339.3            475.7
        :       smokeryes  regionnorthwest  regionsoutheast  regionsouthwest
        :         23847.5           -352.8          -1035.6           -959.3

      - The ~Intercept~ is the predicted value when the independent variables
        are zero (not realistic since living persons have BMI > 0, age > 0).

      - The \beta coefficients indicate the estimated increase (slope) in
        expenses for an increase of one unit in each of the features,
        assuming all other values are held /constant/.

      - For example: for each additional year of ~age~, we expect an average
        of ~256.8~ expense increase per year.

      - The ~lm~ function automatically dummy-codes each ~factor~ type variable
        included, like ~sex~, ~smoker~ and ~region~ (split in four dummy variables).

      - When adding dummy variables, one category is always left out as a
        reference category (e.g. ~sex=female~, ~region=northeast~): e.g. males
        have ~$131.4~ less medical expenses than females per year relatives to
        females[fn:4].

      - Which ~region~ has the highest medical expenses?
        #+begin_quote
        The reference group - ~northeast~, because all other values are negative.
        #+end_quote

      - In summary: old age, smoking and obesity can be linked to additional
        health issues, and additional family members may result in an
        increase. But how well is this model fitting the data?

* Evaluating model performance

- Why don't we use a confusion matrix?
  #+begin_quote
  Answer: the confusion matrix is for classification of categorical
  variables, not continuous numeric variables.
  #+end_quote

- To evaluate model performance, we can use ~summary~:
  #+begin_src R
    summary(ins_model)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = expenses ~ ., data = insurance)

  Residuals:
       Min       1Q   Median       3Q      Max
  -11302.7  -2850.9   -979.6   1383.9  29981.7

  Coefficients:
                  Estimate Std. Error t value Pr(>|t|)
  (Intercept)     -11941.6      987.8 -12.089  < 2e-16 ***
  age                256.8       11.9  21.586  < 2e-16 ***
  sexmale           -131.3      332.9  -0.395 0.693255
  bmi                339.3       28.6  11.864  < 2e-16 ***
  children           475.7      137.8   3.452 0.000574 ***
  smokeryes        23847.5      413.1  57.723  < 2e-16 ***
  regionnorthwest   -352.8      476.3  -0.741 0.458976
  regionsoutheast  -1035.6      478.7  -2.163 0.030685 *
  regionsouthwest   -959.3      477.9  -2.007 0.044921 *
  ---
  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

  Residual standard error: 6062 on 1329 degrees of freedom
  Multiple R-squared:  0.7509,  Adjusted R-squared:  0.7494
  F-statistic: 500.9 on 8 and 1329 DF,  p-value: < 2.2e-16
  #+end_example

- The /summary/ explained:
  #+attr_latex: :width 400px
  #+caption: Evaluation of the regression model with summary()
  [[../img/6_summary.png]]
  1) The *Residuals* give summary statistics: a residual is the true
     value minus the predicted value, the maximum error ~29981.7~
     suggests that the model underperformed and under-predicted
     expenses by $30,000 for at least one observation.

     50% of all errors fall between the 3rd and the 1st quartile,
     i.e. the majority of the predictions were between $2,850 over and
     $1,380 under the true value.

  2) For each coefficient, the ~p-value~ in the last column estimates
     statistical significance: small values suggest that the
     coeffcient is very unlikely to be zero (feature is related to the
     dependent variable). The stars ~***~ represent the significance
     level set beforehand. Few such terms would be cause for concern:
     the features wouldn't be very predictive of the outcome.

  3) The /multiple R-squared/ value (also called 'coefficient of
     determination') is a measure of how well the model as a whole
     explains the values of the dependent variable: the closer to 1
     the better. A value of 0.75 means that the model explains 75% of
     the observed variation in the dependent variable.[fn:5]

- Given these three performance indicators - residual error, p-value
  and multiple R-squared value - the model performs fairly well. The
  large error maximum is worrying but consistent with what we know of
  medical expense data.

* Excursion: z value and Pr(>|z|)

- The z value is the number of standard deviations a value is away
  from the mean.

- The ~Pr(>|z|)~ column represents the /p-value/ associated with the value
  in the z column.

- If the p-value is less than a certain significance level (for
  example \alpha = 0.05), then this indicates that the predictor has a
  statistically significant relationship with the response variable in
  the model.

- /Statistical significance/ means that a prediction is not the result
  of chance but can instead be attributed to a specific cause.

- There are several statistical tests - the t-test compares sample
  means by calculating the t-value (x_{sample}- \mu)/(\sigma/\radic_{n})):
  t-value above the critical line, the sample mean is too far from the
  population mean (and the sample does not model the population):
  #+attr_latex: :width 400px
  #+caption: t-distribution with critical value for probability mass
  [[../img/6_t_test.png]]

- Another way of saying it: \alpha is the probability of the prediction
  rejecting the null hypothesis, and the p-value of a result is the
  probability of obtaining a result at least as extreme, given that
  the null hypothesis is true.

- The /null hypothesis/ is true if there is no relationship between the
  predictor and the target variable, i.e. changes in the predictors
  only lead to random changes in the target variable but not because
  the two are meaningfully correlated.

- So the first thing to do when discovering a correlation is to check
  statistical significance to make sure that the discovery is not the
  result of random fluctuations in the sample.

- The \alpha must be set before evaluation - if it is tampered with when
  the result does not satisfy one's prejudices, this is called
  "p-hacking", which is very widespread e.g. in clinical trials (Adda
  et al, 2020): insights are presented as statistically significant
  even though they're not.

- There is a kind of confusion matrix here, too:
  #+attr_latex: :width 400px
  #+caption: Type I and Type II statistical errors
  [[../img/dc_type.png]]
  1) Type I errors are false positives
  2) Type II errors are false negatives

- What would be the null hypothesis for our prediction of insurance
  expenses?
  #+begin_quote
  - This question only makes sense with regard to a particular
    feature - e.g. our null hypothesis could be "smoking does not lead
    to increased medical expenses."
  - Type I error: we find "Smoking increases expenses" (while it
    actually does not).
  - Type II error: we find "smoking does NOT increase expenses" (while
    it actually does).
  #+end_quote

* Improving model performance

Regression typically leaves feature selection to the user - *subject
matter knowledge* (on how a feature is related to the outcome) is
important! We explore three alterations of the model:

- Adding non-linear relationships among independent variables
- Transform numeric independent variables to binary indicators
- Adding interaction effects between independent variables

** Adding non-linear relationships

- To account for a non-linear relationship, we can add a higher order
  term treating the model as a polynomial:
  #+attr_latex: :width 200px
  #+caption: Adding a higher order term to the regression equation
  [[../img/6_poly.png]]

- The additional \beta coefficient will capture the effect of the x^{2} term.

- Looking at the /loess curve/ (in the scatterplot matrix) which
  revealed non-linearity, ~age~ might be a good candidate for a
  nonlinear term. It is also already the strongest correlated
  independent variable so that its correction will be felt more
  strongly.

- Does this make sense? Consider the health pattern of aging: at the
  beginning and at the end of life, the need for (expensive) medical
  attention is greatest (if there's any need at all).

- In R, we simply create a new variable ~age2~ - this will add a feature
  vector and another column to our \beta coefficients matrix:
  #+begin_src R
    insurance$age2 <- insurance$age^2
    str(insurance)
  #+end_src

  #+RESULTS:
  : 'data.frame':       1338 obs. of  8 variables:
  :  $ age     : int  19 18 28 33 32 31 46 37 37 60 ...
  :  $ sex     : chr  "female" "male" "male" "male" ...
  :  $ bmi     : num  27.9 33.8 33 22.7 28.9 25.7 33.4 27.7 29.8 25.8 ...
  :  $ children: int  0 1 3 0 0 0 1 3 2 0 ...
  :  $ smoker  : chr  "yes" "no" "no" "no" ...
  :  $ region  : chr  "southwest" "southeast" "southeast" "northwest" ...
  :  $ expenses: num  16885 1726 4449 21984 3867 ...
  :  $ age2    : num  361 324 784 1089 1024 ...

- When we build the model, we add both age variables to the formula,
  as in ~expenses ~ age + age2~, allowing ~lm~ to separate the terms.

** Converting numeric variable to binary indicator

- If a feature is not /cumulative/ but rather has has an effect only
  above a certain /threshold/, we would want this dynamical behavior to
  be modeled.

- BMI is such an /indicator/ variable: it is 1 (impactful) if the BMI is
  at least 30, and 0 (neglectable) if the BMI is less than 30. The
  associated \beta indicates the average net impact on expenses for
  individuals with ~bmi >= 30~ relative to those with ~bmi < 30~.

- R's ~ifelse~ function tests a condition for each element in a vector
  and returns a value accordingly - we use this to add another
  variable:
  #+begin_src R
    insurance$bmi30 <- ifelse( test = (insurance$bmi >= 30), ## test condition
                              yes  = 1,                      ## BMI >= 30
                              no   = 0)                      ## BMI <  30
    str(insurance)
  #+end_src

  #+RESULTS:
  #+begin_example
  'data.frame': 1338 obs. of  9 variables:
   $ age     : int  19 18 28 33 32 31 46 37 37 60 ...
   $ sex     : chr  "female" "male" "male" "male" ...
   $ bmi     : num  27.9 33.8 33 22.7 28.9 25.7 33.4 27.7 29.8 25.8 ...
   $ children: int  0 1 3 0 0 0 1 3 2 0 ...
   $ smoker  : chr  "yes" "no" "no" "no" ...
   $ region  : chr  "southwest" "southeast" "southeast" "northwest" ...
   $ expenses: num  16885 1726 4449 21984 3867 ...
   $ age2    : num  361 324 784 1089 1024 ...
   $ bmi30   : num  0 1 1 0 0 0 1 0 0 0 ...
  #+end_example

- In the ~insurance~ model, you can either replace the original ~bmi~ or
  add it depending on the type of impact of BMI:
  1) does the effect add to the impact of BMI?
  2) does the effect replace the impact of BMI?

- What is further statistical evidence if a variable should be
  included or not?
  #+begin_quote
  Examine the p-value - if the variable is not /statistically
  significant/ (p < \alpha), you can drop it without stochastic impact.
  #+end_quote

- A quick check with ~bmi30~ shows that the p-value is still solid:
  #+begin_src R
    ins_model_bmi30 <- lm(expenses ~ age + sex + bmi + bmi30 +
                            children + smoker + region, data=insurance)
    summary(ins_model_bmi30)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = expenses ~ age + sex + bmi + bmi30 + children + 
      smoker + region, data = insurance)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -11943.6  -3430.1   -100.1   1543.8  28486.0 

  Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
  (Intercept)     -7657.59    1279.44  -5.985 2.78e-09 ***
  age               257.19      11.78  21.825  < 2e-16 ***
  sexmale          -161.10     329.78  -0.489 0.625269    
  bmi               149.27      46.26   3.227 0.001282 ** 
  bmi30            2852.84     549.11   5.195 2.36e-07 ***
  children          477.81     136.47   3.501 0.000479 ***
  smokeryes       23846.74     409.16  58.283  < 2e-16 ***
  regionnorthwest  -388.38     471.72  -0.823 0.410472    
  regionsoutheast  -885.48     474.95  -1.864 0.062488 .  
  regionsouthwest  -949.21     473.31  -2.005 0.045116 *  
  ---
  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

  Residual standard error: 6003 on 1328 degrees of freedom
  Multiple R-squared:  0.7559,	Adjusted R-squared:  0.7542 
  F-statistic: 456.9 on 9 and 1328 DF,  p-value: < 2.2e-16
  #+end_example

** Adding interaction effects

- Certain features could have a combined impact on the dependent
  variable - we can model this by their /interaction/.

- Smoking *and* obesity may be harmful separately (~+~) but in combination
  (~*~) they may be even more harmful: "overweight smokers are ill more
  often than slim smokers".

- The ~*~ operator in the formula ~expenses ~ bmi30*smoker~ is shorthand
  for: ~expenses ~ bmi30 + smokeryes + bmi30:smokeryes~ - it includes
  the separate effects plus their interaction (~:~).

- The \beta coefficient of the interaction between ~bmi30~ and ~smokeryes~ is
  the increase of effectiveness of ~bmi30~ for a 1 unit increase in
  ~smokeryes~ and vice versa[fn:6].

** The improved regression model

- We added a nonlinear term ~age2~ for ~age~, we created an indicator
  ~bmi30~ for obesity, and we specified an interaction between obesity
  and smoking (~bmi30*smoker~).

- We train the model as before with ~lm~ and include the new variables
  and the interaction term:
  #+begin_src R :results silent
    ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
                       bmi30*smoker + region, data = insurance)
  #+end_src

- Look at the model:
  #+begin_src R
    ins_model2
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = expenses ~ age + age2 + children + bmi + sex + bmi30 *
      smoker + region, data = insurance)

  Coefficients:
      (Intercept)              age             age2         children              bmi
          139.005          -32.618            3.731          678.602          119.771
          sexmale            bmi30        smokeryes  regionnorthwest  regionsoutheast
         -496.769         -997.935        13404.595         -279.166         -828.035
  regionsouthwest  bmi30:smokeryes
        -1222.162        19810.153
  #+end_example

- Summarize the results:
  #+begin_src R
    summary(ins_model2)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = expenses ~ age + age2 + children + bmi + sex + bmi30 *
      smoker + region, data = insurance)

  Residuals:
       Min       1Q   Median       3Q      Max
  -17297.1  -1656.0  -1262.7   -727.8  24161.6

  Coefficients:
                    Estimate Std. Error t value Pr(>|t|)
  (Intercept)       139.0053  1363.1359   0.102 0.918792
  age               -32.6181    59.8250  -0.545 0.585690
  age2                3.7307     0.7463   4.999 6.54e-07 ***
  children          678.6017   105.8855   6.409 2.03e-10 ***
  bmi               119.7715    34.2796   3.494 0.000492 ***
  sexmale          -496.7690   244.3713  -2.033 0.042267 *
  bmi30            -997.9355   422.9607  -2.359 0.018449 *
  smokeryes       13404.5952   439.9591  30.468  < 2e-16 ***
  regionnorthwest  -279.1661   349.2826  -0.799 0.424285
  regionsoutheast  -828.0345   351.6484  -2.355 0.018682 *
  regionsouthwest -1222.1619   350.5314  -3.487 0.000505 ***
  bmi30:smokeryes 19810.1534   604.6769  32.762  < 2e-16 ***
  ---
  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

  Residual standard error: 4445 on 1326 degrees of freedom
  Multiple R-squared:  0.8664,  Adjusted R-squared:  0.8653
  F-statistic: 781.7 on 11 and 1326 DF,  p-value: < 2.2e-16
  #+end_example

- Interpret the new result:
  #+attr_latex: :width 400px
  #+caption: Summary of the improved medical expenses model
  [[../img/6_model_2.png]]
  1) The model as a whole now explains 87% of the variation in
     medical cost (adjusted R-squared value) - up from 75%.
  2) Higher-order term ~age2~ and obesity indicator ~bmi30~ are
     statistically significant.
  3) The interaction between smoking and obesity suggests a massive
     effect: smoking alone costs over $13,404, and obese smokers
     spend another $19,810 per year.

- Regression modeling makes strong assumptions on the underlying
  data. Before making inferences from the \beta coefficients, you need to
  run diagnostic tests to ensure that these assumptions have not been
  violated.

- These key assumptions are:
  1) multivariate normality (central limit theorem for multiple
     variables)
  2) little multicollinearity (relationship is not perfect)
  3) no autocorrelation (no periodicity in time)
  4) homoscedasticity (homogeneous noise in the relationship between
     independent and dependent variable)

- These assumptions are not relevant for numeric forecasting (model's
  worth is not based on truly capturing the underlying process)
     
* Making predictions with the improved regression model

- We use the model to predict the expenses of future enrollees on the
  health insurance plan.

- Apply the model to the original training data using ~predict~:
  #+begin_src R :results silent
    insurance$pred <- predict(ins_model2, insurance)
  #+end_src

- Compute the correlation between predicted and actual costs:
  #+begin_src R
    cor(insurance$pred,insurance$expenses)
  #+end_src  

  #+RESULTS:
  : [1] 0.9307999

- The model is highly accurate. Let's visualize this using ~plot~:
  #+begin_src R :results graphics file :file ../img/ins_mod2.png
    plot(insurance$pred,insurance$expenses,
         xlab="Predicted costs ($)",
         ylab="Actual expenses ($)",
         main="Medical insurance expenses (N=1338)")
    abline(a = 0, b = 1, col = "red", lwd = 3, lty = 2)
  #+end_src

  #+RESULTS:
  [[file:../img/ins_mod2.png]]

- In the plot, the line y = x shows the points/patients where
  predictions fall very close to the actual values.

- The points above the line are patients whose actual expenses were
  greater than expected, those below the line are those less than
  expected.

- To forecast for a smaller number of enrollees, you can create a
  dataframe on the fly. For example to estimate the insurance expenses
  for a 30-year old, overweight, male non-smoker with two children in
  the Northeast:
  #+begin_src R
    predict(ins_model2,
            data.frame(age = 30, age2 = 30^2, children = 2,
                       bmi = 30, sex = "male", bmi30 = 1,
                       smoker = "no", region = "northeast"))
  #+end_src

  #+RESULTS:
  :        1 
  : 5973.774

- For this beneficiary, the insurance company has to set its prices to
  no less than $6,000 or $500 per month to break even.

- How does this compare to a female with the same characteristics?
  #+begin_src R
    predict(ins_model2,
            data.frame(age = 30, age2 = 30^2, children = 2,
                       bmi = 30, sex = "female", bmi30 = 1,
                       smoker = "no", region = "northeast")) -> pf
    pf
  #+end_src

  #+RESULTS:
  :        1 
  : 6470.543

- The difference between these values is ~-496.769~ the estimated \beta for
  ~sexmale~: all else being equal, males are estimated to have about
  $500 less in expenses for the plan per year, all else being equal.

- The predicted expenses are a sum of each of the \beta values times their
  corresponding prediction settings, e.g. for the number of children
  the \beta is 678.6017. We can predict that reducing the children from 2
  to 0 (childless female), expenses will drop by 2\beta = 1,357.203:
  #+begin_src R
    predict(ins_model2,
            data.frame(age = 30, age2 = 30^2, children = 0,
                       bmi = 30, sex = "female", bmi30 = 1,
                       smoker = "no", region = "northeast")) -> p0
    pf - p0
  #+end_src

  #+RESULTS:
  :        1 
  : 1357.203

- What if you have a lot of kids? Check out the result for 7 children:
  #+begin_src R
    predict(ins_model2,
            data.frame(age = 30, age2 = 30^2, children = 7,
                       bmi = 30, sex = "female", bmi30 = 1,
                       smoker = "no", region = "northeast"))
  #+end_src

  #+RESULTS:
  :        1 
  : 9863.552

- As a mother of 7, you are predicted to incur ca. $3500 more, and not
  (7/2)*$6,400=$22,400, so it's "cheap" to have a large family.

- Following similar steps for a number of additional customer
  segments, the insurance company could develop a profitable pricing
  structure for various demographics.

* TODO Glossary of code

| COMMAND | MEANING |
|---------+---------|
|         |         |

* TODO Summary
* References

- Adda et al (2020). P-hacking in clinical trials and how incentives
  shape the distribution of results across phases. In: Proc Nat Acad
  Sci 117(24):13386-13392. [[https://doi.org/10.1073/pnas.1919906117][URL: doi.org/10.1073/pnas.1919906117]]

- Choueiry (2023). Interpret Interactions in Linear Regression. URL:
  [[https://quantifyinghealth.com/interpret-interactions-in-linear-regression/][quantifyinghealth.com]].

- Data: PacktPublishing (2019). Machine learning with R (3e). URL:
  [[https://github.com/PacktPublishing/Machine-Learning-with-R-Third-Edition/tree/master/Chapter06][github.com]].

- Lantz (2019). Machine learning with R (3e). Packt. URL:
  [[https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864][packtpub.com]].

- R Core Team (2022). R: A language and environment for statistical
  computing. R Foundation for Statistical Computing, Vienna, Austria.
  URL https://www.R-project.org/.

* Footnotes
[fn:6][[https://quantifyinghealth.com/interpret-interactions-in-linear-regression/][Choueiry (2023)]] contains a detailed breakdown of an interaction
term for studying the effect of physical exercise and protein intake
on the amount of muscle the body can build in 1 month.

[fn:5]The Adjusted R-Squared value corrects for models with many features.

[fn:4]In R, the first ~level~ is taken as reference. You can use ~relevel~
to change this.

[fn:3]The result is the same as ~plot(insurance[ins_num])~ but ~pairs~
offers different customization options than the generic ~plot~ - see
~help(pairs)~.

[fn:2]The *skewedness* highlights the opposite of the maximum of the
points - a left/right leaning distribution is skewed to the
right/left, because the outlying points cause the problem in terms of
analysis: they are harder to distinguish and kind of "fall off the
end". Transformations will affect them more strongly.

[fn:1]Normal distribution means that standard stats (mean=expected
value=0, standard deviation=1 etc.) are known, in other words the
distribution is of known spread and centrality. This means we can
compare it better with other distributions (in fact, mapping on a
normal distribution is a way of ensuring comparability), and
deviations stand out more clearly, too.
