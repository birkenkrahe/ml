#+TITLE: Understanding Naive Bayes
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: Supervised Naive Bayes Prediction
#+STARTUP: overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
:REVEAL_PROPERTIES:
#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_INIT_OPTIONS: transition: 'cube'
#+REVEAL_THEME: black
:END:
* Naive Bayes
#+attr_html: :width 300px
[[../img/ThomasBayes.png]]

#+begin_example
- Lecture notes in Markdown file (4_naive_bayes.md)
- Source: Lantz (2019), chapter 4, pp. 89-123
- DataCamp assignment: "Supervised learning with R", ch. 2
#+end_example

* What you will learn
#+attr_html: :width 400px
#+caption: Photo by Dmitry Ratushny on Unsplash
[[../img/learning.jpg]]

- Classification using Naive Bayes
- Bayes' theorem and naive assumptions
- Text classification use case
- R packages for text mining & visualization
- Application: SMS junk message filter

#+begin_notes
A little bit of math in here, but nothing more than basic
arithmetic. The main complexity is to understand the relationship
between features, class, and probability measures, which are our
"similarity" measure for this type of classifier.
#+end_notes

* Probabilistic methods
#+attr_latex: :width 400px
[[../img/6_weather.png]]
#+begin_example
- Probabilistic methods describe uncertainty
- They use data on past events to extrapolate future events
- Such predictions are subject to many assumptions
#+end_example
#+begin_notes
- The chance of rain for example describes the proportion of prior
  days with similar atmospheric conditions in which in rained.
- A 70 percent chance of rain implies that in 7 out of 10 past cases
  with similar conditions, it rained somewhere in the area.
- The Naive Bayes algorithm uses probabilities in a similar way to a
  weather forecast.
#+end_notes

* Probability
#+attr_html: :width 400px
#+caption: Photo by Naser Tamimi on Unsplash
[[../img/prob.jpg]]

#+begin_example
- A probability P is a number between 0 and 1 (0% to 100%)
- P captures the chance that an event will occur based on evidence
- P = 0 indicates that the event will definitely not occur
- P = 1 indicates that the event will occur with absolute certainty
#+end_example

* Bayesian methods
#+attr_html: :width 400px
#+caption: Photo by Markus Spiske on Unsplash
[[../img/bavarian.jpg]]

#+begin_example
- Training data are used to calculate outcome probability
- Evidence is provided by labeled feature values
- Classifier uses calculated probabilities to estimate class
#+end_example

* Applications
#+attr_html: :width 400px
[[../img/ids-in-security.png]]

- Text classification, e.g. spam filter
- Anomaly detection in computer networks
- Diagnosing medical conditions

#+begin_notes
- Best for problems where information from numerous attributes should
  be considered simultaneously to estimate overall probability of an
  outcome.
- E.g. spam filter: various words found in an example/message instance
- Unlike other ML methods, Bayesian methods use all available evidence
  to make predictions.
- Even if a large number of features have minor effects, their
  combined impact in a Bayesian model could have a major impact.
#+end_notes

* Basic idea
#+attr_html: :width 500px
[[../img/6_events.png]]

The estimated likelihood of an *event* or potential outcome is based on
the evidence from multiple *trials* or opportunities for the event to
occur.

#+begin_notes
The more trials the better for the accuracy of the estimate - by way
of the *law of large numbers*: if you repeat an experiment independently
a large number of times and average the result, your result is close to
the expected value (the arithmetic mean):
- Large number of coin flips - P(head) = P(tail) = 50%
- Large number of observed days - weather averages
- Large number of email messages - certain spam prediction
- Large number of elections - certain presidential prediction
- Large number of lottery tickets - certain win

But: real events are never mathematically independent.
#+end_notes

* Spam vs. Ham
#+attr_html: :width 600px
[[../img/6_spam_ham.png]]

In email trials, spam and ham are mutually exclusive and exhaustive
events.

#+begin_notes
- P(event) = no. of occurrences / no. of trials
- Rain on 3/10 days w/similar conditions: 30% prob today
- Adding all P => 100% of the data or \sum P(event)=1 because a trial
  always results in an outcome.
-  P(spam) + P(ham) = 1 implies that spam/ham *mutually exclusive and
  exhaustive*.
- An alternative way of saying this uses a table of records: if you
  record many, many instances, say 1000, you have 200 lines marked as
  'spam' and 800 lines marked as 'ham'.
#+end_notes
* Joint probability
#+attr_html: :width 600px
[[../img/6_joint.png]]

'Viagra' is a non-mutually exclusive event. Its overlap with 'spam'
is large than its overlap with 'ham'.

* Venn diagrams
#+attr_html: :width 600px
[[../img/6_venn.png]]

Calculating P(spam \cap Viagra) depends on the joint probabilities of the
two events, on their *dependency*. 
#+begin_notes
- The Venn diagram illustrates instances that are only spam, only
  Viagra but not spam and spam with Viagra messages.
- Named after 19th century mathematician John Venn
- If the circles aren't touching, the joint prob is 0 and the events
  are said to be *independent*. They can still occur simultaneously.
- A \cap B = 0: Knowing something about the outcome of A reveals nothing
  about the outcome of B. Hard to illustrate in the real world, but:
- The outcome of a coin flip is unlikely to depend on the weather
  being sunny or rainy on any given day.
- *Dependent events are the basis of predictive modeling.*
- The appearance of clouds is predictive of rain, the appearance of
  the word 'Viagra' is predictive of spam.
#+end_notes
* Bayes' theorem
#+attr_html: :width 250px
[[../img/6_bayes_simple.png]]
- For independent events, P(A \cap B) = P(A) * P(B)
- P(Viagra AND spam) = (5/100) * (20/100) = 0.01
- P(A|B) is the probability of A given B occurred
- P(A|B) is the probability of A conditional on B
#+attr_html: :width 400px
[[../img/6_bayes.png]]

#+begin_notes
- Recall: we're trying to predict the chance that a message that
  contains the word 'Viagra' (B) is spam (A).
- The formula states that the best estimate of P(A|B) is the
  proportion of trials in which A occurred with B, P(A \cap B), out of
  all trials in which B occurred (all 'Viagra' messages).
- Extreme cases: if B is very rare, P(B) is small and the correction
  to P(A) is negligible (independence)
- If A and B occur together very often, P(A|B) will be high regardless
  of P(B).
- If Viagra and spam were independent, P(A \cap B) = 0.05 * 0.20 = 0.01
#+end_notes
* Bayesian spam filter
#+attr_html: :width 600px
[[../img/6_bayes_spam_ham.png]]

To calculate the components, construct a frequency table that records
how often 'Viagra' appeared in 'spam' and 'ham' messages.
#+attr_html: :width 600px
[[../img/6_frequency.png]]
#+begin_notes
- Without knowing anything about an incoming messages, our best
  estimate would be P(spam) - the *prior probability* (20%)
- The chance of having any 'Viagra' in a spam message is the *marginal
  likelihood* - having any 'Viagra' at all is the *marginal likelihood*
- What we're after is a computation of the *posterior probability*
  (i.e. *after* applying the condition 'Viagra').
#+end_notes
* Likelihood table
#+attr_html: :width 600px
[[../img/6_likelihood.png]]

The rows of the likelihood table contain the conditional probabilities
for "Viagra" (yes/no) given that an email was spam or ham:
#+begin_example
P(Viagra = Yes | spam) = 4/20 = 0.20
P(spam & Viagra) = P(Viagra|spam) * P(spam) = (4/20)*(20/100) = 0.04
P(spam|Viagra) = (4/20) * (20/100) / (5/100) = 0.80
#+end_example
#+begin_notes
- The computed chance of getting spam AND Viagra is FOUR times as
  large as the chance when independence was assumed
  (P(Viagra)*P(spam)=0.01)
- The posterior probability that a message containing Viagra is spam
  is 80% - any message containing this term should be filtered.
- This is how commercial spam filters work: they consider a much
  larger number of words simultaneously when computing frequency and
  likelihood tables.
- The Naive Bayes algorithm accounts for these additional
  difficulties. It also relies on careful text pre-processing of the
  message data.
#+end_notes

* References

- Lantz (2019). Machine Learning with R (3e). Packt.

- Majka M (2019). naivebayes: High Performance Implementation of the
  Naive Bayes Algorithm in R. R package version 0.9.7,
  https://CRAN.R-project.org/package=naivebayes.

