#+TITLE: AGENDA - MACHINE LEARNING
#+AUTHOR: MARCUS BIRKENKRAHE (pledged)
#+SUBTITLE: Machine Learning (DSC 305) Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* Week 1: Course overview & ML overview
#+attr_latex: :width 500px
[[../img/cover.jpg]]

[[https://unsplash.com/@kasiape][Photo by Katarzyna Pe at Unsplash.com]]

- [X] Course overview - assignments, grading, topics, platforms
- [X] Interesting article: "[[https://blog.replit.com/fourth][The Fourth Age of Programming]]"
- [X] Interesting webinar: "[[https://www.datacamp.com/webinars/2023-data-trends-and-predictions][Data Trends & Predictions 2023]]" report
  ([[https://github.com/birkenkrahe/ds2/blob/main/pdf/data_trends_2023.pdf][PDF]]) (Wed Jan 18, 10AM) - register/attend/watch the recording later
- [ ] Introduction to machine learning
- [X] First DataCamp assignment "Understanding machine learning"

* Week 2: Introduction to ML & Review of R (Part I)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] Introduction to machine learning (continued)
- [X] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on +Thursday, Jan-19+ Jan-24
- [X] First DataCamp home assignment "What is Machine Learning" Jan-19
- [X] First in-class practice "Introduction to machine learning"

Recommendations:
- [X] "Lyon Data & Analytics Nexus" on Discord (invitation only)
- [X] [[https://youtu.be/BRiNw490Eq0][Google search operators tutorial]] (video, 1 hr) => project
- [X] [[https://youtu.be/i_LwzRVP7bg][ML for everybody by FreeCodeCamp]] (video, 4 hrs) => Python

** Lyon data science nerd network (invitation only)
#+attr_latex: :width 400px
#+caption: Lyon Data & Analytics Nexus Discord server dashboard
[[../img/discord.png]]

- Join the discord server to grow your professional network
- Lyon alumni, students, and companies are invited
- Share internship and job opportunities, and more

** Google search for programmers (Goldin, 2023)
#+attr_latex: :width 400px
#+caption: Google search tutorial by Seth Goldin (freeCodeCamp)
[[../img/goldin.png]]

Waste of time?
1) Searching is not finding
2) Finding is not understanding
3) Understanding is not using

*Google's business model = data + algorithm + infrastructure + brand*
1) data are offered by users for free
2) algorithm is largely known and public
3) infrastructure is proprietary and hard to duplicate
4) brand is unique and highly valuable (but also vulnerable)

ChatGPT's answer to "how do I search using Google effectively?"
#+begin_quote
Here are a few tips for searching effectively using Google:

1. Use quotation marks to search for an exact phrase. For
   example, searching for ~"chocolate chip cookies"~ will return pages
   that include that exact phrase, rather than pages that include the
   words ~"chocolate"~, ~"chip"~, and ~"cookies"~ separately.

2. Use a minus sign to exclude certain words from your search. For
   example, searching for ~chocolate chip cookies -recipe~ will return
   pages that include ~chocolate chip cookies~ but exclude any pages
   that include the word "~recipe~".

3. Use the ~site:~ operator to search within a specific website. For
   example, searching for ~site:nytimes.com chocolate chip cookies~ will
   return pages on the New York Times website that include the words
   "~chocolate chip cookies~."

4. Use the ~filetype:~ operator to search for specific file types. For
   example, searching for chocolate chip cookies ~filetype:pdf~ will
   return PDF documents that include the words "~chocolate chip
   cookies~".

5. Use the ~related:~ operator to find websites that are related to a
   specific website. For example, searching for ~related:nytimes.com~
   will return websites that are related to the New York Times
   website.

6. Use the Google [[https://www.google.com/advanced_search][advanced search page]] for more complex queries.

Keep in mind that Google's search algorithm is constantly evolving, so
these tips may not always return the most accurate results.
#+end_quote

*Caveat:* professional programmers use aggregators instead
- [[https://stackoverflow.com/][stackoverflow.com]] with sub-groups like [[https://chess.stackexchange.com/questions/41282/computers-hardware-or-software][chess.stackexchange.com]]
- [[https://en.wikipedia.org/wiki/Internet_Relay_Chat][Internet Relay Chat]] - in Emacs (~M-x erc~), e.g. [[https://libera.chat/][libera.chat]]
- [[https://www.linuxtoday.com/developer/how-to-use-emacs-as-a-usenet-reader-with-gnus/][Usenet]], mailing lists, Slack, Discord, reddit, quora etc.

** Rapid review - intro to ML
#+attr_latex: :width 400px
#+caption: Photo by Benjamin Davies on Unsplash
[[../img/rapid_review.jpg]]

1) Most popular OS for ML?

2) Which tools are we using in this course?

3) What are the deliverables for your project?

4) Where do you get the topic for your term project?

5) What are the steps for a supervised learning process?

6) What is the general ML process?

* Week 3: Introduction to ML & Review of R (Part II)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] "Doctor, Doctor!"
- [X] Condition for repeating tests
- [X] Test 1 review in class
- [X] Project 1st sprint review
- [X] Introduction to machine learning (continued)
- [ ] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on Tuesday 24-Jan
- [X] 2nd DataCamp home assignment "Machine Learning Models" by Jan-31
- [X] In-class practice "Introduction to machine learning" (continued)
- [X] Exercise: build an ML code glossary
- [ ] In-class practice "R data structures"

** DONE [[https://lyon.instructure.com/courses/1021/discussion_topics/2125][Jan 26 session online]]
#+attr_latex: :width 400px
[[../img/zoom.png]]

** DONE Doctor, Doctor!
#+attr_latex: :width 400px
[[../img/couch.jpg]]

- ~M-x doctor~

** DONE Project - first sprint review (Tuesday, Feb 13)
#+attr_latex: :width 400px
[[../img/sprint.jpg]]

- The term project purpose is down to you - or you can approach me for
  an idea (but please do that sooner than later)!

- There are many different ways to present a paper or a slide pack:
  e.g. succinct, verbose, opinionated, objective, accessible or not.

- Your first deliverable is a *project proposal* formatted as an
  Org-mode file. Here is a template for such a file with definitions.

  1) Meta data: preliminary title (~#+TITLE~), list of team members
     (~#+AUTHOR:~), course title (~#+SUBTITLE~).

  2) Headlines: ~Problem~, ~Reason~, ~Constraints~, ~Goals and Non-goals~,
     ~Metrics~, ~References~

     - *Problem*: describe the problem that you're trying to solve.

     - *Reason*: why is this problem interesting (to you) right now?

     - *Constraints:* which difficulties, e.g. technical or conceptual,
       do you foresee right now?

     - *Goals and non-goals*: list all goals that you might want to
       achieve with this project. Order the goals by importance. Add a
       list of non-goals, i.e. things that are outside the scope of
       your project.

     - *Metrics*: how would you measure the success of your project?

     - *References*: list any references that you found already. Make
       sure that they are consistent (same format) and complete
       (author, date, title, place).

- Upload your result to Canvas (*no later than Feb 13*). Missing
  the deadline will cost you points.

Source: Ellis, Data Science Project Proposals (2021). URL:
[[https://crunchingthedata.com/data-science-project-proposals/][crunchingthedata.com]].

** DONE Condition for repeating tests
#+attr_latex: :width 400px
#+caption: Late or missed the test? Talk to me!
[[../img/late.jpg]]

- If you inform me beforehand that you cannot attend an announced
  test, we can make arrangements for you to take the test outside of
  class.

** DONE [[https://lyon.instructure.com/courses/1021/assignments/6737/edit?quiz_lti][Test 1 review]]
** DONE How should you study for data science tests?
#+attr_latex: :width 200px
[[../img/studying.jpg]]

- If you were successful in the test: what did you do?
- If not: what do you think you should have done?

* Week 4: Icestorm
#+attr_latex: :width 400px
[[../img/icestorm.jpg]]

* Week 5: Review of R (Part III)
#+attr_latex: :width 400px
[[../img/TourDeFrance.jpg]]

** DONE Upload practice files 2 GDrive (or 2 stick)
#+attr_latex: :width 400px
[[../img/gdrive.png]]

#+attr_latex: :width 400px
[[../img/sticks.jpeg]]

- Make a bootable Linux stick (e.g. [[https://linuxmint-installation-guide.readthedocs.io/en/latest/burn.html][Linux Mint]]), too

** DONE Create an ~.Rprofile~

- Create or open ~~/.Rprofile~ and add these lines to it:
  #+begin_example R
  options(repos=c("https://cloud.r-project.org/"))
  options(crayon.enabled = FALSE)
  message("*** Loaded .Rprofile ***")
  #+end_example

- Save the file and start an R console to test it (you should also see
  the ~Loaded~ message:
  #+begin_src R
    options()$repos
  #+end_src

- From now on, Windows will no longer ask you to choose a mirror site,
  and you will be able to display a "~tibble~" (a sort of data frame
  popular in the "Tidyverse") in Emacs:
  #+begin_src R :results output
    library(MASS)
    library(tibble)
    tibble(mtcars)
  #+end_src

  #+RESULTS:
  #+begin_example
  # A tibble: 32 × 11
       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
   1  21       6  160    110  3.9   2.62  16.5     0     1     4     4
   2  21       6  160    110  3.9   2.88  17.0     0     1     4     4
   3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1
   4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1
   5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2
   6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1
   7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4
   8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2
   9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
  10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4
  # … with 22 more rows
  # ℹ Use `print(n = ...)` to see more rows
  #+end_example

** DONE Test 2 opens Friday - closes Tuesday Feb 14
#+attr_latex: :width 400px
#+caption: Image by Grovemade via Unsplash.com
[[../img/homeoffice.jpg]]

- This test will be about machine learning models (lecture) and about
  the review of R (in-class assignment).

- You can complete the test at your leisure (within the set time
  limit) but you must complete it before the deadline (Tue-14-Feb)

** DONE Home assignments - project and DataCamp
#+attr_latex: :width 400px
#+caption: Image by Feliphe Schiarolli via Unsplash.com
[[../img/classroom.jpg]]

1) First sprint review: proposal as Org-mode file [[https://lyon.instructure.com/courses/1021/assignments/7108][in Canvas (Feb 13)]]

2) Complete a whole DataCamp course on supervised learning ([[https://lyon.instructure.com/courses/1021/assignments/7387][Mar 24]])
   - Classification using nearest neighbors
   - Classification using Naive Bayes
   - Classifiction using Logistic Regression (curve fitting)
   - Classifiction using Decision Trees

3) We'll review the DataCamp sessions in class and in the tests (I'll
   announce tests 1 week in advance)

** DONE R review: structures/management/exploration
#+attr_latex: :width 250px
[[../img/rlogo.png]]

1) Data structures in R
2) Managing data in R
3) Exploring data in R

[Source: [[https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864][Lantz, Machine learning with R (3e), chapter 2]]]

- Download raw ~2_R_structure_practice.org~ [[https://raw.githubusercontent.com/birkenkrahe/ml/main/org/2_R_structure_practice.org][from GitHub (birkenkrahe/ml)]]
- Open a CMD line terminal (Windows search: CMD, Mac: terminal)
- Navigate to the download directory with ~cd~
- Open the file in a terminal Emacs (can you take this command apart?)
  #+begin_example sh
    emacs -nw --file 2_R_structure_practice.org
  #+end_example
  #+attr_latex: :width 400px
  [[../img/emacs.png]]
- Finish practice files started in class on your own by the deadline

** READ Understanding R startup
#+attr_latex: :width 400px
#+caption: From "R for Enterprise: Understanding R's Startup (Lopp, 2019)
[[../img/rstartup.jpeg]]

[[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][Here is an article]] (Lopp, 2019) on R startup variables and
settings. Includes an explanation why the ~.Rprofile~ startup file was
not read when some of you opened the R console in the shell (you
should probably try ~Rgui~ on the command line, too).

** NEW Get bonus points when practicing
#+attr_latex: :width 200px
[[../img/datacamp2.png]]
- You can get 10 bonus points if you keep a practice streak of 10 days
- You can do this up to 3 times for a maximum of 30 points, which will
  be applied to your weakest final grade category
- Submit a screenshot of your mobile (or desktop) streak in Canvas
- If you lose your streak between day 5 and 10, you still get 5 points
- On the dashboard, DataCamp will suggest practice categories for you,
  and also in the mobile app
- This option ends on May 3rd (last day of spring term)
- You can get this bonus only in one of my courses (if you attend > 1)
#+attr_latex: :width 400p
[[../img/datacamp3.png]]

** NEW GNU Treats: ~speed-type~, ~treemacs~ and ~gtypist~

- An attractive alternative to ~Dired~ is the ~treemacs~ package. It
  looks like this on my PC (and also works for the terminal Emacs):
  #+attr_latex: :width 400px
  [[../img/t_treemacs.png]]

- If you want to be faster on the keyboard, try [[https://www.gnu.org/savannah-checkouts/gnu/gtypist/gtypist.html#:~:text=GNU%20Typist%20(also%20called%20gtypist,the%20GNU%20General%20Public%20License.][GNU Typist]], a free
  10-lesson online trainer for increasing your typing skills.
  #+attr_latex: :width 400px
  [[../img/gtypist.png]]

- There is also an Emacs package to practice touch/speed typing in
  Emacs called ~speed-type~. You have to install it with ~M-x
  package-list-packages~, then find the package in the list and install
  with ~i~ and ~x~. [[https://github.com/dakra/speed-type][More information on GitHub.]]

* Week 6: ML models overview
#+attr_latex: :width 400px
[[../img/ml_types2.png]]

- [X] Remember to upload practice (there are deadlines) [[https://lyon.instructure.com/courses/1021/assignments][to Canvas]]
- [X] Open ~2_R_explore_practice.org~ and load the data
- [X] Let's finish the review and upload the completed file to Canvas
- [X] What is R? Good overview [[https://www.datacamp.com/blog/all-about-r][in this DataCamp blog post]] (05/22)
- [X] [[https://lyon.instructure.com/courses/1021/assignments/6997][Test 2 (open book) is live online until Fri 17-Feb, 11:59 pm]].

** Review: exploring numerical data structure:

Open an Org-mode file if you want to code along.

1) [X] How can you get an overview of statistical information for ~Nile~?
   #+begin_src R :results output
     summary(Nile)
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :   456.0   798.5   893.5   919.4  1032.5  1370.0

2) [X] What about the ~time~ of the ~Nile~ observations?
   #+begin_src R :results output
     summary(time(Nile))
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :    1871    1896    1920    1920    1945    1970

3) [X] How many values of ~Nile~ are outliers? Which values are these?
   #+begin_quote
   *Update:* as the computation of ~IQR*1.5~ for ~Nile~ shows, there are no
   outliers in the data set - the boxplot (below) shows this,
   too. Tukey's method (~car::outlier~) is not applicable to time
   series, alas.
   #+end_quote

   Various tests:
   #+begin_src R :results output
     IQR(Nile) * 1.5 # standard outlier value
     length(Nile > (IQR(Nile) * 1.5))
     scale(Nile) # z-score method
     Nile[which(Nile > (IQR(Nile) * 1.5))]
   #+end_src
   #+begin_src R :results output
     library(car)
     any(ls('package:car')=="outlier")
                                             # outlierTest(Nile)  ## not applicable to time series
     library(qdap)
     outlier_detect(as.character(Nile))
   #+end_src
   #+begin_src R
     car::outlierTest(Nile)
     ## outlier.test(c(1,2,3,1000))
   #+end_src
4) [ ] How would you make a boxplot of the values of ~Nile~?
   #+begin_src R :results graphics file output :file ../img/Rbox.png
     boxplot(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rbox.png]]

5) [ ] How would you visualize how often certain values of ~Nile~ occur?
   #+begin_src R :results graphics file output :file ../img/Rhist.png
     hist(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rhist.png]]

6) [ ] How many observations of the ~Nile~ flow are between 800 and 1000
   mio cubic metres?
   #+begin_src R
     length(
       Nile[
         which(
           Nile > 800 & Nile < 1000
         )
       ]
     )
   #+end_src

   #+RESULTS:
   : 44

** AI's Bid for Freedom
#+attr_latex: :width 400px
[[../img/freedom.png]]

- My courses for Fall 2023:

  1) Artificial Intelligence (DSC 482.01) - seminar. The 2023 edition
     will include deep learning with R/Python and more code poetry.

  2) Data and process modeling (DSC 482.02) - seminar. The 2023
     edition will include exploring differential equations with R

  3) Introduction to data science (DSC 105): The 2023 edition will
     include R and Python.

- All courses are for everybody who's interested in data science: tell
  your friends!

** Project proposal feedback in the classroom
** You can submit an improved proposal by Fri-Feb-24

* Week 7: k-Nearest-Neighbor (k-NN) algorithms
#+attr_latex: :width 400px
#+caption: Photo by Avin CP on Unsplash
[[../img/tomato.jpg]]
5_ductal_carcinoma.jpg
- [X] Test 2 now open for late attempts (partial credit)

- [X] [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][Man beats machine at Go in human victory over AI]] (Waters, 2023)
  #+begin_quote
  "The discovery of a weakness in some of the most advanced Go-playing
  machines points to a fundamental flaw in the deep-learning systems
  that underpin today’s most advanced AI [...]: The systems can
  “understand” only specific situations they have been exposed to in
  the past and are unable to *generalize* in a way that humans find
  easy. It shows once again we’ve been far too hasty to ascribe
  superhuman levels of intelligence to machines.” - Russell (AIMA)
  #+end_quote

- [-] k-nearest-neighbor (knn) models:
  1) [X] Lecture (HTML, Org-mode or [[https://github.com/birkenkrahe/ml/blob/main/md/4_knn.md][Markdown]] with notes in GitHub)
  2) [X] Practice (demo: cancer classification with k-NN)
  3) [ ] Exercise (bonus assignment in R)
  4) [ ] Test 3 (opens next week)

* Week 8: k-NN case study - cancer prediction (cont'd)
#+attr_latex: :width 400px
#+caption: Ductal Carcinoma in situ (Source: jhp.edu)
[[../img/5_ductal_carcinoma.jpg]]

** Test 3 preview

Here are [[https://github.com/birkenkrahe/ml/blob/main/org/agenda.org#next-test-3-preview][10 questions]] that preview the next (graded) ML test
*When prompted, write down your answer or keywords to the answer.*

1) [X] How is similarity measured in k-NN?
   #+begin_notes
   By computing a distance measure, e.g. Euclidean distance, the usual
   distance between two vectors, see ~dist~.
   #+end_notes
2) [X] What is used for "training" and "testing" the k-NN classifier?
   #+begin_notes
   Two data sets, one known, labelled data to train the model on a
   label, the other one unseen, unlabelled data to test the trained
   model. The training and test data should be of comparable
   quality and randomized.
   #+end_notes
3) [X] What is the meaning of the parameter 'k'? (4)
   #+begin_notes
   - The number of nearest neigbors after computing distance
   - A measure of the size of the classification neighborhood
   - A measure for the degree of over- or underfitting
   - An argument in the ~class::knn~ function
   #+end_notes
4) [X] What are the arguments of ~knn~? (4)
   #+begin_src R :results output :exports none
     library(class); args(knn)
   #+end_src

   #+RESULTS:
   : function (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
   : NULL
5) [X] What is the purpose of splitting the data in training and test
   data? Why don't we just use ALL available data for training?
   #+begin_notes
   - To avoid overfitting: without testing, it is not clear to which
     extent the model will generalize to unseen data. We split one
     data set because the data are of comparable quality/structure.
   #+end_notes
6) [X] What happens when voting is tied after measuring the distances?
   #+begin_notes
   In ~class::knn~, the class is decided at random. Use odd ~k~ to avoid this.
   #+end_notes
7) [X] If I have 10 features q_{1}...q_{10} and 1 target label
   - How many dimensions does the feature space have?
   - How many terms under the square root of the distance formula?
   - How many variables are used for classification of p?
     #+begin_notes
     - 10 features, hence 10-dim feature space
     - Each point has 10 coordinates. The distance formula has 10 terms
       p_{i}-q_{ij}, one for each i-th coordinate of q_{j}, and 11
       variables are involved: 10 features (predictors) and 1 target
       variable (predicted).
     #+end_notes
8) [X] Is a bigger ~k~ always better? Why or why not?
   #+begin_notes
   Optimal k depends on noise in the data, on the pattern to be
   identified, one starts off with the square root of the number of
   training examples, to find the optimal k, try different values.
   #+end_notes
9) [X] Measuring model performance:
   - What is the confusion matrix? What's its *dimension*?
   - How can you *compute* the confusion matrix for kNN in R?
   - How can you assess the accuracy *numerically*?
     #+begin_notes
     - The confusion matrix is a 2 x 2 cross-table if the target feature
       had 2 labels/categories (e.g. male/female or benign/malignant),
       or a 3 x 3 for 3 labels. It shows, which levels were accurately
       identified and which were not ([[https://github.com/birkenkrahe/ml/blob/main/img/confusion1.png][solution]]).
       #+attr_latex: :width 400px
       #+caption: Confusion matrix (empty) and classification results
       [[../img/confusion.png]]
     - With the ~table~ function and the ~factor~ levels or ~character~
       values as arguments, e.g. ~table(signs_pred,signs_actual)~.
     - With the ~mean~ function by averaging over the vector resulting
       from comparing original and predicted values:
       ~mean(sign_actual==sign_pred)~.
     #+end_notes
10) [X] What does bias-tradeoff refer to?
    #+begin_notes
    #+attr_latex: :width 400px
    [[../img/bias_tradeoff.png]]
    The image shows subsequent classification attempts. Perfect
    prediction corresponds to the bulls-eye. The further away a point
    is, the worse the prediction.
    - Bias leads to a systematic prediction error, e.g. because of
      existing patterns in the data, or other redundancies.
    - Low bias (upper row) means low error and high bias (lower row)
      means large error.
    - Variance corresponds to spread. Low variance (left column)
      leaves the results together, high variance blows them up.
    - The best case is low bias and low variance: low error, points
      close together, the worst is high bias and high variance.
    #+end_notes
** Assignment - submit by March 10
#+attr_latex: :width 400px
#+caption: Photo by Andrew Neel, Unsplash.com
[[../img/assignment.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8405][Next assignment: NAIVE BAYES method - deadline MARCH 7, 11:59 pm]]

** Test - submit by March 10
#+attr_latex: :width 400px
#+caption: Photo by Ben Mullins, Unsplash.com
[[../img/0_test.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8789/edit?quiz_lti][Test 3 is live from later today until Friday, MARCH 10, 11:59pm.]]

** Project - 2nd sprint review by March 17

** Continue: kNN case study
#+attr_latex: :width 600px
#+caption: k-NN workflow (Gavagsaz, 2022)
[[../img/knn.jpg]]

- If you missed the last session, *download* ~5_knn_practice.org~
  fromGitHub and head to the chapter *Intermission* to catch up!

- Lecture: ~5_knn_case.org~ & 2 bonus exercises

- *We hope to cover today:*
  1) Normalizing (rescaling) numeric data
  2) Creating training and test data sets
  3) Training a model on the data
  4) Evaluating model performance
  5) Exercises: improve the model! (bonus)

* Week 9: k-NN improvement & Naive Bayes
#+attr_latex: :width 400px
[[../img/amazing-thomas-bayes-illustration.jpg]]

*REMINDERS* (check your Canvas calendar):
- Complete *test 3* by Friday 11:59 pm this week!
- Complete *DataCamp assignment* "Naive Bayes" by Friday this week!
- [[https://lyon.instructure.com/courses/1021/assignments/9116][Literature review (2nd sprint review) by end of next week]]!

*THIS WEEK*:
- Evaluating kNN performance (practice)
- Improving kNN performance (2 x exercise)
- Lecture and practice: supervised learning with Naive Bayes

*UPCOMING TOPICS*:
- Text mining to build spam filter with Naive Bayes
- Word cloud visualization
- Regression methods: linear and logistic methods
- Regression use case: predicting medical expenses

** DONE Review: kNN case study (continued)
#+attr_latex: :width 400px
[[../img/rapid_review.jpg]]

- [ ] What are we trying to do?

- [ ] What does "training a kNN learner" mean?

- [ ] What have we done so far?
  #+begin_quotes
  1) Data collection and storage (data frame)
  2) Exploration of the class/target label and predictors/features
  3) Cleaning, randomization and normalization of the data
  4) Splitting of the dataset into training and test data
  5) Isolating the training and test labels
  6) Running the ~class::knn~ function with set ~k~ value


  - [ ] What's left?
    #+begin_quotes
    - Evaluating the performance of the model (confusion matrix, accuracy)
    - Improving the performance (standardization, varying k-values)
    - Running the model on new datasets (prepare data, run ~predict~)
  #+end_quotes

** TODO Game programming in Snap! with AI
#+attr_latex: :width 400px
[[../img/snap.png]]

- https://github.com/birkenkrahe/snap (HTML, PDF, Org, md)
- Next fall: [[https://ecraft2learn.github.io/ai/][AI extensions to Snap!]] with [[https://ecraft2learn.github.io/ai/student-projects/][student project examples]]

** TODO Bonus exercise & March assignments
#+attr_latex: :width 400px
[[../img/bonus.png]]

- Remember how to compute the prediction accuracy: as the ~mean~ over
  the values of the original and the predicted ~factor~ vector ([[https://github.com/birkenkrahe/ml/blob/main/org/4_knn_case.org#computing-accuracy-as-an-average][GitHub]])

* Week 10: Naive Bayes algorithm and case study
#+attr_latex: :width 400px
#+caption: Illustration by Peter Eich (2006), Wikimedia (CC BY-SA 2.5)
[[../img/5_Spamfilter.jpg]]

- Naive Bayes review and algorithm
- Naive Bayes case study: spam filter
- Naive Bayes test 5 (due March 31)

** Pie Day!
#+attr_html: :width 300px
[[../img/pi.jpg]] [[../img/limerick.png]]

** Results: Test 3 and Tsunami of assignments
#+attr_latex: :width 400px
[[../img/mlTestBox.png]]

- Feel free to complete test 3 by Friday for partial credit (60%)

- [[https://github.com/birkenkrahe/org/blob/master/pdf/midterm.pdf][How to improve your mid-term grades if you choose to do so]]. The link
  to [[https://www.quora.com/Why-are-computer-science-degrees-so-math-intensive-when-the-field-doesnt-seem-to-use-much-math-at-all][this Quora comment]] is not available in the document, alas.

- Do not forget that there are *three DataCamp lessons* waiting for
  completion by March 31, 2023: Naive Bayes, Logistic Regression, and
  Classification Trees.

- I'll decide over spring break which of these (if any) we'll pick up
  for detailed treatment in class. In any case: more certificates 4U!

** Review: Simple Naive Bayes
#+attr_html: :width 300px
#+caption: Photo by Ben Mullins on Unsplash
[[../img/review.jpg]]

1) Which evidence (in the data) is used for Naive Bayes?
   #+begin_notes
   Answer: NB uses all available data with stochastic weights or
   probabilities attached to individual features.
   #+end_notes
2) For these *events*, what's a suitable *trial* to establish probability?
   - Spam message
   - Lottery win
   - Sunny weather
3) What determines the class our classifier is trying to predict?
   #+begin_notes
   Answer: 1) our observational data (e.g. recorded labels spam
   vs. ham), 2) our problem (e.g. reduce spam)
   #+end_notes
4) Can the same dataset be used to predict different things? For
   example, could a dataset that contains spam vs. ham labels (in
   addition to many other text features) be used for sentiment
   analysis?
5) Why are more trials better for the accuracy of a probabilistic
   prediction?
   #+begin_notes
   Because of the "law of large numbers", which holds for independent
   trials of the same experiment - the expected value (sum of all
   values weighted by their individual probabilities) becomes the
   sample mean (arithmetic average of all values).
   #+end_notes
6) What kind of events are "spam vs. ham", or "win vs. loss",
   "benign vs. malignant".
   #+begin_notes
   Answer: they are mutually exclusive and exhaustive events, i.e. an
   event is either one or the other but never both, the sum of their
   probabilities adds up to one, and their joint probability is zero.
   #+end_notes
7) What's is the joint probability of the events "being home" and
   "being at the office"?
   #+begin_notes
   Answer: it is zero provided that these events are mutually
   exclusive.
   #+end_notes
8) If event B is "being home" and event A is "working", what is
   the conditional probability of working at home, P(A|B)?
   #+begin_notes
   (1) conditional probability: how likely is an event A based on
   occurrence of a previous event B?  (2) P(A|B) = P(A and B)/P(B),
   joint probability of A and B divided by the probability of B,
   (3) P(A|B)=P(B|A) * P(A)/P(B)
   #+end_notes
9) What is the basis of predictive, probabilistic modeling?
   #+begin_notes
   Answer: dependent events - dependency of events (recorded as
   feature values) means that the occurrence of one event is
   conditional on another: for example, clouds are a condition for
   rain. The probability of rain is conditional on the probability of
   clouds appearing - the latter reveals something about the future of
   the former that we can use to predict the weather. Clouds are
   predictors for rain.
   #+end_notes
10) What does independence of two events imply?
    #+begin_notes
    Answer: knowing something about one event reveals nothing about
    the other event. Neither can be used as a predictor for the
    other. Their joint probability (the probability that they occur
    together) is the product of the individual probabilities.
    #+end_notes
11) You're trying to establish the probability that a message with the
    word "URGENT" in upper case letters is a spam message. What else
    do you need to compute this?
    #+begin_notes
    Answer: according to Bayes' theorem, the conditional probability
    P(spam|URGENT) = P(URGENT|spam) * P(spam) / P(URGENT). You need:
    - P(URGENT|spam): likelihood of finding URGENT in a spam message;
    - P(spam): chance of spam in a given sample of messages (trials)
    - P(URGENT): chance of finding "URGENT" in a sample of messages
    #+end_notes
12) Compute P(spam|URGENT) from the following *frequency* table:
    | FREQUENCY | URGENT=YES | URGENT=NO | TOTAL |
    |-----------+------------+-----------+-------|
    | spam      |          9 |        16 |    25 |
    | ham       |          5 |       120 |   125 |
    |-----------+------------+-----------+-------|
    | Total     |         14 |       136 |   150 |
    #+begin_notes
    Answer: create *likelihood* table - the conditional probabilities
    | LIKELIHOOD | URGENT=YES | URGENT=NO | TOTAL |
    |------------+------------+-----------+-------|
    | spam       | 9/25       | 16/25     |    25 |
    | ham        | 5/125      | 120/125   |   125 |
    |------------+------------+-----------+-------|
    | Total      | 14/150     | 136/150   |   150 |

    + ~P(URGENT|spam)~ = 9/25 (likelihood of URGENT in spam)
    + ~P(spam)~ = 25/150 (prior probability - before URGENT)
    + ~P(URGENT)~ = 14/150 (marginal likelihood)
    #+end_notes
    #+begin_src R
      p <- (9/25) * (25/150) / (14/150)
      paste("Posterior probability:",format(p*100,digits=2),"%")
    #+end_src

    #+RESULTS:
    : Posterior probability: 64 %

** Reading therapy for spring break: ELIZA
#+attr_latex: :width 400px
[[../img/eliza.png]]

- You will get a print copy of this seminal article to read and
  (perhaps) discuss.

- Cool online implementation on a DEC VT100 terminal at
  [[https://www.masswerk.at/eliza/][masswerk.at/eliza/]]

- ELIZA - A Computer Program For the Study of Natural Language
  Communication Between Man And Machine - Weizenbaum, CACM 9(1)
  (1966):36-46. URL: [[https://dl.acm.org/doi/pdf/10.1145/365153.365168][dl.acm.org]].

- See also: The computational therapeutic: exploring Weizenbaum’s ELIZA as a
  history of the present - Bassett, AI & Society 34
  (2019):803-812. URL: [[https://link.springer.com/article/10.1007/s00146-018-0825-9][link.springer.com]].

** Review: Spam Filter I
#+attr_html: :width 300px
#+caption: Photo by Ben Mullins on Unsplash
[[../img/review1.jpg]]

1) How is the usual ML workflow altered for a message spam filter?
   #+begin_notes
   Answer: the data cleaning includes text formatting, tokenization,
   stemming, stopwords, corpus creation, and a term frequency table.
   #+end_notes
2) How many features does the raw data set (SMS Spam Collection) have?
   How many will the cleaned data set have?
   #+begin_notes
   Answer: the initial dataset has 2 features - class label (spam,
   ham), and message. The final dataset has as many features as spam
   trigger terms (probably several hundred) - our example: > 6,000.
   #+end_notes
3) You've just loaded the R package "tm". How would you check (1) that
   the package is loaded, (2) which functions and (3) which datasets
   (if any) it contains?
   #+begin_src R :results output
     library(tm)   # load package provided it's been installed
     search()  # check package environments search path
     ls('package:tm') # check methods/functions inside tm
     data(package="tm") # list datasets contained in package
   #+end_src

   #+RESULTS:
   #+begin_example
    [1] ".GlobalEnv"        "package:tm"        "package:NLP"
    [4] "ESSR"              "package:stats"     "package:graphics"
    [7] "package:grDevices" "package:utils"     "package:datasets"
   [10] "package:stringr"   "package:httr"      "package:methods"
   [13] "Autoloads"         "package:base"
    [1] "as.DocumentTermMatrix"   "as.TermDocumentMatrix"
    [3] "as.VCorpus"              "Boost_tokenizer"
    [5] "content_transformer"     "Corpus"
    [7] "DataframeSource"         "DirSource"
    [9] "Docs"                    "DocumentTermMatrix"
   [11] "DublinCore"              "DublinCore<-"
   [13] "eoi"                     "findAssocs"
   [15] "findFreqTerms"           "findMostFreqTerms"
   [17] "FunctionGenerator"       "getElem"
   [19] "getMeta"                 "getReaders"
   [21] "getSources"              "getTokenizers"
   [23] "getTransformations"      "Heaps_plot"
   [25] "inspect"                 "MC_tokenizer"
   [27] "nDocs"                   "nTerms"
   [29] "PCorpus"                 "pGetElem"
   [31] "PlainTextDocument"       "read_dtm_Blei_et_al"
   [33] "read_dtm_MC"             "readDataframe"
   [35] "readDOC"                 "reader"
   [37] "readPDF"                 "readPlain"
   [39] "readRCV1"                "readRCV1asPlain"
   [41] "readReut21578XML"        "readReut21578XMLasPlain"
   [43] "readTagged"              "readXML"
   [45] "removeNumbers"           "removePunctuation"
   [47] "removeSparseTerms"       "removeWords"
   [49] "scan_tokenizer"          "SimpleCorpus"
   [51] "SimpleSource"            "stemCompletion"
   [53] "stemDocument"            "stepNext"
   [55] "stopwords"               "stripWhitespace"
   [57] "TermDocumentMatrix"      "termFreq"
   [59] "Terms"                   "tm_filter"
   [61] "tm_index"                "tm_map"
   [63] "tm_parLapply"            "tm_parLapply_engine"
   [65] "tm_reduce"               "tm_term_score"
   [67] "URISource"               "VCorpus"
   [69] "VectorSource"            "weightBin"
   [71] "WeightFunction"          "weightSMART"
   [73] "weightTf"                "weightTfIdf"
   [75] "writeCorpus"             "XMLSource"
   [77] "XMLTextDocument"         "Zipf_plot"
   [79] "ZipSource"
   Data sets in package 'tm':

   acq                     50 Exemplary News Articles from the
                           Reuters-21578 Data Set of Topic acq
   crude                   20 Exemplary News Articles from the
                           Reuters-21578 Data Set of Topic crude
   #+end_example

Now go to GitHub, save ~5.2.org~ to ~Downloads~ and insert the file to
your practice file ~5_naive_bayes_practice.org~ with ~C-x i~.

Save the file and run all code blocks: ~M-x org-babel-execute-buffer~,
then check with ~ls~ and ~search~ that all is ready to move on.

* Week 11: AI and chatbots / Saving & loading ~.RData~
#+attr_latex: :width 400px
#+caption: G. Washington's teeth, Library of Congress
[[../img/teeth.jpg]]

- [X] I evaluated & graded outstanding exercises (*please check before*)
- [X] The literature reviews were all impressive in their own way. The
  best one even referenced other sources within the review.
- [X] Those who have not submitted reviews yet please do so *by March 31*
  for partial credit.
- [X] Next sprint review: results! *Due on April 21.*
- [X] *Test 4* is live from 2.30 pm 28-March: full points if you submit by
  March 31, partial credit if you miss the deadline but submit by
  April 7.
- [X] *Bonus points* for experience report from the job fair April 4 -
  [[https://lyon.instructure.com/courses/1021/assignments/9611][see Canvas]]

** In the meantime: the charade continues
#+attr_latex: :width 550px
#+caption: Source: Twitter
[[../img/neil_gaiman.png]]
[[../img/leo_pires.png]]

In response to a "Sandman" fan discovering chatbot lies.
#+attr_latex: :width 400px
#+caption: Noam Chomsky, MIT.
[[../img/chomsky.png]]

[[https://drive.google.com/file/d/1lptWLxqzpFbSPKsumU5KsnsKUD6__PyO/view?usp=sharing][Right down this alley: Chomsky's biting critique of ChatGPT in the NYT]]

** DataCamp assignments update
#+attr_latex: :width 400px
[[../img/datacampupdate.png]]

- DataCamp assignments: *due dates* changed to weekly in April
- *Removed* two assignments (we need some time for neural nets)
- The last assignments carry *more points* (you're more on your own)
- If you already completed both courses, contact me for a bonus
- *When you complete these, try to maximize your XP (don't cheat)*

** ELIZA - "Fake Rogerian Therapist"
#+attr_latex: :width 400px
[[../img/weizenbaum-eliza.jpg]]

[[https://www.masswerk.at/elizabot/][TLDR? Here's a summary]].

*20 MINUTES "CLOSE READING" AND DISCUSSION:*
1) Split into pairs or groups of three
2) Discuss your views of ELIZA in the light of both what you have learnt
   about machine learning, and of Weizenbaum's conclusions (5 min)
3) Say in your own words what an "augmented ELIZA program" should be
   able to do and contrast this what you know of state-of-the-art
   chatbots programs (5 min)
5) Highlight at least one hypothesis from [[https://www.reddit.com/r/ChatGPT/comments/11ng6t6/noam_chomsky_the_false_promise_of_chatgpt/][Chomsky's article]] "The False
   Promise of ChatGPT" (he's got a lot of 'em - I counted 10) (5 min)
6) What does all this mean for the future of machine learning? (5 min)

Along the way, write down any QUESTIONS that you may have!

#+begin_quote
*In addition, Weizenbaum wrote:*
- "The whole issue of the credibility (to humans) of machine output
  demands investigation. Important decisions increasingly tend to be
  made in response to computer output. ELIZA shows [...] how easy it
  is to create and maintain the illusion of understanding. A certain
  danger lurks here. [...] An augmented program is a system which
  already has access to a store of information about some aspects of
  the real world which, by means of conversational interaction with
  people, can reveal both what it knows (behave as an information
  retrieval system) and where its knowledge ends and needs to be
  augmented. Hopefully, the augmentation of its knowledge will also be
  a direct consequence of its conversational experience."
#+end_quote
#+begin_quote
*Notes from reading Weizenbaum's article:*
- "Rogerian psychotherapist": empathy-based therapy
- "dyadic conversation": between two people
- MAC time-sharing system: precursor of Unix (MAC -> EMACs)
- Separation of code (scripted, edited) and data (transformed)
- Text data transformation with tokenization functions
- Text interpretation after applying stop words transformation
- "Augmented ELIZA" is not realized yet, ChatGPT is just another
  "translating processor" (without understanding).
#+end_quote
#+begin_quote
*Chomsky's hypotheses:*
1) Machine learning will degrade our science and debase our ethics by
   incorporating into our technology a fundamentally flawed conception
   of language and knowledge.
2) True understanding has not, will not and cannot occur if ML
   programs like ChatGPT dominate the field of AI.
3) The human mind is not a lumbering statistical pattern matching
   engine gorging on Big Data and extrapolating the most probable
   answer by inferring correlations.
4) The human mind operates with small amount of information and
   creates explanations.
5) Chatbots are stuck in a prehuman phase of cognitive evolution
   because they lack the ability to think of counterfactuals.
6) The predictions of ML systems will always be superficial and
   dubious.
7) ML predictions of scientific facts are pseudoscience.
8) True intelligence is demonstrated in the ability to think and
   express improbable but insightful things.
9) True intelligence is capable of moral thinking, i.e. constraining
   creativity with ethical principles.
10) Chatbots are constitutionally unable to balance creativity with
    (moral) constraint.

*Questions you might have had:*
- Who was Jorge Luis Borges? ([[https://en.wikipedia.org/wiki/Jorge_Luis_Borges][Blind Argentinian librarian and author]])
- What is '[[https://en.wikipedia.org/wiki/Universal_grammar][universal grammar]]'? (Chomskian theory of language, 1965)
- What is 'the banality of evil'? (Arendt: "Can one do evil without
  being evil?", in "[[https://en.wikipedia.org/wiki/Eichmann_in_Jerusalem#Banality_of_evil][Eichmann In Jerusalem]]", 1963).
- Who is Jeffrey Watumull? (Chief Philosophy Officer of Oceanit -
  "Intellectual Anarchy through Disruptive Innovation")
#+end_quote

** Lex Fridman (MIT) interviews Sam Altman (OpenAI)
#+attr_latex: :width 400px
#+caption: Lex Fridman (MIT) and Sam Altman (OpenAI) 26-March-2023
[[../img/altman.png]]

- [[https://youtu.be/L_Guz73e6fw][Link to the > 2 hr interview (YouTube)]]

** Bonus points for job fair experience report!
#+attr_latex: :width 400px
[[../img/fair_flickr_color.jpg]]

- Write long paragraph about your job fair experience for 10 points.
- Great opportunity to network, mix and mingle, and show off.
- Bring 1 page resume, a few questions, a story, and dress up.
- Must go: graduating seniors. Should go: everyone else.
- Motivate each other by going as a pair, a group, a team.
- Post your experience report in Canvas.

** Review: text mining
#+attr_latex: :width 400px
[[../img/sparse_dense.gif]]

1) What is the relative proportion of spam vs. ham messages in the
   ~sms_raw~ dataset if the ~type~ feature contains this information, in
   the ~format~ of ~numeric~ output with ~2~ ~digits~ after the decimal point?
   #+begin_src R
     <<load_sms_raw>>
     ## easy: intermediate storage
     spam_ham <- table(sms_raw$type)  # frequencies of spam vs ham
     spam_ham_prop <- prop.table(spam_ham)  # proportions of spam vs ham
     spam_ham_prop_2 <- format(spam_ham_prop,digits=2)
     spam_ham_prop_2 <- as.numeric(spam_ham_prop_2)
     spam_ham_prop_2
                                             #as.numeric(spam_ham_prop_2)

     ## medium: nested/functional call
     as.numeric(format(prop.table(table(sms_raw$type)),digits=2))
   #+end_src

   #+RESULTS:
   : [1] 0.87 0.13
   : [1] 0.87 0.13
   
2) Can you compute the same thing using a pipeline?
   #+begin_src R
     <<load_sms_raw>>
     ## hard: pipeline/shell call
     sms_raw$type |>
       table() |>
       prop.table() |>
       format(digits=2) |>
       as.numeric()
   #+end_src

3) How can you check directly that the ~sms_raw~ dataframe was actually
   loaded into the current R session - with a ~logical~ answer?
   #+begin_src R
     any(ls()=="sms_raw")
   #+end_src

   #+RESULTS:
   : [1] TRUE
   :  [1] "api_key"          "ask_chatgpt"      "ham"              "m"               
   :  [5] "sms_corpus"       "sms_corpus_clean" "sms_dtm"          "sms_dtm_test"    
   :  [9] "sms_dtm_train"    "sms_dtm2"         "sms_raw"          "sms_test_labels" 
   : [13] "sms_train_labels" "spam"             "spam_ham"         "spam_ham_prop"   
   : [17] "spam_ham_prop_2"  "string"           "stringX"          "tokens"

4) Is the ~removeWords~ function part of the ~tm~ package? What are the
   steps to check this?
   #+begin_src R
     library(tm)
     any(ls('package:tm')=="removeWords")
     ls('package:tm')
   #+end_src

   #+RESULTS:
   #+begin_example
   [1] TRUE
    [1] "as.DocumentTermMatrix"   "as.TermDocumentMatrix"  
    [3] "as.VCorpus"              "Boost_tokenizer"        
    [5] "content_transformer"     "Corpus"                 
    [7] "DataframeSource"         "DirSource"              
    [9] "Docs"                    "DocumentTermMatrix"     
   [11] "DublinCore"              "DublinCore<-"           
   [13] "eoi"                     "findAssocs"             
   [15] "findFreqTerms"           "findMostFreqTerms"      
   [17] "FunctionGenerator"       "getElem"                
   [19] "getMeta"                 "getReaders"             
   [21] "getSources"              "getTokenizers"          
   [23] "getTransformations"      "Heaps_plot"             
   [25] "inspect"                 "MC_tokenizer"           
   [27] "nDocs"                   "nTerms"                 
   [29] "PCorpus"                 "pGetElem"               
   [31] "PlainTextDocument"       "read_dtm_Blei_et_al"    
   [33] "read_dtm_MC"             "readDataframe"          
   [35] "readDOC"                 "reader"                 
   [37] "readPDF"                 "readPlain"              
   [39] "readRCV1"                "readRCV1asPlain"        
   [41] "readReut21578XML"        "readReut21578XMLasPlain"
   [43] "readTagged"              "readXML"                
   [45] "removeNumbers"           "removePunctuation"      
   [47] "removeSparseTerms"       "removeWords"            
   [49] "scan_tokenizer"          "SimpleCorpus"           
   [51] "SimpleSource"            "stemCompletion"         
   [53] "stemDocument"            "stepNext"               
   [55] "stopwords"               "stripWhitespace"        
   [57] "TermDocumentMatrix"      "termFreq"               
   [59] "Terms"                   "tm_filter"              
   [61] "tm_index"                "tm_map"                 
   [63] "tm_parLapply"            "tm_parLapply_engine"    
   [65] "tm_reduce"               "tm_term_score"          
   [67] "URISource"               "VCorpus"                
   [69] "VectorSource"            "weightBin"              
   [71] "WeightFunction"          "weightSMART"            
   [73] "weightTf"                "weightTfIdf"            
   [75] "writeCorpus"             "XMLSource"              
   [77] "XMLTextDocument"         "Zipf_plot"              
   [79] "ZipSource"
   #+end_example

5) Display a list of all datasets contained in the ~tm~ package:
   #+begin_src R
     data(package="tm")
   #+end_src

   #+RESULTS:
   : Data sets in package 'tm':
   : 
   : acq                     50 Exemplary News Articles from the
   :                         Reuters-21578 Data Set of Topic acq
   : crude                   20 Exemplary News Articles from the
   :                         Reuters-21578 Data Set of Topic crude

6) After turning a CSV file into a dataframe, then into a source, then
   into a volatile corpus ~list~, what kind of output do you get when
   entering the name of the corpus?
   #+begin_src R
     <<create_sms_corpus>>
     sms_corpus # output is meta data only
   #+end_src

   #+RESULTS:
   : <<VCorpus>>
   : Metadata:  corpus specific: 0, document level (indexed): 0
   : Content:  documents: 5559

7) How many characters does message no. 444 from ~sms_corpus~ have?
   #+begin_src R
     nchar(content(sms_corpus[[444]]))  # nested version
     sms_corpus[[444]] |> content() |> nchar()   # pipeline version
   #+end_src

   #+RESULTS:
   : [1] 47
   : [1] 47

8) What is the average number of characters of all messages from
   ~sms_corpus~? (Use: ~lapply~, ~nchar~ and ~mean~)
   #+begin_src R
     ## apply content to each list element
     all_msg <- lapply(sms_corpus, content)
     ## count number of chars with nchar
     all_msg_chars <- nchar(all_msg)
     ## compute the mean across the vector
     mean(all_msg_chars, na.rm=TRUE)
                                             # nested:
     mean(nchar(lapply(sms_corpus, content)))
                                             # pipeline:
     sms_corpus |>
       lapply(FUN=content) |>
       nchar() |>
       mean(na.rm=TRUE)
   #+end_src

   #+RESULTS:
   : [1] 79.77136
   : [1] 79.77136
   : [1] 79.77136

9) How can you remove all numbers from the message no. 200?
   #+begin_src R
     ## the wrapper tm_map only works for
     content(sms_corpus[[200]])
     content(removeNumbers(sms_corpus[[200]]))
   #+end_src

   #+RESULTS:
   : [1] "WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"
   : [1] "WELL DONE! Your * Costa Del Sol Holiday or £ await collection. Call  Now toClaim. SAE, TCs, POBox, Stockport, SKxh, Cost£./pm, Maxmins"

10) Could you also use the wrapper function ~tm_map~ to remove the
    numbers from an individual message?
    #+begin_src R
      ## tm_map only accepts the whole corpus as data argument!
      tm_map(sms_corpus,removeNumbers) -> sms_corpus_no_numbers
      content(sms_corpus[[200]])
      content(sms_corpus_no_numbers[[200]])
    #+end_src

    #+RESULTS:
    : [1] "WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"
    : [1] "WELL DONE! Your * Costa Del Sol Holiday or £ await collection. Call  Now toClaim. SAE, TCs, POBox, Stockport, SKxh, Cost£./pm, Maxmins"

11) What is ~stopwords("german")~ and how can you look at its end?
    #+begin_src R
      ## stopwords("de") is character vector of small, irrelevant German
      ## words used to clean a German text.
      tail(stopwords("de"))
    #+end_src

    #+RESULTS:
    : [1] "würden"   "zu"       "zum"      "zur"      "zwar"     "zwischen"

12) Is the word "Your" in ~stopwords("en")~? How would you find out?
    #+begin_src R
      any(stopwords("en")=="your")
    #+end_src

    #+RESULTS:
    : [1] TRUE

13) Remove all English stopwords and "Your" from the SMS corpus and
    check the result for SMS message no. 200.
    #+begin_src R
      ## removing English stopwords + "Yours" from sms_corpus
      content(sms_corpus[[200]])   # original msg
      content(tm_map(sms_corpus,
                     removeWords,
                     c(stopwords("en"),"Your"))[[200]])
    #+end_src

    #+RESULTS:
    : [1] "WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"
    : [1] "WELL DONE!  4* Costa Del Sol Holiday  £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"

14) What does "Porter's algorithm" from the ~SnowballC~ package do?
    #+begin_notes
    Answer: it breaks down all words into root + affixes using a set
    of rules. The algorithm doesn't find the roots of all words
    though! For example it does not recognize that 'learner' comes
    from 'learn'.
    #+end_notes

15) Why does the Naive Bayes algorithm benefit from a clean text corpus?
    #+begin_notes
    In Naive Bayes, probabilities are attached to every feature, and
    conditional probabilities of the predictors are computed for
    classification using Bayes' formula. For a textual dataset, the
    features/predictors are individual words - the fewer words we have
    to account for, the fewer computations need to be carried out!
    #+end_notes

16) Does the order of text mining operations matter for the final
    result? And what is the format of the final result?
   #+begin_notes
    Answer: yes, it does matter. The DocumentTermMatrix function,
    which creates the final result, a frequency table of documents
    (rows) vs. terms (columns/words) uses a different stopwords
    dictionary, and therefore we have a different number of
    features. In Naive Bayes, this is especially relevant since ALL
    information is used that enters the final probabilities
    computation.
    #+end_notes

** What did you learn this week?

Tue:
1. The chatbot narrative has an arc of 60 years (1963-2023)
2. Some think LLMs "degrade science and debase ethics"
3. People who aren't evil can do evil (Arendt's "Banality of Evil")

Thu:
1. How to save and load a complete interactive R session
2. Run an R file in batch mode (R CMD BATCH)
3. How to do text mining (review)

* Week 12: Finish the spam filter for SMS messages
#+attr_latex: :width 400px
#+caption: Photo by Jonathan Borba on Unsplash.
[[../img/regression.jpg]]

/Image: Regression towards the mean./

*Why not?*
- Complete [[https://lyon.instructure.com/courses/1021/assignments/9560][test 4]] by Friday for partial credit (60%).
- Complete test 5 by Tuesday next week for full points.

*Useful:*
- Feed [[https://lyon.instructure.com/calendar][Canvas calendar]] to your Google calendar ([[https://community.canvaslms.com/t5/Student-Guide/How-do-I-subscribe-to-the-Calendar-feed-using-Google-Calendar-as/ta-p/535][instructions]])

*Next: finish Naive Bayes*
- Wordcloud visualization
- Training and test data set creation
- Dimensionality reduction
- Transforming feature values to categories
- Model evaluation and improvement

*Interesting:*
- [[https://www.symmetrymagazine.org/article/lhc-experiments-see-four-top-quarks][Quark quartet observed at CERN using ML methods]]
  #+begin_quote
   “With machine learning, applications like Photoshop can map the
   background of a digital photograph in a much more complex way. But
   in this case, the signal and background can look so similar that
   it’s less like removing an unsightly parking lot from a selfie, and
   more like isolating a specific person in a grainy picture taken
   during a massive concert. “We had such a large and complicated
   background that we couldn’t simulate it; we had to use the
   data. This was the first time CMS used machine learning to estimate
   a data-driven background.”
  #+end_quote

** Naive Bayes: model training and testing
#+attr_latex: :width 300px
#+caption: Testpilot Lt John A. MacReady (Source: Flickr.com/LOC)
[[../img/testing.jpg]]


- Load the ~.RData~ file directly [[https://github.com/birkenkrahe/ml/blob/main/data/.RData][from GitHub]] (*bit.ly/ml_rdata*) into
  your current R session and check that packages and user-defined
  objects were loaded (make sure the location is correct - this ~load~
  command assumes ~.RData~ is in the same directory as this file):
  #+begin_src R
    load(".RData")
    search()
    ls()
  #+end_src

- Now: continue with your practice file ~5_naive_bayes_practice.org~

* Week 13: Introduction to regression with examples
#+attr_latex: :width 400px
#+caption: linear and logistic regression
[[../img/6_linear.png]]
[[../img/6_logistic.png]]

*Housekeeping*
- [X] Submit your job fair experience by tonight for extra credit
- [X] Test 5 is online - for full points complete by end of this week
- [X] Only 65% completed the last DataCamp assignment!?

*Topics*
- [X] Short introduction to simple and multiple linear regression
- [X] Case study: Challenger space shuttle risk analysis
- [X] Case study: Predicting medical expenses

*Outlook*
- [X] [[https://lyon.instructure.com/courses/1021/assignments/9904][3rd sprint review for projects ("abstract") due April 21]]
- [X] Week 14: Artificial Neural Networks (unsupervised learning)
- [X] Final presentations Tue 25-Apr, Thu 27-Apr, Tue 2-May ([[https://lyon.instructure.com/courses/1021/pages/sign-up-for-final-presentations][sign up]])

** Regression case study "Star Surgeon"
#+attr_latex: :width 400px
#+caption: Sci-Fi novel cover "Star Surgeon" by James White (1963)
[[../img/star_surgeon.jpg]]

** Further study: regression at DataCamp
#+attr_latex: :width 300px
#+caption: Source: Wikipedia ROC entry
[[../img/zumel.png]]

- DataCamp offers *14 different courses on regression techniques* in R
- The basic course is "[[https://app.datacamp.com/learn/courses/introduction-to-regression-in-r][Introduction to Regression in R]]" (w/Tidyverse)
- The best course is "[[https://app.datacamp.com/learn/courses/supervised-learning-in-r-regression][Supervised learning in R: regression]]" - its
  creators also wrote "Practical Data Science with R" (Manning, 2019).
- [[https://app.datacamp.com/learn/projects/552][DataCamp guided project "Clustering Heart Disease Patient Data"]]

** Test review questions: regression
#+attr_latex: :width 400px
#+caption: Source: Wikipedia ROC entry
[[../img/Roc_curve.svg]]

1) *Causation:* what's our basic assumption when modeling data?

2) Correlation: what's our main tool to establish it?
   #+begin_notes
   The Pearson correlation coefficient defined as
   Var(x,y)/(sd(x)sd(y)) - indicating how x,y grow/fall together
   #+end_notes
3) What's the downside of measuring correlation?
   #+begin_notes
   Only measures linear relationships.
   #+end_notes
4) What are the possible values for the correlation coefficient?

5) What's logistic regression for?
   #+begin_notes
   Answer: binary classification (categorical features)
   #+end_notes
6) What's an ROC curve, and what is AUC?
   #+begin_notes
   - Rationale: when outcomes are very rare, predicting the opposite
     can result in a very high accuracy - you may need to reduce
     accuracy in order to capture rare events
   - ROC = "Receiver Operating Characteristic" originally developed
     for operators of military radar receivers, a plot of the true
     positive rate against the false positive rate to establish hits.
   - AUC = "Area Under the [ROC] Curve": measures the area under the
     ROC curve as a quantifier of the predictive hitrate - a random
     model (50:50) has an AUC of 0.5, a perfect model AUC=1.0
   #+end_notes
7) Which R function is used to model logistic regression?
   #+begin_notes
   The ~glm~ function (or family of functions - it's a generic
   function), for 'generalised linear model', with the ~family~ argument
   set to ~"binomial"~.
   #+end_notes
8) What is forward or backward stepwise regression?
   
9) Why 'regression'?
   #+begin_notes
   Historically: Galton's experiment showed a 'regression to the mean'
   of the height of sons of fathers. Practically: all ML models use
   functions to estimate similarity in comparison to a normal
   distribution.
   #+end_notes
10) What does 0 correlation mean? What does it not mean?
    #+begin_notes
    For linearly correlated samples, it shows independence of the
    features or all events connected to them (measured by them). For
    other samples (with non-linear effects), it shows nothing.
    #+end_notes
11) What is "imputation" (as opposed to "amputation") in regression?
    #+begin_notes
    A way to deal with missing values by estimating values for them,
    e.g. the mean of all non-missing values. You need to add another
    column as an indicator if imputation was performed.
    #+end_notes

** Exciting Zoom meeting with UAMS today
#+attr_latex: :width 400px
[[../img/uams.png]]

- They especially need computer science and data science students
- Computing and infrastructure skills are often missing
- They have resources including the Nat'l Cancer Imaging Archive
- You need R and Python (SQL less relevant here): "Python is pitiful
  when you want to use random forest".
- Building better models for imaging purposes is challenging both
  computationally, conceptually and mathematically

* What did you learn this week?

Tue:
- Understanding linear regression
- Pearson correlation coefficient
- Simple vs. multiple linear regression

Thu: 
- Creating and interpreting correlation and scatterplot matrix - all
  features correlated against one another.
- Special scatterplot matrix with ~psych::pairs.panels~ showed
  histograms, numerical correlations, correlation ellipses and loess
  curves (local polynomial regression around minima and maxima)
- Modeling with ~stats::lm~ and formula ~y ~ .~, ~y ~ x1 + x2~, ~y ~ x1 * x2~
  #+begin_example R
    m <- lm(y ~ ., data=df) ## use all features
    m <- lm(y ~ x1 + x2, data=df) ## use features x1 and x2 independently
    m <- lm(y ~ x1 * x2, data=df) ## use both x1 and x2 
  #+end_example
- Prediction with ~stats::predict~
  #+begin_example R
    p <- predict(model, test) # test data as (unseen) input for model
  #+end_example
- Factor features are automatically dummy-coded with the first level
  used as reference category.

* IN PROGRESS Week 14: Trees & Regression & ANN
#+attr_latex: :width 400px
#+caption: Gnarled tree by Mustang Joe @ Flickr.com (Public Domain)
[[../img/tree.jpg]]

- [X] [[https://lyon.instructure.com/courses/1021/pages/sign-up-for-final-presentations][Three teams have not yet signed up in Canvas]]: no matter, present
  what you have achieved - we're not looking for perfection but for
  diligence - do you know the difference?[fn:2]
- [X] Last two DataCamp assignments are *bonus* assignments (complete by
  May 3rd)
- [X] Test preview/review questions: decision trees
- [ ] What we did last week
- [ ] Finish linear regression case study: improving/predicting
- [ ] Course summary and outlook

* TODO [[https://github.com/birkenkrahe/org/blob/master/grading/Presentation_Assessment_Form.pdf][Rubric for evaluating your presentations]]
#+attr_latex: :width 400px
#+caption: Photo: US office of war information, 1944 (Flickr.com)
[[../img/presentation.jpg]]

- The rubric covers all aspects of a successful presentation
- Test the technology before presenting - the devil's in the detail!
- Zip all materials and upload them to Canvas no later than May 3rd
- If I don't get your material by that time, you won't get any points

* TODO What did you learn this week?

Tue: ???

Thu: ???

* TODO ML - too large for its own good?
#+attr_latex: :width 400px
#+caption: Robert Wadlow, with his family in Alton, Ill.
[[../img/12_Robert_Wadlow.png]]

** Summary

- Supervised methods (kNN, Naive Bayes, Decision Trees, Linear and
  Logistic Regression) result in clear understanding of the
  relationships between the input and the output.

- Unsupervised Deep Learning ("neural net") methods are extremely
  powerful as classifiers that can be applied in both categorical and
  numeric prediction problems but they result in
  nearly-incomprehensible models.

- Unsupervised learning methods also include clustering methods
  originally from signal processing that partition observations into
  clusters. This is useful for image recognition and natural language
  processing (sentiment analysis).

- All ML methods share the same workflow. Just like for other crafts
  like Data Science (and Computer Science), you can find your own
  niche depending on what's you greatest skill and/or passion.

- For example, if you're interested in [[https://www.cancerimagingarchive.net/publications/][cancer imaging ML]], you can
  focus on: image storing, image classification algorithms, legal and
  ethical aspects of data, image processing optimization, process
  mining for clinical pathways, software systems supporting residents,
  clinical trials data gathering, data annotation and labelling,
  algorithm design, algorithm debugging and improvement, and more.

** What could have been better?

- A proper (practice-oriented) course on machine learning requires one
  term each for supervised and unsupervised methods.

- More guidance for projects, perhaps including a portfolio session on
  how to capitalize on term projects for internships and job interviews?

- Leave one session for reviewing the final exam questions?

- Let me know what you liked and what you didn't like: fill in the
  survey ([[https://lyon.instructure.com/courses/1021/assignments/10076]["CourseEval"@Canvas]]) by May 4, 5:59 pm (*bonus points*)

** What could you do next to learn?

1) Fill in the course survey (bonus points) by May 5, 5:59 pm.
2) Work through some good books (see below)
3) Learn Python, too (see below) and especially SQL (DataCamp)
4) Complete plenty of [fun] projects and document them
5) Participate in the 2023 Arkansas Research Summer Institute 
6) Remember: what you'll apply will grow, what you won't use will
   wither on the vine.
-----

- The Art of Machine Learning by Norman Matloff (NoStarch, 2023) is
  shaping up to be the best book for ML with R (and ML altogether):
  #+attr_latex: :width 200px
  #+caption: Cover, The Art of Machine Learning by N Matloff (NoStarch, 2023)
  [[../img/matloffTAOML.png]]

- Deep learning with R -  (Manning, 2022)
  #+attr_latex: :width 200px
  #+caption: Cover, Deep learning with R by Chollet/Allaire (Manning, 2022)
  [[../img/cholletallaire.png]]

- [[https://www.manning.com/books/grokking-machine-learning][Grokking machine learning (Manning, 2022)]]
  #+attr_latex: :width 200px
  #+caption: Cover, Grokking machine learning by Serrano (Manning, 2022)
  [[../img/serrano.png]]

- Learn Python (@DataCamp | Summer I: CSC 109 @Lyon)
  #+attr_latex: :width 200px
  #+caption: Cover, Automate the boring stuff with Python by Sweigart (NoStarch, 2019)
  [[../img/python.jpg]]

- Improve your modeling skills (Fall 23: CSC/DSC 482.02)  
  #+attr_latex: :width 200px
  #+caption: Cover, Modeling and Simulation in Python by Downey (NoStarch, 2022)
  [[../img/modeling.png]]

** What could you do next to get a job?

- ​I thought these tips were quite good though I personally favor a
  more *surgical*​, less of a mass-mailing​ ​​approach​​.​​But I also have not
  been in the tech professional job market for a ​few years​, and I've
  not worked in the US since 2002​. But the tips strike me as useful no
  matter what: [[https://link.medium.com/pLGMURvz1yb][How To Improve Your Job Search With 4 Quick Tips]]
  1) Apply to a lot of jobs
  2) Practice interviewing at tier 2 firms
  3) Adjust your resume every 20 or so applications
  4) Practice everything
  5) Fortune favors the bold

- Above and beyond, what you need in this field (CompSci and Data
  Science) is a set of GOALS and a SYSTEM of continuous improvement to
  meet these goals.

- Example GOAL: "I want to be able to build and interpret a predictive
  linear regression model for a suitable large dataset, because I know
  that this method is hugely popular and can be applied in all
  industries."

- Example SYSTEM components:
  1) *One-off:* complete DataCamp course on regression in R.
  2) *Routine:* practice daily on ALL completed DataCamp courses.
  3) *Routine:* read recent papers on regression applications beginning
     with literature reviews [[https://pdfs.semanticscholar.org/5d7c/b4891c73ad8ec642e4d8ab2220bb52a64086.pdf][like this one]]. Look for niche
     applications.
  4) *Routine*: find experts and join their communities on Twitter,
     Discord, Substack or Medium (e.g. Cassie Kozyrkov/Google, Andrei
     Karpathy/Tesla, Andrew Ng/DeepLearning.AI).
  5) *Repeatedly:* pull datasets off Kaggle or UCI and go through the ML
     workflow on your own using an interactive notebook (for
     sharing). Especially useful if you have a feeling/knowledge for
     the data (e.g. sports data if you're an athlete).

* DONE [[https://docs.google.com/forms/d/e/1FAIpQLSdLkJ65AaAsx-g2_cGj2y3SXwmpz_HdH9mr86QDKGqZBkSNPA/viewform][2023 Arkansas Summer Research Institute]]
#+attr_latex: :width 400px
#+caption: Cat on Sidewalk (1959, Angelo Rizzuto) Flickr.com
[[../img/practice.jpg]]

- Great opportunity to network and learn
- Free and virtual
- Only 2 weeks long (June 1-19)
- Share link with others: https://tinyurl.com/apply2023asri

* DONE DataCamp review questions: decision trees
#+attr_latex: :width 400px
#+caption: DataCamp professional certificate for your resume
[[../img/certificate.png]]

1) What does a decision tree, e.g. with a credit approval model that
   was trained on loan amount and credit score as independent
   variables, give you?
   #+begin_quote
   That depends on the dependent variable: you can create a decision
   tree for any categorical outcome variable for which you have
   historical data. In the DataCamp lesson, the tree was built to
   predict loan outcome as "default" or "repaid".
   #+end_quote
   #+begin_src R
     str(loans)
   #+end_src

   #+RESULTS:
   #+begin_example
   'data.frame':	39732 obs. of  16 variables:
    $ keep              : int  1 1 0 0 0 0 0 1 1 1 ...
    $ rand              : num  0.13 0.998 0.628 0.252 0.474 ...
    $ default           : int  0 1 0 0 0 0 0 0 1 1 ...
    $ loan_amount       : chr  "LOW" "LOW" "LOW" "MEDIUM" ...
    $ emp_length        : chr  "10+ years" "< 2 years" "10+ years" "10+ years" ...
    $ home_ownership    : chr  "RENT" "RENT" "RENT" "RENT" ...
    $ income            : chr  "LOW" "LOW" "LOW" "MEDIUM" ...
    $ loan_purpose      : chr  "credit_card" "car" "small_business" "other" ...
    $ debt_to_income    : chr  "HIGH" "LOW" "AVERAGE" "HIGH" ...
    $ credit_score      : chr  "AVERAGE" "AVERAGE" "AVERAGE" "AVERAGE" ...
    $ recent_inquiry    : chr  "YES" "YES" "YES" "YES" ...
    $ delinquent        : chr  "NEVER" "NEVER" "NEVER" "MORE THAN 2 YEARS AGO" ...
    $ credit_accounts   : chr  "FEW" "FEW" "FEW" "AVERAGE" ...
    $ bad_public_record : chr  "NO" "NO" "NO" "NO" ...
    $ credit_utilization: chr  "HIGH" "LOW" "HIGH" "LOW" ...
    $ past_bankrupt     : chr  "NO" "NO" "NO" "NO" ...
   #+end_example
   
2) Which features could be used to build a decision tree to decide on
   a job offer?
   #+begin_quote
   Any attributes of the job such as: salary, commuting time, company
   car, travel time, remote work, etc.
   #+end_quote

3) What kind of algorithm is used to build a decision tree?
   #+attr_latex: :width 400px
   [[../img/divide_and_conquer.png]]
   
4) Which group of data points would this algorithm prioritize?
   #+begin_quote
   Divide-and-conquer always looks to create the split resulting in
   the greatest improvement to purity - it favors splits that result
   in the greater homogeneity of a subgroup.
   #+end_quote
   
5) What's a downside when using decision trees for classification?
   #+begin_quote
   - Splits are always axis-parallel (only 1 feature at a time)
   - Trees can become very complex and large very quickly
   - The decision tree model may overfit the data
   - An overfitted tree models noise rather than signals
   - Similar to too small k values in the kNN model
   #+end_quote

6) How can you check if a decision tree is a good classifier?
   #+begin_quote
   You simulate unseen data by constructing a test data set, e.g. by
   holding out a small random portion of the full dataset and
   comparing the peformance on training vs. test dataset - it should
   be comparable.
   #+end_quote
   
7) How can you split a data frame randomly into two for training and
   testing? E.g. by 75% for training and 25% for testing.
   #+begin_src R
     loans <- read.csv("~/Documents/GitHub/ml/data/loans.csv",
                       header=TRUE)
     dim(loans)
     ## sample() pulls 'size' elements x with/out replacement 
     ## and with a probability weight default NULL (normal)
     args(sample)
     ## Determine the number of rows for training
     nrow(loans)
     ## Create a random sample of row IDs
     sample_rows <- sample(nrow(loans), 0.75*nrow(loans))
     head(sample_rows,10)
     ## Create the training dataset
     loans_train <- loans[sample_rows,]
     dim(loans_train)
     ## Create the test dataset
     loans_test <- loans[-sample_rows,]
     dim(loans_test)
     ## test and train datasets ought to add up to full dataset
     dim(loans)[1]-dim(loans_test)[1]-dim(loans_train)[1]
   #+end_src

   #+RESULTS:
   : [1] 39732    16
   : function (x, size, replace = FALSE, prob = NULL) 
   : NULL
   : [1] 39732
   :  [1] 28188 38394 12611 35354 36449 10161 27286  3964 13207 34350
   : [1] 29799    16
   : [1] 9933   16
   : [1] 0

8) What is the workflow once you have training and test data, if you
   want to check a decision tree model?
   #+begin_example R
   ## grow a tree using all available features on TRAINING data
   model <- rpart(outcome ~ .,
                  data = [training data],
                  method = "class")
   ## predict on the TEST dataset 
   prediction <- predict(model,
                         newdata = [test data],
                         type = "class")
   ## evaluate: confusion matrix predicted vs. actual values
   table(prediction, [full data]$outcome)

   ## accuracy as average difference predicted vs. test values
   mean(prediction == [test data]$outcome)
   
   Example output for loan data and outcome "default" or "repaid":
   :           default repaid
   :   default    3501   1356
   :   repaid     1542   3534
   : [1] 0.7082452
   #+end_example
   
9) When is a decision tree model pruned?
   #+attr_latex: :width 400px
   #+caption: error rate vs. complexity for a decision tree model
   [[../img/pruning.png]]
   #+begin_quote
   Below the dotted line, the error rate becomes statistically similar
   to the most complex decision tree model - additional branches of
   the model can not reduce the error (or increase accuracy).
   #+end_quote

10) What is a "random forest" model?
    #+begin_quote
    A growing algorithm that grows a large number of decision trees in
    which both the features and examples (records) may differ from
    tree to tree. Implemented in R with ~randomForest::randomForest~.
    #+end_quote

* TODO Week 15: Projects I
#+attr_latex: :width 400px
#+caption: Double cluster in Perseus by Stephen Rahn@Flickr.com (Public Domain)
[[../img/cluster.jpg]]

** DataCamp review questions: k-means clustering
Extras:
- [[https://app.datacamp.com/learn/projects/584][Guided DataCamp HR project "Degrees that pay you back"]]
- [[https://pieriantraining.com/k-means-clustering-machine-learning-in-python/][Tutorial blog post: k-means clustering in Python]] (3/17/2023)

* TODO Week 16: Projects II and Closing
#+attr_latex: :width 400px
#+caption: The Olympic Games, Year 2020 (1970s Sci-Fi Art on Twitter)
[[../img/future.jpg]]

** DataCamp review questions: hierarchical clustering

* TODO What next?
* Resources

- Import the CSV data and save them to a data frame ~sms_raw~. Do not
  automatically convert ~character~ to ~factor~ vectors. Use the
  appropriate function arguments:
  #+name: load_sms_raw
  #+begin_src R :results silent
    ## save CSV data as data frame sms_raw
    sms_raw <- read.csv(file="https://bit.ly/sms_spam_csv",
                        header=TRUE,
                        stringsAsFactors=FALSE)
  #+end_src
- Three steps lead from a data frame with text to a corpus:
  1) Isolate the text vector
  2) Turn the vector into a source
  3) Turn the source into a corpus
  4) Check that the corpus is there
  #+name: create_sms_corpus
  #+begin_src R
    sms_corpus <- VCorpus(VectorSource(sms_raw$text))
  #+end_src

* References

- Gavagsaz, E (May 2022). Efficient Parallel Processing of
  k-Nearest Neighbor Queries by Using a Centroid-based and
  Hierarchical Clustering Algorithm, DOI: 10.30564/aia.v4i1.4668

- Goldin, S @freeCodeCamp.org (Sep 9, 2023). Google Like a Pro –
  All Advanced Search Operators Tutorial [2023 Tips]. Online:
  [[https://youtu.be/BRiNw490Eq0][youtube.com]].

- Lopp, S (Apr 4, 2019). R for Enterprise: Understanding R's
  Startup. In: R Views. Online: [[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][rviews.rstudio.com]].

- Stokes, J (Jan 4, 2023). The Fourth Age Of Programming
  [Blog]. URL: [[https://blog.replit.com/fourth][blog.repolit.com]]

- Waters, R (Feb 19, 2023). Man beats machine at Go in human
  victory over AI. URL: [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][arstechnica.com]].

- Worsley, S (Mar 2022). What is R? The Statistical Computing
  Powerhouse. [[https://www.datacamp.com/blog/all-about-r][Online: datacamp.com/blog.]]

- Ying, K @freeCodeCamp.org (Sep 26, 2022). Machine Learning for
  Everybody - Full Course. Online: [[https://youtu.be/i_LwzRVP7bg][youtube.com]].

- Zumel, N and Mount J (2010). Practical Data Science with R
  (2e). Manning. Online: [[https://www.manning.com/books/practical-data-science-with-r-second-edition][manning.com]].

- Photos by: Ben Mullins, Katarzyna Pe, Benjamin Davies, Grovemade,
  Feliphe Schiarolli, Avin CP, Andrew Neel, Jonathan Borba on
  Unsplash.com

* Footnotes
[fn:2]It's the difference between being flawless (which is impossible)
and being flawed (which is normal) and knowing your flaws so that you
can improve on them.

[fn:1]It is possible that the existence of an ~.RData~ file converts
warnings about packages you're trying to load into errors, which stops
the packages from being loaded. In this case /erase/ the ~.RData~ file and
restart R.
