#+TITLE: AGENDA - MACHINE LEARNING
#+AUTHOR: MARCUS BIRKENKRAHE (pledged)
#+SUBTITLE: Machine Learning (DSC 305) Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results: output :exports both :noweb yes
* Week 1 - Course overview & ML overview
#+attr_latex: :width 500px
[[../img/cover.jpg]]

[[https://unsplash.com/@kasiape][Photo by Katarzyna Pe at Unsplash.com]]

- [X] Course overview - assignments, grading, topics, platforms
- [X] Interesting article: "[[https://blog.replit.com/fourth][The Fourth Age of Programming]]"
- [X] Interesting webinar: "[[https://www.datacamp.com/webinars/2023-data-trends-and-predictions][Data Trends & Predictions 2023]]" report
  ([[https://github.com/birkenkrahe/ds2/blob/main/pdf/data_trends_2023.pdf][PDF]]) (Wed Jan 18, 10AM) - register/attend/watch the recording later
- [ ] Introduction to machine learning
- [X] First DataCamp assignment "Understanding machine learning"

* Week 2 - Introduction to ML & Review of R (Part I)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] Introduction to machine learning (continued)
- [X] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on +Thursday, Jan-19+ Jan-24
- [X] First DataCamp home assignment "What is Machine Learning" Jan-19
- [X] First in-class practice "Introduction to machine learning"

Recommendations:
- [X] "Lyon Data & Analytics Nexus" on Discord (invitation only)
- [X] [[https://youtu.be/BRiNw490Eq0][Google search operators tutorial]] (video, 1 hr) => project
- [X] [[https://youtu.be/i_LwzRVP7bg][ML for everybody by FreeCodeCamp]] (video, 4 hrs) => Python

** Lyon data science nerd network (invitation only)
#+attr_latex: :width 400px
#+caption: Lyon Data & Analytics Nexus Discord server dashboard
[[../img/discord.png]]

- Join the discord server to grow your professional network
- Lyon alumni, students, and companies are invited
- Share internship and job opportunities, and more

** Google search for programmers (Goldin, 2023)
#+attr_latex: :width 400px
#+caption: Google search tutorial by Seth Goldin (freeCodeCamp)
[[../img/goldin.png]]

Waste of time?
1) Searching is not finding
2) Finding is not understanding
3) Understanding is not using

*Google's business model = data + algorithm + infrastructure + brand*
1) data are offered by users for free
2) algorithm is largely known and public
3) infrastructure is proprietary and hard to duplicate
4) brand is unique and highly valuable (but also vulnerable)

ChatGPT's answer to "how do I search using Google effectively?"
#+begin_quote
Here are a few tips for searching effectively using Google:

1. Use quotation marks to search for an exact phrase. For
   example, searching for ~"chocolate chip cookies"~ will return pages
   that include that exact phrase, rather than pages that include the
   words ~"chocolate"~, ~"chip"~, and ~"cookies"~ separately.

2. Use a minus sign to exclude certain words from your search. For
   example, searching for ~chocolate chip cookies -recipe~ will return
   pages that include ~chocolate chip cookies~ but exclude any pages
   that include the word "~recipe~".

3. Use the ~site:~ operator to search within a specific website. For
   example, searching for ~site:nytimes.com chocolate chip cookies~ will
   return pages on the New York Times website that include the words
   "~chocolate chip cookies~."

4. Use the ~filetype:~ operator to search for specific file types. For
   example, searching for chocolate chip cookies ~filetype:pdf~ will
   return PDF documents that include the words "~chocolate chip
   cookies~".

5. Use the ~related:~ operator to find websites that are related to a
   specific website. For example, searching for ~related:nytimes.com~
   will return websites that are related to the New York Times
   website.

6. Use the Google [[https://www.google.com/advanced_search][advanced search page]] for more complex queries.

Keep in mind that Google's search algorithm is constantly evolving, so
these tips may not always return the most accurate results.
#+end_quote

*Caveat:* professional programmers use aggregators instead
- [[https://stackoverflow.com/][stackoverflow.com]] with sub-groups like [[https://chess.stackexchange.com/questions/41282/computers-hardware-or-software][chess.stackexchange.com]]
- [[https://en.wikipedia.org/wiki/Internet_Relay_Chat][Internet Relay Chat]] - in Emacs (~M-x erc~), e.g. [[https://libera.chat/][libera.chat]]
- [[https://www.linuxtoday.com/developer/how-to-use-emacs-as-a-usenet-reader-with-gnus/][Usenet]], mailing lists, Slack, Discord, reddit, quora etc.

** Rapid review - intro to ML
#+attr_latex: :width 400px
#+caption: Photo by Benjamin Davies on Unsplash
[[../img/rapid_review.jpg]]

1) Most popular OS for ML?

2) Which tools are we using in this course?

3) What are the deliverables for your project?

4) Where do you get the topic for your term project?

5) What are the steps for a supervised learning process?

6) What is the general ML process?

* Week 3 - Introduction to ML & Review of R (Part II)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] "Doctor, Doctor!"
- [X] Condition for repeating tests
- [X] Test 1 review in class
- [X] Project 1st sprint review
- [X] Introduction to machine learning (continued)
- [ ] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on Tuesday 24-Jan
- [X] 2nd DataCamp home assignment "Machine Learning Models" by Jan-31
- [X] In-class practice "Introduction to machine learning" (continued)
- [X] Exercise: build an ML code glossary
- [ ] In-class practice "R data structures"

** DONE [[https://lyon.instructure.com/courses/1021/discussion_topics/2125][Jan 26 session online]]
#+attr_latex: :width 400px
[[../img/zoom.png]]

** DONE Doctor, Doctor!
#+attr_latex: :width 400px
[[../img/couch.jpg]]

- ~M-x doctor~

** DONE Project - first sprint review (Tuesday, Feb 13)
#+attr_latex: :width 400px
[[../img/sprint.jpg]]

- The term project purpose is down to you - or you can approach me for
  an idea (but please do that sooner than later)!

- There are many different ways to present a paper or a slide pack:
  e.g. succinct, verbose, opinionated, objective, accessible or not.

- Your first deliverable is a *project proposal* formatted as an
  Org-mode file. Here is a template for such a file with definitions.

  1) Meta data: preliminary title (~#+TITLE~), list of team members
     (~#+AUTHOR:~), course title (~#+SUBTITLE~).

  2) Headlines: ~Problem~, ~Reason~, ~Constraints~, ~Goals and Non-goals~,
     ~Metrics~, ~References~

     - *Problem*: describe the problem that you're trying to solve.

     - *Reason*: why is this problem interesting (to you) right now?

     - *Constraints:* which difficulties, e.g. technical or conceptual,
       do you foresee right now?

     - *Goals and non-goals*: list all goals that you might want to
       achieve with this project. Order the goals by importance. Add a
       list of non-goals, i.e. things that are outside the scope of
       your project.

     - *Metrics*: how would you measure the success of your project?

     - *References*: list any references that you found already. Make
       sure that they are consistent (same format) and complete
       (author, date, title, place).

- Upload your result to Canvas (*no later than Feb 13*). Missing
  the deadline will cost you points.

Source: Ellis, Data Science Project Proposals (2021). URL:
[[https://crunchingthedata.com/data-science-project-proposals/][crunchingthedata.com]].

** DONE Condition for repeating tests
#+attr_latex: :width 400px
#+caption: Late or missed the test? Talk to me!
[[../img/late.jpg]]

- If you inform me beforehand that you cannot attend an announced
  test, we can make arrangements for you to take the test outside of
  class.

** DONE [[https://lyon.instructure.com/courses/1021/assignments/6737/edit?quiz_lti][Test 1 review]]
** DONE How should you study for data science tests?
#+attr_latex: :width 200px
[[../img/studying.jpg]]

- If you were successful in the test: what did you do?
- If not: what do you think you should have done?

* Week 4 - Icestorm
#+attr_latex: :width 400px
[[../img/icestorm.jpg]]

* Week 5 - Review of R (Part III)
#+attr_latex: :width 400px
[[../img/TourDeFrance.jpg]]

** DONE Upload practice files 2 GDrive (or 2 stick)
#+attr_latex: :width 400px
[[../img/gdrive.png]]

#+attr_latex: :width 400px
[[../img/sticks.jpeg]]

- Make a bootable Linux stick (e.g. [[https://linuxmint-installation-guide.readthedocs.io/en/latest/burn.html][Linux Mint]]), too

** DONE Create an ~.Rprofile~

- Create or open ~~/.Rprofile~ and add these lines to it:
  #+begin_example R
  options(repos=c("https://cloud.r-project.org/"))
  options(crayon.enabled = FALSE)
  message("*** Loaded .Rprofile ***")
  #+end_example

- Save the file and start an R console to test it (you should also see
  the ~Loaded~ message:
  #+begin_src R
    options()$repos
  #+end_src

- From now on, Windows will no longer ask you to choose a mirror site,
  and you will be able to display a "~tibble~" (a sort of data frame
  popular in the "Tidyverse") in Emacs:
  #+begin_src R :results output
    library(MASS)
    tibble(mtcars)
  #+end_src

** DONE Test 2 opens Friday - closes Tuesday Feb 14
#+attr_latex: :width 400px
#+caption: Image by Grovemade via Unsplash.com
[[../img/homeoffice.jpg]]

- This test will be about machine learning models (lecture) and about
  the review of R (in-class assignment).

- You can complete the test at your leisure (within the set time
  limit) but you must complete it before the deadline (Tue-14-Feb)

** DONE Home assignments - project and DataCamp
#+attr_latex: :width 400px
#+caption: Image by Feliphe Schiarolli via Unsplash.com
[[../img/classroom.jpg]]

1) First sprint review: proposal as Org-mode file [[https://lyon.instructure.com/courses/1021/assignments/7108][in Canvas (Feb 13)]]

2) Complete a whole DataCamp course on supervised learning ([[https://lyon.instructure.com/courses/1021/assignments/7387][Mar 24]])
   - Classification using nearest neighbors
   - Classification using Naive Bayes
   - Classifiction using Logistic Regression (curve fitting)
   - Classifiction using Decision Trees

3) We'll review the DataCamp sessions in class and in the tests (I'll
   announce tests 1 week in advance)

** DONE R review: structures/management/exploration
#+attr_latex: :width 250px
[[../img/rlogo.png]]

1) Data structures in R
2) Managing data in R
3) Exploring data in R

[Source: [[https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864][Lantz, Machine learning with R (3e), chapter 2]]]

- Download raw ~2_R_structure_practice.org~ [[https://raw.githubusercontent.com/birkenkrahe/ml/main/org/2_R_structure_practice.org][from GitHub (birkenkrahe/ml)]]
- Open a CMD line terminal (Windows search: CMD, Mac: terminal)
- Navigate to the download directory with ~cd~
- Open the file in a terminal Emacs (can you take this command apart?)
  #+begin_example sh
    emacs -nw --file 2_R_structure_practice.org
  #+end_example
  #+attr_latex: :width 400px
  [[../img/emacs.png]]
- Finish practice files started in class on your own by the deadline

** READ Understanding R startup
#+attr_latex: :width 400px
#+caption: From "R for Enterprise: Understanding R's Startup (Lopp, 2019)
[[../img/rstartup.jpeg]]

[[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][Here is an article]] (Lopp, 2019) on R startup variables and
settings. Includes an explanation why the ~.Rprofile~ startup file was
not read when some of you opened the R console in the shell (you
should probably try ~Rgui~ on the command line, too).

** NEW Get bonus points when practicing
#+attr_latex: :width 200px
[[../img/datacamp2.png]]
- You can get 10 bonus points if you keep a practice streak of 10 days
- You can do this up to 3 times for a maximum of 30 points, which will
  be applied to your weakest final grade category
- Submit a screenshot of your mobile (or desktop) streak in Canvas
- If you lose your streak between day 5 and 10, you still get 5 points
- On the dashboard, DataCamp will suggest practice categories for you,
  and also in the mobile app
- This option ends on May 3rd (last day of spring term)
- You can get this bonus only in one of my courses (if you attend > 1)
#+attr_latex: :width 400p
[[../img/datacamp3.png]]

** NEW GNU Treats: ~speed-type~, ~treemacs~ and ~gtypist~

- An attractive alternative to ~Dired~ is the ~treemacs~ package. It
  looks like this on my PC (and also works for the terminal Emacs):
  #+attr_latex: :width 400px
  [[../img/t_treemacs.png]]

- If you want to be faster on the keyboard, try [[https://www.gnu.org/savannah-checkouts/gnu/gtypist/gtypist.html#:~:text=GNU%20Typist%20(also%20called%20gtypist,the%20GNU%20General%20Public%20License.][GNU Typist]], a free
  10-lesson online trainer for increasing your typing skills.
  #+attr_latex: :width 400px
  [[../img/gtypist.png]]

- There is also an Emacs package to practice touch/speed typing in
  Emacs called ~speed-type~. You have to install it with ~M-x
  package-list-packages~, then find the package in the list and install
  with ~i~ and ~x~. [[https://github.com/dakra/speed-type][More information on GitHub.]]

* Week 6 - ML models overview
#+attr_latex: :width 400px
[[../img/ml_types2.png]]

- [X] Remember to upload practice (there are deadlines) [[https://lyon.instructure.com/courses/1021/assignments][to Canvas]]
- [X] Open ~2_R_explore_practice.org~ and load the data
- [X] Let's finish the review and upload the completed file to Canvas
- [X] What is R? Good overview [[https://www.datacamp.com/blog/all-about-r][in this DataCamp blog post]] (05/22)
- [X] [[https://lyon.instructure.com/courses/1021/assignments/6997][Test 2 (open book) is live online until Fri 17-Feb, 11:59 pm]].

** Review: exploring numerical data structure:

Open an Org-mode file if you want to code along.

1) [X] How can you get an overview of statistical information for ~Nile~?
   #+begin_src R :results output
     summary(Nile)
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :   456.0   798.5   893.5   919.4  1032.5  1370.0

2) [X] What about the ~time~ of the ~Nile~ observations?
   #+begin_src R :results output
     summary(time(Nile))
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :    1871    1896    1920    1920    1945    1970

3) [X] How many values of ~Nile~ are outliers? Which values are these?
   #+begin_quote
   *Update:* as the computation of ~IQR*1.5~ for ~Nile~ shows, there are no
   outliers in the data set - the boxplot (below) shows this,
   too. Tukey's method (~car::outlier~) is not applicable to time
   series, alas.
   #+end_quote

   Various tests:
   #+begin_src R :results output
     IQR(Nile) * 1.5 # standard outlier value
     length(Nile > (IQR(Nile) * 1.5))
     scale(Nile) # z-score method
     Nile[which(Nile > (IQR(Nile) * 1.5))]
   #+end_src
   #+begin_src R :results output
     library(car)
     any(ls('package:car')=="outlier")
                                             # outlierTest(Nile)  ## not applicable to time series
     library(qdap)
     outlier_detect(as.character(Nile))
   #+end_src
   #+begin_src R
     car::outlierTest(Nile)
     ## outlier.test(c(1,2,3,1000))
   #+end_src
4) [ ] How would you make a boxplot of the values of ~Nile~?
   #+begin_src R :results graphics file output :file ../img/Rbox.png
     boxplot(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rbox.png]]

5) [ ] How would you visualize how often certain values of ~Nile~ occur?
   #+begin_src R :results graphics file output :file ../img/Rhist.png
     hist(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rhist.png]]

6) [ ] How many observations of the ~Nile~ flow are between 800 and 1000
   mio cubic metres?
   #+begin_src R
     length(
       Nile[
         which(
           Nile > 800 & Nile < 1000
         )
       ]
     )
   #+end_src

   #+RESULTS:
   : 44

** AI's Bid for Freedom
#+attr_latex: :width 400px
[[../img/freedom.png]]

- My courses for Fall 2023:

  1) Artificial Intelligence (DSC 482.01) - seminar. The 2023 edition
     will include deep learning with R/Python and more code poetry.

  2) Data and process modeling (DSC 482.02) - seminar. The 2023
     edition will include exploring differential equations with R

  3) Introduction to data science (DSC 105): The 2023 edition will
     include R and Python.

- All courses are for everybody who's interested in data science: tell
  your friends!

** Project proposal feedback in the classroom
** You can submit an improved proposal by Fri-Feb-24

* Week 7: k-Nearest-Neighbor (k-NN) algorithms
#+attr_latex: :width 400px
#+caption: Photo by Avin CP on Unsplash
[[../img/tomato.jpg]]
5_ductal_carcinoma.jpg
- [X] Test 2 now open for late attempts (partial credit)

- [X] [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][Man beats machine at Go in human victory over AI]] (Waters, 2023)
  #+begin_quote
  "The discovery of a weakness in some of the most advanced Go-playing
  machines points to a fundamental flaw in the deep-learning systems
  that underpin today’s most advanced AI [...]: The systems can
  “understand” only specific situations they have been exposed to in
  the past and are unable to *generalize* in a way that humans find
  easy. It shows once again we’ve been far too hasty to ascribe
  superhuman levels of intelligence to machines.” - Russell[fn:1]
  #+end_quote

- [-] k-nearest-neighbor (knn) models:
  1) [X] Lecture (HTML, Org-mode or [[https://github.com/birkenkrahe/ml/blob/main/md/4_knn.md][Markdown]] with notes in GitHub)
  2) [X] Practice (demo: cancer classification with k-NN)
  3) [ ] Exercise (bonus assignment in R)
  4) [ ] Test 3 (opens next week)

* Week 8: k-NN case study - cancer prediction (cont'd)
#+attr_latex: :width 400px
#+caption: Ductal Carcinoma in situ (Source: jhp.edu)
[[../img/5_ductal_carcinoma.jpg]]

** NEXT Test 3 preview

*When prompted, write down your answer or keywords to the answer.*

1) [ ] How is similarity measured in k-NN?
   #+begin_notes
   By computing a distance measure, e.g. Euclidean distance, the usual
   distance between two vectors, see ~dist~.
   #+end_notes
2) [ ] What is used for "training" and "testing" the k-NN classifier?
   #+begin_notes
   Two data sets, one known, labelled data to train the model on a
   label, the other one unseen, unlabelled data to test the trained
   model. The training and test data should be of comparable
   quality and randomized.
   #+end_notes
3) [ ] What is the meaning of the parameter 'k'? (4)
   #+begin_notes
   - The number of nearest neigbors after computing distance
   - A measure of the size of the classification neighborhood
   - A measure for the degree of over- or underfitting
   - An argument in the ~class::knn~ function
   #+end_notes
4) [ ] What are the arguments of ~knn~? (4)
   #+begin_src R :results output
     args(knn)
   #+end_src
5) [ ] What is the purpose of splitting the data in training and test
   data? Why don't we just use ALL available data for training?
   #+begin_notes
   - To avoid overfitting: without testing, it is not clear to which
     extent the model will generalize to unseen data. We split one
     data set because the data are of comparable quality/structure.
   #+end_notes
6) [ ] What happens when voting is tied after measuring the distances?
   #+begin_notes
   In ~class::knn~, the class is decided at random. Use odd ~k~ to avoid this.
   #+end_notes
7) [ ] If I have 10 features q_{1}...q_{10} and 1 target label
   - How many dimensions does the feature space have?
   - How many terms under the square root of the distance formula?
   - How many variables are used for classification of p?
   #+begin_notes
   - 10 features, hence 10-dim feature space
   - Each point has 10 coordinates. The distance formula has 10 terms
     p_{i}-q_{ij}, one for each i-th coordinate of q_{j}, and 11
     variables are involved: 10 features (predictors) and 1 target
     variable (predicted).
   #+end_notes
8) [ ] Is a bigger ~k~ always better? Why or why not?
   #+begin_notes
   Optimal k depends on noise in the data, on the pattern to be
   identified, one starts off with the square root of the number of
   training examples, to find the optimal k, try different values.
   #+end_notes
9) [ ] Measuring model performance:
   - What is the confusion matrix? What's its *dimension*?
   - How can you *compute* the confusion matrix for kNN in R?
   - How can you assess the accuracy *numerically*?
   #+begin_notes
   - The confusion matrix is a 2 x 2 cross-table if the target feature
     had 2 labels/categories (e.g. male/female or benign/malignant),
     or a 3 x 3 for 3 labels. It shows, which levels were accurately
     identified and which were not (solution).
     #+attr_latex: :width 400px
     [[../img/confusion.png]]
   - With the ~table~ function and the ~factor~ levels or ~character~
     values as arguments, e.g. ~table(signs_pred,signs_actual)~.
   - With the ~mean~ function by averaging over the vector resulting
     from comparing original and predicted values:
     ~mean(sign_actual==~sign_pred)~.
   #+end_notes
10) [ ] What does bias-tradeoff refer to?
    #+attr_latex: :width 400px
    [[../img/bias_tradeoff.png]]
    #+begin_notes
    The image shows subsequent classification attempts. Perfect
    prediction corresponds to the bulls-eye. The further away a point
    is, the worse the prediction.
    - Bias leads to a systematic prediction error, e.g. because of
      existing patterns in the data, or other redundancies.
    - Low bias (upper row) means low error and high bias (lower row)
      means large error.
    - Variance corresponds to spread. Low variance (left column)
      leaves the results together, high variance blows them up.
    - The best case is low bias and low variance: low error, points
      close together, the worst is high bias and high variance.
    #+end_notes

** NEXT Continue: kNN case study
#+attr_latex: :width 600px
#+caption: k-NN workflow (Gavagsaz, 2022)
[[../img/knn.jpg]]

- If you missed the last session, download ~5_knn_case_practice.org~
  fromGitHub and head to the chapter *Intermission* to catch up!

- *We hope to cover today:*
  1) Normalizing (rescaling) numeric data
  2) Creating training and test data sets
  3) Training a model on the data
  4) Evaluating model performance
  5) Exercises: improve the model! (bonus)

** NEXT assignment - submit by March 7
#+attr_latex: :width 400px
#+caption: Photo by Andrew Neel, Unsplash.com
[[../img/assignment.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8405][Next assignment: NAIVE BAYES method - deadline MARCH 7, 11:59 pm]]

** NEXT test - submit by March 7
#+attr_latex: :width 400px
#+caption: Photo by Ben Mullins, Unsplash.com
[[../img/0_test.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8789/edit?quiz_lti][Next test (topic: kNN) is live now until Tuesday, MARCH 7, 11:59pm.]]


* TODO Week 9: Naive Bayes
* TODO Week 10: Logistic regression
* TODO Week 11: Classification trees
* TODO Week 12: k-means

- [[https://app.datacamp.com/learn/projects/584][Guided DataCamp HR project "Degrees that pay you back"]]

* TODO Week 13: Hierarchical clustering

- [[https://app.datacamp.com/learn/projects/552][DataCamp guided project "Clustering Heart Disease Patient Data"]]

* TODO Week 14: Dimensionality reduction with PCA
* TODO Week 15: Case study: breast cancer data
* TODO Week 16: Project presentations
* TODO What next?

- Deep learning with R -  (Manning, 2022)
  #+attr_latex: :width 200px
  #+caption: Cover, Deep learning with R by Chollet/Allaire (Manning, 2022)
  [[../img/cholletallaire.png]]

- [[https://www.manning.com/books/grokking-machine-learning][Grokking machine learning (Manning, 2022)]]
  #+attr_latex: :width 200px
  #+caption: Cover, Grokking machine learning by Serrano (Manning, 2022)
  [[../img/serrano.png]]

- Learn Python (@DataCamp | CSC 109 @Lyon)
  #+attr_latex: :width 200px
  #+caption: Cover, Automate the boring stuff with Python by Sweigart (Manning, 2022)
  [[../img/python.jpg]]


* References

- Gavagsaz, Elaheh (May 2022). Efficient Parallel Processing of
  k-Nearest Neighbor Queries by Using a Centroid-based and
  Hierarchical Clustering Algorithm, DOI: 10.30564/aia.v4i1.4668

- Goldin, Seth @freeCodeCamp.org (Sep 9, 2023). Google Like a Pro –
  All Advanced Search Operators Tutorial [2023 Tips]. Online:
  [[https://youtu.be/BRiNw490Eq0][youtube.com]].

- Lopp, Sean (Apr 4, 2019). R for Enterprise: Understanding R's
  Startup. In: R Views. Online: [[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][rviews.rstudio.com]].

- Stokes, Jon (Jan 4, 2023). The Fourth Age Of Programming
  [Blog]. URL: [[https://blog.replit.com/fourth][blog.repolit.com]]

- Waters, Richard (Feb 19, 2023). Man beats machine at Go in human
  victory over AI. URL: [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][arstechnica.com]].

- Worsley, S (Mar 2022). What is R? The Statistical Computing
  Powerhouse. [[https://www.datacamp.com/blog/all-about-r][Online: datacamp.com/blog.]]

- Ying, K. @freeCodeCamp.org (Sep 26, 2022). Machine Learning for
  Everybody - Full Course. Online: [[https://youtu.be/i_LwzRVP7bg][youtube.com]].

* Footnotes
[fn:2]The data sets are listed at the bottom of all DataCamp courses,
e.g. "[[https://app.datacamp.com/learn/courses/supervised-learning-in-r-classification][Supervised learning in R]]" the link opens to the [[https://app.datacamp.com/learn/courses/supervised-learning-in-r-classification][raw CSV file]],
which you can either download and read the CSV file from your PC, or
use the URL as ~filename~. If you get a "cannot open connection" error,
check the URL or try again.

[fn:1]Russell is a coauthor of AIMA, one of the textbooks we're going
to use for my fall seminar on "Artificial Intelligence" (DSC/CSC
482.01).
