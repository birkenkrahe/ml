#+TITLE: AGENDA - MACHINE LEARNING
#+AUTHOR: MARCUS BIRKENKRAHE (pledged)
#+SUBTITLE: Machine Learning (DSC 305) Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results: output :exports both :noweb yes
* Week 1 - Course overview & ML overview
#+attr_latex: :width 500px
[[../img/cover.jpg]]

[[https://unsplash.com/@kasiape][Photo by Katarzyna Pe at Unsplash.com]]

- [X] Course overview - assignments, grading, topics, platforms
- [X] Interesting article: "[[https://blog.replit.com/fourth][The Fourth Age of Programming]]"
- [X] Interesting webinar: "[[https://www.datacamp.com/webinars/2023-data-trends-and-predictions][Data Trends & Predictions 2023]]" report
  ([[https://github.com/birkenkrahe/ds2/blob/main/pdf/data_trends_2023.pdf][PDF]]) (Wed Jan 18, 10AM) - register/attend/watch the recording later
- [ ] Introduction to machine learning
- [X] First DataCamp assignment "Understanding machine learning"

* Week 2 - Introduction to ML & Review of R (Part I)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] Introduction to machine learning (continued)
- [X] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on +Thursday, Jan-19+ Jan-24
- [X] First DataCamp home assignment "What is Machine Learning" Jan-19
- [X] First in-class practice "Introduction to machine learning"

Recommendations:
- [X] "Lyon Data & Analytics Nexus" on Discord (invitation only)
- [X] [[https://youtu.be/BRiNw490Eq0][Google search operators tutorial]] (video, 1 hr) => project
- [X] [[https://youtu.be/i_LwzRVP7bg][ML for everybody by FreeCodeCamp]] (video, 4 hrs) => Python

** Lyon data science nerd network (invitation only)
#+attr_latex: :width 400px
#+caption: Lyon Data & Analytics Nexus Discord server dashboard
[[../img/discord.png]]

- Join the discord server to grow your professional network
- Lyon alumni, students, and companies are invited
- Share internship and job opportunities, and more

** Google search for programmers (Goldin, 2023)
#+attr_latex: :width 400px
#+caption: Google search tutorial by Seth Goldin (freeCodeCamp)
[[../img/goldin.png]]

Waste of time?
1) Searching is not finding
2) Finding is not understanding
3) Understanding is not using

*Google's business model = data + algorithm + infrastructure + brand*
1) data are offered by users for free
2) algorithm is largely known and public
3) infrastructure is proprietary and hard to duplicate
4) brand is unique and highly valuable (but also vulnerable)

ChatGPT's answer to "how do I search using Google effectively?"
#+begin_quote
Here are a few tips for searching effectively using Google:

1. Use quotation marks to search for an exact phrase. For
   example, searching for ~"chocolate chip cookies"~ will return pages
   that include that exact phrase, rather than pages that include the
   words ~"chocolate"~, ~"chip"~, and ~"cookies"~ separately.

2. Use a minus sign to exclude certain words from your search. For
   example, searching for ~chocolate chip cookies -recipe~ will return
   pages that include ~chocolate chip cookies~ but exclude any pages
   that include the word "~recipe~".

3. Use the ~site:~ operator to search within a specific website. For
   example, searching for ~site:nytimes.com chocolate chip cookies~ will
   return pages on the New York Times website that include the words
   "~chocolate chip cookies~."

4. Use the ~filetype:~ operator to search for specific file types. For
   example, searching for chocolate chip cookies ~filetype:pdf~ will
   return PDF documents that include the words "~chocolate chip
   cookies~".

5. Use the ~related:~ operator to find websites that are related to a
   specific website. For example, searching for ~related:nytimes.com~
   will return websites that are related to the New York Times
   website.

6. Use the Google [[https://www.google.com/advanced_search][advanced search page]] for more complex queries.

Keep in mind that Google's search algorithm is constantly evolving, so
these tips may not always return the most accurate results.
#+end_quote

*Caveat:* professional programmers use aggregators instead
- [[https://stackoverflow.com/][stackoverflow.com]] with sub-groups like [[https://chess.stackexchange.com/questions/41282/computers-hardware-or-software][chess.stackexchange.com]]
- [[https://en.wikipedia.org/wiki/Internet_Relay_Chat][Internet Relay Chat]] - in Emacs (~M-x erc~), e.g. [[https://libera.chat/][libera.chat]]
- [[https://www.linuxtoday.com/developer/how-to-use-emacs-as-a-usenet-reader-with-gnus/][Usenet]], mailing lists, Slack, Discord, reddit, quora etc.

** Rapid review - intro to ML
#+attr_latex: :width 400px
#+caption: Photo by Benjamin Davies on Unsplash
[[../img/rapid_review.jpg]]

1) Most popular OS for ML?

2) Which tools are we using in this course?

3) What are the deliverables for your project?

4) Where do you get the topic for your term project?

5) What are the steps for a supervised learning process?

6) What is the general ML process?

* Week 3 - Introduction to ML & Review of R (Part II)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] "Doctor, Doctor!"
- [X] Condition for repeating tests
- [X] Test 1 review in class
- [X] Project 1st sprint review
- [X] Introduction to machine learning (continued)
- [ ] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on Tuesday 24-Jan
- [X] 2nd DataCamp home assignment "Machine Learning Models" by Jan-31
- [X] In-class practice "Introduction to machine learning" (continued)
- [X] Exercise: build an ML code glossary
- [ ] In-class practice "R data structures"

** DONE [[https://lyon.instructure.com/courses/1021/discussion_topics/2125][Jan 26 session online]]
#+attr_latex: :width 400px
[[../img/zoom.png]]

** DONE Doctor, Doctor!
#+attr_latex: :width 400px
[[../img/couch.jpg]]

- ~M-x doctor~

** DONE Project - first sprint review (Tuesday, Feb 13)
#+attr_latex: :width 400px
[[../img/sprint.jpg]]

- The term project purpose is down to you - or you can approach me for
  an idea (but please do that sooner than later)!

- There are many different ways to present a paper or a slide pack:
  e.g. succinct, verbose, opinionated, objective, accessible or not.

- Your first deliverable is a *project proposal* formatted as an
  Org-mode file. Here is a template for such a file with definitions.

  1) Meta data: preliminary title (~#+TITLE~), list of team members
     (~#+AUTHOR:~), course title (~#+SUBTITLE~).

  2) Headlines: ~Problem~, ~Reason~, ~Constraints~, ~Goals and Non-goals~,
     ~Metrics~, ~References~

     - *Problem*: describe the problem that you're trying to solve.

     - *Reason*: why is this problem interesting (to you) right now?

     - *Constraints:* which difficulties, e.g. technical or conceptual,
       do you foresee right now?

     - *Goals and non-goals*: list all goals that you might want to
       achieve with this project. Order the goals by importance. Add a
       list of non-goals, i.e. things that are outside the scope of
       your project.

     - *Metrics*: how would you measure the success of your project?

     - *References*: list any references that you found already. Make
       sure that they are consistent (same format) and complete
       (author, date, title, place).

- Upload your result to Canvas (*no later than Feb 13*). Missing
  the deadline will cost you points.

Source: Ellis, Data Science Project Proposals (2021). URL:
[[https://crunchingthedata.com/data-science-project-proposals/][crunchingthedata.com]].

** DONE Condition for repeating tests
#+attr_latex: :width 400px
#+caption: Late or missed the test? Talk to me!
[[../img/late.jpg]]

- If you inform me beforehand that you cannot attend an announced
  test, we can make arrangements for you to take the test outside of
  class.

** DONE [[https://lyon.instructure.com/courses/1021/assignments/6737/edit?quiz_lti][Test 1 review]]
** DONE How should you study for data science tests?
#+attr_latex: :width 200px
[[../img/studying.jpg]]

- If you were successful in the test: what did you do?
- If not: what do you think you should have done?

* Week 4 - Icestorm
#+attr_latex: :width 400px
[[../img/icestorm.jpg]]

* Week 5 - Review of R (Part III)
#+attr_latex: :width 400px
[[../img/TourDeFrance.jpg]]

** DONE Upload practice files 2 GDrive (or 2 stick)
#+attr_latex: :width 400px
[[../img/gdrive.png]]

#+attr_latex: :width 400px
[[../img/sticks.jpeg]]

- Make a bootable Linux stick (e.g. [[https://linuxmint-installation-guide.readthedocs.io/en/latest/burn.html][Linux Mint]]), too

** DONE Create an ~.Rprofile~

- Create or open ~~/.Rprofile~ and add these lines to it:
  #+begin_example R
  options(repos=c("https://cloud.r-project.org/"))
  options(crayon.enabled = FALSE)
  message("*** Loaded .Rprofile ***")
  #+end_example

- Save the file and start an R console to test it (you should also see
  the ~Loaded~ message:
  #+begin_src R
    options()$repos
  #+end_src

- From now on, Windows will no longer ask you to choose a mirror site,
  and you will be able to display a "~tibble~" (a sort of data frame
  popular in the "Tidyverse") in Emacs:
  #+begin_src R :results output
    library(MASS)
    tibble(mtcars)
  #+end_src

** DONE Test 2 opens Friday - closes Tuesday Feb 14
#+attr_latex: :width 400px
#+caption: Image by Grovemade via Unsplash.com
[[../img/homeoffice.jpg]]

- This test will be about machine learning models (lecture) and about
  the review of R (in-class assignment).

- You can complete the test at your leisure (within the set time
  limit) but you must complete it before the deadline (Tue-14-Feb)

** DONE Home assignments - project and DataCamp
#+attr_latex: :width 400px
#+caption: Image by Feliphe Schiarolli via Unsplash.com
[[../img/classroom.jpg]]

1) First sprint review: proposal as Org-mode file [[https://lyon.instructure.com/courses/1021/assignments/7108][in Canvas (Feb 13)]]

2) Complete a whole DataCamp course on supervised learning ([[https://lyon.instructure.com/courses/1021/assignments/7387][Mar 24]])
   - Classification using nearest neighbors
   - Classification using Naive Bayes
   - Classifiction using Logistic Regression (curve fitting)
   - Classifiction using Decision Trees

3) We'll review the DataCamp sessions in class and in the tests (I'll
   announce tests 1 week in advance)

** DONE R review: structures/management/exploration
#+attr_latex: :width 250px
[[../img/rlogo.png]]

1) Data structures in R
2) Managing data in R
3) Exploring data in R

[Source: [[https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864][Lantz, Machine learning with R (3e), chapter 2]]]

- Download raw ~2_R_structure_practice.org~ [[https://raw.githubusercontent.com/birkenkrahe/ml/main/org/2_R_structure_practice.org][from GitHub (birkenkrahe/ml)]]
- Open a CMD line terminal (Windows search: CMD, Mac: terminal)
- Navigate to the download directory with ~cd~
- Open the file in a terminal Emacs (can you take this command apart?)
  #+begin_example sh
    emacs -nw --file 2_R_structure_practice.org
  #+end_example
  #+attr_latex: :width 400px
  [[../img/emacs.png]]
- Finish practice files started in class on your own by the deadline

** READ Understanding R startup
#+attr_latex: :width 400px
#+caption: From "R for Enterprise: Understanding R's Startup (Lopp, 2019)
[[../img/rstartup.jpeg]]

[[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][Here is an article]] (Lopp, 2019) on R startup variables and
settings. Includes an explanation why the ~.Rprofile~ startup file was
not read when some of you opened the R console in the shell (you
should probably try ~Rgui~ on the command line, too).

** NEW Get bonus points when practicing
#+attr_latex: :width 200px
[[../img/datacamp2.png]]
- You can get 10 bonus points if you keep a practice streak of 10 days
- You can do this up to 3 times for a maximum of 30 points, which will
  be applied to your weakest final grade category
- Submit a screenshot of your mobile (or desktop) streak in Canvas
- If you lose your streak between day 5 and 10, you still get 5 points
- On the dashboard, DataCamp will suggest practice categories for you,
  and also in the mobile app
- This option ends on May 3rd (last day of spring term)
- You can get this bonus only in one of my courses (if you attend > 1)
#+attr_latex: :width 400p
[[../img/datacamp3.png]]

** NEW GNU Treats: ~speed-type~, ~treemacs~ and ~gtypist~

- An attractive alternative to ~Dired~ is the ~treemacs~ package. It
  looks like this on my PC (and also works for the terminal Emacs):
  #+attr_latex: :width 400px
  [[../img/t_treemacs.png]]

- If you want to be faster on the keyboard, try [[https://www.gnu.org/savannah-checkouts/gnu/gtypist/gtypist.html#:~:text=GNU%20Typist%20(also%20called%20gtypist,the%20GNU%20General%20Public%20License.][GNU Typist]], a free
  10-lesson online trainer for increasing your typing skills.
  #+attr_latex: :width 400px
  [[../img/gtypist.png]]

- There is also an Emacs package to practice touch/speed typing in
  Emacs called ~speed-type~. You have to install it with ~M-x
  package-list-packages~, then find the package in the list and install
  with ~i~ and ~x~. [[https://github.com/dakra/speed-type][More information on GitHub.]]

* Week 6 - ML models overview
#+attr_latex: :width 400px
[[../img/ml_types2.png]]

- [X] Remember to upload practice (there are deadlines) [[https://lyon.instructure.com/courses/1021/assignments][to Canvas]]
- [X] Open ~2_R_explore_practice.org~ and load the data
- [X] Let's finish the review and upload the completed file to Canvas
- [X] What is R? Good overview [[https://www.datacamp.com/blog/all-about-r][in this DataCamp blog post]] (05/22)
- [X] [[https://lyon.instructure.com/courses/1021/assignments/6997][Test 2 (open book) is live online until Fri 17-Feb, 11:59 pm]].

** Review: exploring numerical data structure:

Open an Org-mode file if you want to code along.

1) [X] How can you get an overview of statistical information for ~Nile~?
   #+begin_src R :results output
     summary(Nile)
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :   456.0   798.5   893.5   919.4  1032.5  1370.0

2) [X] What about the ~time~ of the ~Nile~ observations?
   #+begin_src R :results output
     summary(time(Nile))
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :    1871    1896    1920    1920    1945    1970

3) [X] How many values of ~Nile~ are outliers? Which values are these?
   #+begin_quote
   *Update:* as the computation of ~IQR*1.5~ for ~Nile~ shows, there are no
   outliers in the data set - the boxplot (below) shows this,
   too. Tukey's method (~car::outlier~) is not applicable to time
   series, alas.
   #+end_quote

   Various tests:
   #+begin_src R :results output
     IQR(Nile) * 1.5 # standard outlier value
     length(Nile > (IQR(Nile) * 1.5))
     scale(Nile) # z-score method
     Nile[which(Nile > (IQR(Nile) * 1.5))]
   #+end_src
   #+begin_src R :results output
     library(car)
     any(ls('package:car')=="outlier")
                                             # outlierTest(Nile)  ## not applicable to time series
     library(qdap)
     outlier_detect(as.character(Nile))
   #+end_src
   #+begin_src R
     car::outlierTest(Nile)
     ## outlier.test(c(1,2,3,1000))
   #+end_src
4) [ ] How would you make a boxplot of the values of ~Nile~?
   #+begin_src R :results graphics file output :file ../img/Rbox.png
     boxplot(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rbox.png]]

5) [ ] How would you visualize how often certain values of ~Nile~ occur?
   #+begin_src R :results graphics file output :file ../img/Rhist.png
     hist(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rhist.png]]

6) [ ] How many observations of the ~Nile~ flow are between 800 and 1000
   mio cubic metres?
   #+begin_src R
     length(
       Nile[
         which(
           Nile > 800 & Nile < 1000
         )
       ]
     )
   #+end_src

   #+RESULTS:
   : 44

** AI's Bid for Freedom
#+attr_latex: :width 400px
[[../img/freedom.png]]

- My courses for Fall 2023:

  1) Artificial Intelligence (DSC 482.01) - seminar. The 2023 edition
     will include deep learning with R/Python and more code poetry.

  2) Data and process modeling (DSC 482.02) - seminar. The 2023
     edition will include exploring differential equations with R

  3) Introduction to data science (DSC 105): The 2023 edition will
     include R and Python.

- All courses are for everybody who's interested in data science: tell
  your friends!

** Project proposal feedback in the classroom
** You can submit an improved proposal by Fri-Feb-24

* Week 7: k-Nearest-Neighbor (k-NN) algorithms
#+attr_latex: :width 400px
#+caption: Photo by Avin CP on Unsplash
[[../img/tomato.jpg]]
5_ductal_carcinoma.jpg
- [X] Test 2 now open for late attempts (partial credit)

- [X] [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][Man beats machine at Go in human victory over AI]] (Waters, 2023)
  #+begin_quote
  "The discovery of a weakness in some of the most advanced Go-playing
  machines points to a fundamental flaw in the deep-learning systems
  that underpin today’s most advanced AI [...]: The systems can
  “understand” only specific situations they have been exposed to in
  the past and are unable to *generalize* in a way that humans find
  easy. It shows once again we’ve been far too hasty to ascribe
  superhuman levels of intelligence to machines.” - Russell[fn:1]
  #+end_quote

- [-] k-nearest-neighbor (knn) models:
  1) [X] Lecture (HTML, Org-mode or [[https://github.com/birkenkrahe/ml/blob/main/md/4_knn.md][Markdown]] with notes in GitHub)
  2) [X] Practice (demo: cancer classification with k-NN)
  3) [ ] Exercise (bonus assignment in R)
  4) [ ] Test 3 (opens next week)

* Week 8: k-NN case study - cancer prediction (cont'd)
#+attr_latex: :width 400px
#+caption: Ductal Carcinoma in situ (Source: jhp.edu)
[[../img/5_ductal_carcinoma.jpg]]

** Test 3 preview

Here are [[https://github.com/birkenkrahe/ml/blob/main/org/agenda.org#next-test-3-preview][10 questions]] that preview the next (graded) ML test
*When prompted, write down your answer or keywords to the answer.*

1) [X] How is similarity measured in k-NN?
     #+begin_notes
   By computing a distance measure, e.g. Euclidean distance, the usual
   distance between two vectors, see ~dist~.
     #+end_notes
2) [X] What is used for "training" and "testing" the k-NN classifier?
     #+begin_notes
   Two data sets, one known, labelled data to train the model on a
   label, the other one unseen, unlabelled data to test the trained
   model. The training and test data should be of comparable
   quality and randomized.
     #+end_notes
3) [X] What is the meaning of the parameter 'k'? (4)
     #+begin_notes
   - The number of nearest neigbors after computing distance
   - A measure of the size of the classification neighborhood
   - A measure for the degree of over- or underfitting
   - An argument in the ~class::knn~ function
     #+end_notes
4) [X] What are the arguments of ~knn~? (4)
     #+begin_src R :results output :exports none
     library(class); args(knn)
     #+end_src

  #+RESULTS:
  : function (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) 
  : NULL
5) [X] What is the purpose of splitting the data in training and test
   data? Why don't we just use ALL available data for training?
     #+begin_notes
   - To avoid overfitting: without testing, it is not clear to which
     extent the model will generalize to unseen data. We split one
     data set because the data are of comparable quality/structure.
     #+end_notes
6) [X] What happens when voting is tied after measuring the distances?
     #+begin_notes
   In ~class::knn~, the class is decided at random. Use odd ~k~ to avoid this.
     #+end_notes
7) [X] If I have 10 features q_{1}...q_{10} and 1 target label
   - How many dimensions does the feature space have?
   - How many terms under the square root of the distance formula?
   - How many variables are used for classification of p?
     #+begin_notes
   - 10 features, hence 10-dim feature space
   - Each point has 10 coordinates. The distance formula has 10 terms
     p_{i}-q_{ij}, one for each i-th coordinate of q_{j}, and 11
     variables are involved: 10 features (predictors) and 1 target
     variable (predicted).
     #+end_notes
8) [X] Is a bigger ~k~ always better? Why or why not?
       #+begin_notes
   Optimal k depends on noise in the data, on the pattern to be
   identified, one starts off with the square root of the number of
   training examples, to find the optimal k, try different values.
       #+end_notes
9) [X] Measuring model performance:
   - What is the confusion matrix? What's its *dimension*?
   - How can you *compute* the confusion matrix for kNN in R?
   - How can you assess the accuracy *numerically*?
       #+begin_notes
   - The confusion matrix is a 2 x 2 cross-table if the target feature
     had 2 labels/categories (e.g. male/female or benign/malignant),
     or a 3 x 3 for 3 labels. It shows, which levels were accurately
     identified and which were not ([[https://github.com/birkenkrahe/ml/blob/main/img/confusion1.png][solution]]).
         #+attr_latex: :width 400px
         #+caption: Confusion matrix (empty) and classification results
     [[../img/confusion.png]]
   - With the ~table~ function and the ~factor~ levels or ~character~
     values as arguments, e.g. ~table(signs_pred,signs_actual)~.
   - With the ~mean~ function by averaging over the vector resulting
     from comparing original and predicted values:
     ~mean(sign_actual==sign_pred)~.
       #+end_notes
10) [X] What does bias-tradeoff refer to?
        #+begin_notes
        #+attr_latex: :width 400px
    [[../img/bias_tradeoff.png]]
    The image shows subsequent classification attempts. Perfect
    prediction corresponds to the bulls-eye. The further away a point
    is, the worse the prediction.
    - Bias leads to a systematic prediction error, e.g. because of
      existing patterns in the data, or other redundancies.
    - Low bias (upper row) means low error and high bias (lower row)
      means large error.
    - Variance corresponds to spread. Low variance (left column)
      leaves the results together, high variance blows them up.
    - The best case is low bias and low variance: low error, points
      close together, the worst is high bias and high variance.
        #+end_notes
** Assignment - submit by March 10
#+attr_latex: :width 400px
#+caption: Photo by Andrew Neel, Unsplash.com
[[../img/assignment.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8405][Next assignment: NAIVE BAYES method - deadline MARCH 7, 11:59 pm]]

** Test - submit by March 10
#+attr_latex: :width 400px
#+caption: Photo by Ben Mullins, Unsplash.com
[[../img/0_test.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8789/edit?quiz_lti][Test 3 is live from later today until Friday, MARCH 10, 11:59pm.]]

** Project - 2nd sprint review by March 17

** Continue: kNN case study
#+attr_latex: :width 600px
#+caption: k-NN workflow (Gavagsaz, 2022)
[[../img/knn.jpg]]

- If you missed the last session, *download* ~5_knn_practice.org~
  fromGitHub and head to the chapter *Intermission* to catch up!

- Lecture: ~5_knn_case.org~ & 2 bonus exercises 
  
- *We hope to cover today:*
  1) Normalizing (rescaling) numeric data
  2) Creating training and test data sets
  3) Training a model on the data
  4) Evaluating model performance
  5) Exercises: improve the model! (bonus)

* Week 9: k-NN improvement & Naive Bayes
#+attr_latex: :width 400px
[[../img/amazing-thomas-bayes-illustration.jpg]]

*REMINDERS* (check your Canvas calendar):
- Complete *test 3* by Friday 11:59 pm this week!
- Complete *DataCamp assignment* "Naive Bayes" by Friday this week!
- [[https://lyon.instructure.com/courses/1021/assignments/9116][Literature review (2nd sprint review) by end of next week]]!

*THIS WEEK*:
- Evaluating kNN performance (practice)
- Improving kNN performance (2 x exercise)
- Lecture and practice: supervised learning with Naive Bayes 

*UPCOMING TOPICS*:
- Text mining to build spam filter with Naive Bayes
- Word cloud visualization
- Regression methods: linear and logistic methods
- Regression use case: predicting medical expenses

** DONE Review: kNN case study (continued)
#+attr_latex: :width 400px
[[../img/rapid_review.jpg]]

- [ ] What are we trying to do?
  
- [ ] What does "training a kNN learner" mean?

- [ ] What have we done so far?
  #+begin_quotes
  1) Data collection and storage (data frame)
  2) Exploration of the class/target label and predictors/features
  3) Cleaning, randomization and normalization of the data
  4) Splitting of the dataset into training and test data
  5) Isolating the training and test labels
  6) Running the ~class::knn~ function with set ~k~ value


- [ ] What's left?
  #+begin_quotes
  - Evaluating the performance of the model (confusion matrix, accuracy)
  - Improving the performance (standardization, varying k-values)
  - Running the model on new datasets (prepare data, run ~predict~)
  #+end_quotes
  
** TODO Game programming in Snap! with AI
#+attr_latex: :width 400px
[[../img/snap.png]]

- https://github.com/birkenkrahe/snap (HTML, PDF, Org, md)
- Next fall: [[https://ecraft2learn.github.io/ai/][AI extensions to Snap!]] with [[https://ecraft2learn.github.io/ai/student-projects/][student project examples]]

** TODO Bonus exercise & March assignments
#+attr_latex: :width 400px
[[../img/bonus.png]]

- Remember how to compute the prediction accuracy: as the ~mean~ over
  the values of the original and the predicted ~factor~ vector ([[https://github.com/birkenkrahe/ml/blob/main/org/4_knn_case.org#computing-accuracy-as-an-average][GitHub]])

* IN PROGRESS Week 10: Naive Bayes algorithm and case study
#+attr_latex: :width 400px
#+caption: Illustration by Peter Eich (2006), Wikimedia (CC BY-SA 2.5)
[[../img/5_Spamfilter.jpg]]

- Naive Bayes review and algorithm
- Naive Bayes case study: spam filter
- Naive Bayes test 5 (due March 31)

* DONE Pie Day!
#+attr_html: :width 300px
[[../img/pi.jpg]] [[../img/limerick.png]]

* DONE Results: Test 3 and Tsunami of assignments
#+attr_latex: :width 400px
[[../img/mlTestBox.png]]

- Feel free to complete test 3 by Friday for partial credit (60%)

- [[https://github.com/birkenkrahe/org/blob/master/pdf/midterm.pdf][How to improve your mid-term grades if you choose to do so]]. The link
  to [[https://www.quora.com/Why-are-computer-science-degrees-so-math-intensive-when-the-field-doesnt-seem-to-use-much-math-at-all][this Quora comment]] is not available in the document, alas.
  
- Do not forget that there are *three DataCamp lessons* waiting for
  completion by March 31, 2023: Naive Bayes, Logistic Regression, and
  Classification Trees.

- I'll decide over spring break which of these (if any) we'll pick up
  for detailed treatment in class. In any case: more certificates 4U!
  
* DONE Review: Simple Naive Bayes
#+attr_html: :width 300px
#+caption: Photo by Ben Mullins on Unsplash
[[../img/review.jpg]]

1) Which evidence (in the data) is used for Naive Bayes?
   #+begin_notes
   Answer: NB uses all available data with stochastic weights or
   probabilities attached to individual features.
   #+end_notes

2) For these *events*, what's a suitable *trial* to establish probability?
   - Spam message
   - Lottery win
   - Sunny weather

3) What determines the class our classifier is trying to predict?
   #+begin_notes
   Answer: 1) our observational data (e.g. recorded labels spam
   vs. ham), 2) our problem (e.g. reduce spam)
   #+end_notes

4) Can the same dataset be used to predict different things? For
   example, could a dataset that contains spam vs. ham labels (in
   addition to many other text features) be used for sentiment
   analysis?

5) Why are more trials better for the accuracy of a probabilistic
   prediction?
   #+begin_notes
   Because of the "law of large numbers", which holds for independent
   trials of the same experiment - the expected value (sum of all
   values weighted by their individual probabilities) becomes the
   sample mean (arithmetic average of all values).
   #+end_notes

6) What kind of events are "spam vs. ham", or "win vs. loss", 
   "benign vs. malignant".
   #+begin_notes
   Answer: they are mutually exclusive and exhaustive events, i.e. an
   event is either one or the other but never both, the sum of their
   probabilities adds up to one, and their joint probability is zero.
   #+end_notes
7) What's is the joint probability of the events "being home" and
   "being at the office"?
   #+begin_notes
   Answer: it is zero provided that these events are mutually
   exclusive. 
   #+end_notes
8) What does the conditional probability (or likelihood) of the events
   "being home" and "being at work" mean?
   #+end_notes
9) What is the basis of predictive, probabilistic modeling?
   #+begin_notes
   Answer: dependent events - dependency of events (recorded as
   feature values) means that the occurrence of one event is
   conditional on another: for example, clouds are a condition for
   rain. The probability of rain is conditional on the probability of
   clouds appearing - the latter reveals something about the future of
   the former that we can use to predict the weather. Clouds are
   predictors for rain.
   #+end_notes
10) What does independence of two events imply?
    #+begin_notes
    Answer: knowing something about one event reveals nothing about
    the other event. Neither can be used as a predictor for the
    other. Their joint probability (the probability that they occur
    together) is the product of the individual probabilities.
    #+end_notes
11) You're trying to establish the probability that a message with the
    word "URGENT" in upper case letters is a spam message. What else
    do you need to compute this?
    #+begin_notes
    Answer: according to Bayes' theorem, the conditional probability
    P(spam|URGENT) = P(URGENT|spam) * P(spam) / P(URGENT). You need:
    - P(URGENT|spam): likelihood of finding URGENT in a spam message;
    - P(spam): chance of spam in a given sample of messages (trials)
    - P(URGENT): chance of finding "URGENT" in a sample of messages
    #+end_notes
12) Compute P(spam|URGENT) from the following *frequency* table:
    | FREQUENCY | URGENT=YES | URGENT=NO | TOTAL |
    |-----------+------------+-----------+-------|
    | spam      |          9 |        16 |    25 |
    | ham       |          5 |       120 |   125 |
    |-----------+------------+-----------+-------|
    | Total     |         14 |       136 |   150 |
    #+begin_notes
    Answer: create *likelihood* table - the conditional probabilities
    | LIKELIHOOD | URGENT=YES | URGENT=NO | TOTAL |
    |------------+------------+-----------+-------|
    | spam       | 9/25       | 16/25     |    25 |
    | ham        | 5/125      | 120/125   |   125 |
    |------------+------------+-----------+-------|
    | Total      | 14/150     | 136/150   |   150 |
    
    + ~P(URGENT|spam)~ = 9/25 (likelihood of URGENT in spam)
    + ~P(spam)~ = 25/150 (prior probability - before URGENT)
    + ~P(URGENT)~ = 14/150 (marginal likelihood)
    #+end_notes
    #+begin_src R
      p <- (9/25) * (25/150) / (14/150)
      paste("Posterior probability:",format(p*100,digits=2),"%")
    #+end_src

    #+RESULTS:
    : Posterior probability: 64 %
    
* TODO Reading therapy for spring break: ELIZA
#+attr_latex: :width 400px
[[../img/eliza.png]]

- You will get a print copy of this seminal article to read and
  (perhaps) discuss.

- Cool online implementation on a DEC VT100 terminal at
  [[https://www.masswerk.at/eliza/][masswerk.at/eliza/]]

- ELIZA - A Computer Program For the Study of Natural Language
  Communication Between Man And Machine - Weizenbaum, CACM 9(1)
  (1966):36-46. URL: [[https://dl.acm.org/doi/pdf/10.1145/365153.365168][dl.acm.org]].

- See also: The computational therapeutic: exploring Weizenbaum’s ELIZA as a
  history of the present - Bassett, AI & Society 34
  (2019):803-812. URL: [[https://link.springer.com/article/10.1007/s00146-018-0825-9][link.springer.com]].

* TODO Review: Spam Filter I
#+attr_html: :width 300px
#+caption: Photo by Ben Mullins on Unsplash
[[../img/review1.jpg]]

1) How is the usual ML workflow altered for a message spam filter?
   #+begin_notes
   Answer: the data cleaning includes text formatting, tokenization,
   stemming, stopwords, corpus creation, and a term frequency table.
   #+end_notes
2) How many features does the raw data set have? How many will the
   cleaned data set have?
   #+begin_notes
   Answer: the initial dataset has 2 features - class label (spam,
   ham), and message. The final dataset has as many features as spam
   trigger terms (probably several hundred).
   #+end_notes
3) You've just loaded the R package "tm". How would you check (1) that
   the package is loaded, (2) which functions and (3) which datasets
   (if any) it contains?
   #+begin_src R :results output
     library(tm)   # load package provided it's been installed
     search()  # check package environments search path
     ls('package:tm') # check methods/functions inside tm
     data(package="tm") # list datasets contained in package
     #+end_src

Now go to GitHub, save ~5.2.org~ to ~Downloads~ and insert the file to
your practice file ~5_naive_bayes_practice.org~ with ~C-x i~.

Save the file and run all code blocks: ~M-x org-babel-execute-buffer~,
then check with ~ls~ and ~search~ that all is ready to move on.

* TODO Week 11: Regression
* TODO Week 12: Decision trees
* TODO Week 13: k-means clustering

- [[https://app.datacamp.com/learn/projects/584][Guided DataCamp HR project "Degrees that pay you back"]]

* TODO Week 14: Hierarchical clustering

- [[https://app.datacamp.com/learn/projects/552][DataCamp guided project "Clustering Heart Disease Patient Data"]]

* TODO Week 15: Dimensionality reduction with PCA
* TODO Week 16: Case: Breast Cancer Data once more
* TODO What next?

- Deep learning with R -  (Manning, 2022)
  #+attr_latex: :width 200px
  #+caption: Cover, Deep learning with R by Chollet/Allaire (Manning, 2022)
  [[../img/cholletallaire.png]]

- [[https://www.manning.com/books/grokking-machine-learning][Grokking machine learning (Manning, 2022)]]
  #+attr_latex: :width 200px
  #+caption: Cover, Grokking machine learning by Serrano (Manning, 2022)
  [[../img/serrano.png]]

- Learn Python (@DataCamp | CSC 109 @Lyon)
  #+attr_latex: :width 200px
  #+caption: Cover, Automate the boring stuff with Python by Sweigart (Manning, 2022)
  [[../img/python.jpg]]


* References

- Gavagsaz, Elaheh (May 2022). Efficient Parallel Processing of
  k-Nearest Neighbor Queries by Using a Centroid-based and
  Hierarchical Clustering Algorithm, DOI: 10.30564/aia.v4i1.4668

- Goldin, Seth @freeCodeCamp.org (Sep 9, 2023). Google Like a Pro –
  All Advanced Search Operators Tutorial [2023 Tips]. Online:
  [[https://youtu.be/BRiNw490Eq0][youtube.com]].

- Lopp, Sean (Apr 4, 2019). R for Enterprise: Understanding R's
  Startup. In: R Views. Online: [[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][rviews.rstudio.com]].

- Stokes, Jon (Jan 4, 2023). The Fourth Age Of Programming
  [Blog]. URL: [[https://blog.replit.com/fourth][blog.repolit.com]]

- Waters, Richard (Feb 19, 2023). Man beats machine at Go in human
  victory over AI. URL: [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][arstechnica.com]].

- Worsley, S (Mar 2022). What is R? The Statistical Computing
  Powerhouse. [[https://www.datacamp.com/blog/all-about-r][Online: datacamp.com/blog.]]

- Ying, K. @freeCodeCamp.org (Sep 26, 2022). Machine Learning for
  Everybody - Full Course. Online: [[https://youtu.be/i_LwzRVP7bg][youtube.com]].

- Photos by: Ben Mullins, Katarzyna Pe, Benjamin Davies, Grovemade,
  Feliphe Schiarolli, Avin CP, Andrew Neel, on Unsplash.com
  
* Footnotes
[fn:2]The data sets are listed at the bottom of all DataCamp courses,
e.g. "[[https://app.datacamp.com/learn/courses/supervised-learning-in-r-classification][Supervised learning in R]]" the link opens to the [[https://app.datacamp.com/learn/courses/supervised-learning-in-r-classification][raw CSV file]],
which you can either download and read the CSV file from your PC, or
use the URL as ~filename~. If you get a "cannot open connection" error,
check the URL or try again.

[fn:1]Russell is a coauthor of AIMA, one of the textbooks we're going
to use for my fall seminar on "Artificial Intelligence" (DSC/CSC
482.01).

