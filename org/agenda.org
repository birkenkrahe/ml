#+TITLE: AGENDA - MACHINE LEARNING
#+AUTHOR: MARCUS BIRKENKRAHE (pledged)
#+SUBTITLE: Machine Learning (DSC 305) Spring 2023
#+STARTUP:overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* Week 1: Course overview & ML overview
#+attr_latex: :width 500px
[[../img/cover.jpg]]

[[https://unsplash.com/@kasiape][Photo by Katarzyna Pe at Unsplash.com]]

- [X] Course overview - assignments, grading, topics, platforms
- [X] Interesting article: "[[https://blog.replit.com/fourth][The Fourth Age of Programming]]"
- [X] Interesting webinar: "[[https://www.datacamp.com/webinars/2023-data-trends-and-predictions][Data Trends & Predictions 2023]]" report
  ([[https://github.com/birkenkrahe/ds2/blob/main/pdf/data_trends_2023.pdf][PDF]]) (Wed Jan 18, 10AM) - register/attend/watch the recording later
- [ ] Introduction to machine learning
- [X] First DataCamp assignment "Understanding machine learning"

* Week 2: Introduction to ML & Review of R (Part I)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] Introduction to machine learning (continued)
- [X] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on +Thursday, Jan-19+ Jan-24
- [X] First DataCamp home assignment "What is Machine Learning" Jan-19
- [X] First in-class practice "Introduction to machine learning"

Recommendations:
- [X] "Lyon Data & Analytics Nexus" on Discord (invitation only)
- [X] [[https://youtu.be/BRiNw490Eq0][Google search operators tutorial]] (video, 1 hr) => project
- [X] [[https://youtu.be/i_LwzRVP7bg][ML for everybody by FreeCodeCamp]] (video, 4 hrs) => Python

** Lyon data science nerd network (invitation only)
#+attr_latex: :width 400px
#+caption: Lyon Data & Analytics Nexus Discord server dashboard
[[../img/discord.png]]

- Join the discord server to grow your professional network
- Lyon alumni, students, and companies are invited
- Share internship and job opportunities, and more

** Google search for programmers (Goldin, 2023)
#+attr_latex: :width 400px
#+caption: Google search tutorial by Seth Goldin (freeCodeCamp)
[[../img/goldin.png]]

Waste of time?
1) Searching is not finding
2) Finding is not understanding
3) Understanding is not using

*Google's business model = data + algorithm + infrastructure + brand*
1) data are offered by users for free
2) algorithm is largely known and public
3) infrastructure is proprietary and hard to duplicate
4) brand is unique and highly valuable (but also vulnerable)

ChatGPT's answer to "how do I search using Google effectively?"
#+begin_quote
Here are a few tips for searching effectively using Google:

1. Use quotation marks to search for an exact phrase. For
   example, searching for ~"chocolate chip cookies"~ will return pages
   that include that exact phrase, rather than pages that include the
   words ~"chocolate"~, ~"chip"~, and ~"cookies"~ separately.

2. Use a minus sign to exclude certain words from your search. For
   example, searching for ~chocolate chip cookies -recipe~ will return
   pages that include ~chocolate chip cookies~ but exclude any pages
   that include the word "~recipe~".

3. Use the ~site:~ operator to search within a specific website. For
   example, searching for ~site:nytimes.com chocolate chip cookies~ will
   return pages on the New York Times website that include the words
   "~chocolate chip cookies~."

4. Use the ~filetype:~ operator to search for specific file types. For
   example, searching for chocolate chip cookies ~filetype:pdf~ will
   return PDF documents that include the words "~chocolate chip
   cookies~".

5. Use the ~related:~ operator to find websites that are related to a
   specific website. For example, searching for ~related:nytimes.com~
   will return websites that are related to the New York Times
   website.

6. Use the Google [[https://www.google.com/advanced_search][advanced search page]] for more complex queries.

Keep in mind that Google's search algorithm is constantly evolving, so
these tips may not always return the most accurate results.
#+end_quote

*Caveat:* professional programmers use aggregators instead
- [[https://stackoverflow.com/][stackoverflow.com]] with sub-groups like [[https://chess.stackexchange.com/questions/41282/computers-hardware-or-software][chess.stackexchange.com]]
- [[https://en.wikipedia.org/wiki/Internet_Relay_Chat][Internet Relay Chat]] - in Emacs (~M-x erc~), e.g. [[https://libera.chat/][libera.chat]]
- [[https://www.linuxtoday.com/developer/how-to-use-emacs-as-a-usenet-reader-with-gnus/][Usenet]], mailing lists, Slack, Discord, reddit, quora etc.

** Rapid review - intro to ML
#+attr_latex: :width 400px
#+caption: Photo by Benjamin Davies on Unsplash
[[../img/rapid_review.jpg]]

1) Most popular OS for ML?

2) Which tools are we using in this course?

3) What are the deliverables for your project?

4) Where do you get the topic for your term project?

5) What are the steps for a supervised learning process?

6) What is the general ML process?

* Week 3: Introduction to ML & Review of R (Part II)
#+attr_latex: :width 400px
#+caption: ML intro & R review in week 2 of DSC 305
[[../img/ml_R.png]]

Topics:
- [X] "Doctor, Doctor!"
- [X] Condition for repeating tests
- [X] Test 1 review in class
- [X] Project 1st sprint review
- [X] Introduction to machine learning (continued)
- [ ] Review: R data structures, environment, exploration

Submissions:
- [X] First multiple-choice test in class on Tuesday 24-Jan
- [X] 2nd DataCamp home assignment "Machine Learning Models" by Jan-31
- [X] In-class practice "Introduction to machine learning" (continued)
- [X] Exercise: build an ML code glossary
- [ ] In-class practice "R data structures"

** DONE [[https://lyon.instructure.com/courses/1021/discussion_topics/2125][Jan 26 session online]]
#+attr_latex: :width 400px
[[../img/zoom.png]]

** DONE Doctor, Doctor!
#+attr_latex: :width 400px
[[../img/couch.jpg]]

- ~M-x doctor~

** DONE Project - first sprint review (Tuesday, Feb 13)
#+attr_latex: :width 400px
[[../img/sprint.jpg]]

- The term project purpose is down to you - or you can approach me for
  an idea (but please do that sooner than later)!

- There are many different ways to present a paper or a slide pack:
  e.g. succinct, verbose, opinionated, objective, accessible or not.

- Your first deliverable is a *project proposal* formatted as an
  Org-mode file. Here is a template for such a file with definitions.

  1) Meta data: preliminary title (~#+TITLE~), list of team members
     (~#+AUTHOR:~), course title (~#+SUBTITLE~).

  2) Headlines: ~Problem~, ~Reason~, ~Constraints~, ~Goals and Non-goals~,
     ~Metrics~, ~References~

     - *Problem*: describe the problem that you're trying to solve.

     - *Reason*: why is this problem interesting (to you) right now?

     - *Constraints:* which difficulties, e.g. technical or conceptual,
       do you foresee right now?

     - *Goals and non-goals*: list all goals that you might want to
       achieve with this project. Order the goals by importance. Add a
       list of non-goals, i.e. things that are outside the scope of
       your project.

     - *Metrics*: how would you measure the success of your project?

     - *References*: list any references that you found already. Make
       sure that they are consistent (same format) and complete
       (author, date, title, place).

- Upload your result to Canvas (*no later than Feb 13*). Missing
  the deadline will cost you points.

Source: Ellis, Data Science Project Proposals (2021). URL:
[[https://crunchingthedata.com/data-science-project-proposals/][crunchingthedata.com]].

** DONE Condition for repeating tests
#+attr_latex: :width 400px
#+caption: Late or missed the test? Talk to me!
[[../img/late.jpg]]

- If you inform me beforehand that you cannot attend an announced
  test, we can make arrangements for you to take the test outside of
  class.

** DONE [[https://lyon.instructure.com/courses/1021/assignments/6737/edit?quiz_lti][Test 1 review]]
** DONE How should you study for data science tests?
#+attr_latex: :width 200px
[[../img/studying.jpg]]

- If you were successful in the test: what did you do?
- If not: what do you think you should have done?

* Week 4: Icestorm
#+attr_latex: :width 400px
[[../img/icestorm.jpg]]

* Week 5: Review of R (Part III)
#+attr_latex: :width 400px
[[../img/TourDeFrance.jpg]]

** DONE Upload practice files 2 GDrive (or 2 stick)
#+attr_latex: :width 400px
[[../img/gdrive.png]]

#+attr_latex: :width 400px
[[../img/sticks.jpeg]]

- Make a bootable Linux stick (e.g. [[https://linuxmint-installation-guide.readthedocs.io/en/latest/burn.html][Linux Mint]]), too

** DONE Create an ~.Rprofile~

- Create or open ~~/.Rprofile~ and add these lines to it:
  #+begin_example R
  options(repos=c("https://cloud.r-project.org/"))
  options(crayon.enabled = FALSE)
  message("*** Loaded .Rprofile ***")
  #+end_example

- Save the file and start an R console to test it (you should also see
  the ~Loaded~ message:
  #+begin_src R
    options()$repos
  #+end_src

- From now on, Windows will no longer ask you to choose a mirror site,
  and you will be able to display a "~tibble~" (a sort of data frame
  popular in the "Tidyverse") in Emacs:
  #+begin_src R :results output
    library(MASS)
    tibble(mtcars)
  #+end_src

** DONE Test 2 opens Friday - closes Tuesday Feb 14
#+attr_latex: :width 400px
#+caption: Image by Grovemade via Unsplash.com
[[../img/homeoffice.jpg]]

- This test will be about machine learning models (lecture) and about
  the review of R (in-class assignment).

- You can complete the test at your leisure (within the set time
  limit) but you must complete it before the deadline (Tue-14-Feb)

** DONE Home assignments - project and DataCamp
#+attr_latex: :width 400px
#+caption: Image by Feliphe Schiarolli via Unsplash.com
[[../img/classroom.jpg]]

1) First sprint review: proposal as Org-mode file [[https://lyon.instructure.com/courses/1021/assignments/7108][in Canvas (Feb 13)]]

2) Complete a whole DataCamp course on supervised learning ([[https://lyon.instructure.com/courses/1021/assignments/7387][Mar 24]])
   - Classification using nearest neighbors
   - Classification using Naive Bayes
   - Classifiction using Logistic Regression (curve fitting)
   - Classifiction using Decision Trees

3) We'll review the DataCamp sessions in class and in the tests (I'll
   announce tests 1 week in advance)

** DONE R review: structures/management/exploration
#+attr_latex: :width 250px
[[../img/rlogo.png]]

1) Data structures in R
2) Managing data in R
3) Exploring data in R

[Source: [[https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864][Lantz, Machine learning with R (3e), chapter 2]]]

- Download raw ~2_R_structure_practice.org~ [[https://raw.githubusercontent.com/birkenkrahe/ml/main/org/2_R_structure_practice.org][from GitHub (birkenkrahe/ml)]]
- Open a CMD line terminal (Windows search: CMD, Mac: terminal)
- Navigate to the download directory with ~cd~
- Open the file in a terminal Emacs (can you take this command apart?)
  #+begin_example sh
    emacs -nw --file 2_R_structure_practice.org
  #+end_example
  #+attr_latex: :width 400px
  [[../img/emacs.png]]
- Finish practice files started in class on your own by the deadline

** READ Understanding R startup
#+attr_latex: :width 400px
#+caption: From "R for Enterprise: Understanding R's Startup (Lopp, 2019)
[[../img/rstartup.jpeg]]

[[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][Here is an article]] (Lopp, 2019) on R startup variables and
settings. Includes an explanation why the ~.Rprofile~ startup file was
not read when some of you opened the R console in the shell (you
should probably try ~Rgui~ on the command line, too).

** NEW Get bonus points when practicing
#+attr_latex: :width 200px
[[../img/datacamp2.png]]
- You can get 10 bonus points if you keep a practice streak of 10 days
- You can do this up to 3 times for a maximum of 30 points, which will
  be applied to your weakest final grade category
- Submit a screenshot of your mobile (or desktop) streak in Canvas
- If you lose your streak between day 5 and 10, you still get 5 points
- On the dashboard, DataCamp will suggest practice categories for you,
  and also in the mobile app
- This option ends on May 3rd (last day of spring term)
- You can get this bonus only in one of my courses (if you attend > 1)
#+attr_latex: :width 400p
[[../img/datacamp3.png]]

** NEW GNU Treats: ~speed-type~, ~treemacs~ and ~gtypist~

- An attractive alternative to ~Dired~ is the ~treemacs~ package. It
  looks like this on my PC (and also works for the terminal Emacs):
  #+attr_latex: :width 400px
  [[../img/t_treemacs.png]]

- If you want to be faster on the keyboard, try [[https://www.gnu.org/savannah-checkouts/gnu/gtypist/gtypist.html#:~:text=GNU%20Typist%20(also%20called%20gtypist,the%20GNU%20General%20Public%20License.][GNU Typist]], a free
  10-lesson online trainer for increasing your typing skills.
  #+attr_latex: :width 400px
  [[../img/gtypist.png]]

- There is also an Emacs package to practice touch/speed typing in
  Emacs called ~speed-type~. You have to install it with ~M-x
  package-list-packages~, then find the package in the list and install
  with ~i~ and ~x~. [[https://github.com/dakra/speed-type][More information on GitHub.]]

* Week 6: ML models overview
#+attr_latex: :width 400px
[[../img/ml_types2.png]]

- [X] Remember to upload practice (there are deadlines) [[https://lyon.instructure.com/courses/1021/assignments][to Canvas]]
- [X] Open ~2_R_explore_practice.org~ and load the data
- [X] Let's finish the review and upload the completed file to Canvas
- [X] What is R? Good overview [[https://www.datacamp.com/blog/all-about-r][in this DataCamp blog post]] (05/22)
- [X] [[https://lyon.instructure.com/courses/1021/assignments/6997][Test 2 (open book) is live online until Fri 17-Feb, 11:59 pm]].

** Review: exploring numerical data structure:

Open an Org-mode file if you want to code along.

1) [X] How can you get an overview of statistical information for ~Nile~?
   #+begin_src R :results output
     summary(Nile)
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :   456.0   798.5   893.5   919.4  1032.5  1370.0

2) [X] What about the ~time~ of the ~Nile~ observations?
   #+begin_src R :results output
     summary(time(Nile))
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :    1871    1896    1920    1920    1945    1970

3) [X] How many values of ~Nile~ are outliers? Which values are these?
   #+begin_quote
   *Update:* as the computation of ~IQR*1.5~ for ~Nile~ shows, there are no
   outliers in the data set - the boxplot (below) shows this,
   too. Tukey's method (~car::outlier~) is not applicable to time
   series, alas.
   #+end_quote

   Various tests:
   #+begin_src R :results output
     IQR(Nile) * 1.5 # standard outlier value
     length(Nile > (IQR(Nile) * 1.5))
     scale(Nile) # z-score method
     Nile[which(Nile > (IQR(Nile) * 1.5))]
   #+end_src
   #+begin_src R :results output
     library(car)
     any(ls('package:car')=="outlier")
                                             # outlierTest(Nile)  ## not applicable to time series
     library(qdap)
     outlier_detect(as.character(Nile))
   #+end_src
   #+begin_src R
     car::outlierTest(Nile)
     ## outlier.test(c(1,2,3,1000))
   #+end_src
4) [ ] How would you make a boxplot of the values of ~Nile~?
   #+begin_src R :results graphics file output :file ../img/Rbox.png
     boxplot(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rbox.png]]

5) [ ] How would you visualize how often certain values of ~Nile~ occur?
   #+begin_src R :results graphics file output :file ../img/Rhist.png
     hist(Nile)
   #+end_src

   #+RESULTS:
   [[file:../img/Rhist.png]]

6) [ ] How many observations of the ~Nile~ flow are between 800 and 1000
   mio cubic metres?
   #+begin_src R
     length(
       Nile[
         which(
           Nile > 800 & Nile < 1000
         )
       ]
     )
   #+end_src

   #+RESULTS:
   : 44

** AI's Bid for Freedom
#+attr_latex: :width 400px
[[../img/freedom.png]]

- My courses for Fall 2023:

  1) Artificial Intelligence (DSC 482.01) - seminar. The 2023 edition
     will include deep learning with R/Python and more code poetry.

  2) Data and process modeling (DSC 482.02) - seminar. The 2023
     edition will include exploring differential equations with R

  3) Introduction to data science (DSC 105): The 2023 edition will
     include R and Python.

- All courses are for everybody who's interested in data science: tell
  your friends!

** Project proposal feedback in the classroom
** You can submit an improved proposal by Fri-Feb-24

* Week 7: k-Nearest-Neighbor (k-NN) algorithms
#+attr_latex: :width 400px
#+caption: Photo by Avin CP on Unsplash
[[../img/tomato.jpg]]
5_ductal_carcinoma.jpg
- [X] Test 2 now open for late attempts (partial credit)

- [X] [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][Man beats machine at Go in human victory over AI]] (Waters, 2023)
  #+begin_quote
  "The discovery of a weakness in some of the most advanced Go-playing
  machines points to a fundamental flaw in the deep-learning systems
  that underpin today’s most advanced AI [...]: The systems can
  “understand” only specific situations they have been exposed to in
  the past and are unable to *generalize* in a way that humans find
  easy. It shows once again we’ve been far too hasty to ascribe
  superhuman levels of intelligence to machines.” - Russell (AIMA)
  #+end_quote

- [-] k-nearest-neighbor (knn) models:
  1) [X] Lecture (HTML, Org-mode or [[https://github.com/birkenkrahe/ml/blob/main/md/4_knn.md][Markdown]] with notes in GitHub)
  2) [X] Practice (demo: cancer classification with k-NN)
  3) [ ] Exercise (bonus assignment in R)
  4) [ ] Test 3 (opens next week)

* Week 8: k-NN case study - cancer prediction (cont'd)
#+attr_latex: :width 400px
#+caption: Ductal Carcinoma in situ (Source: jhp.edu)
[[../img/5_ductal_carcinoma.jpg]]

** Test 3 preview

Here are [[https://github.com/birkenkrahe/ml/blob/main/org/agenda.org#next-test-3-preview][10 questions]] that preview the next (graded) ML test
*When prompted, write down your answer or keywords to the answer.*

1) [X] How is similarity measured in k-NN?
   #+begin_notes
   By computing a distance measure, e.g. Euclidean distance, the usual
   distance between two vectors, see ~dist~.
   #+end_notes
2) [X] What is used for "training" and "testing" the k-NN classifier?
   #+begin_notes
   Two data sets, one known, labelled data to train the model on a
   label, the other one unseen, unlabelled data to test the trained
   model. The training and test data should be of comparable
   quality and randomized.
   #+end_notes
3) [X] What is the meaning of the parameter 'k'? (4)
   #+begin_notes
   - The number of nearest neigbors after computing distance
   - A measure of the size of the classification neighborhood
   - A measure for the degree of over- or underfitting
   - An argument in the ~class::knn~ function
   #+end_notes
4) [X] What are the arguments of ~knn~? (4)
   #+begin_src R :results output :exports none
     library(class); args(knn)
   #+end_src

   #+RESULTS:
   : function (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)
   : NULL
5) [X] What is the purpose of splitting the data in training and test
   data? Why don't we just use ALL available data for training?
   #+begin_notes
   - To avoid overfitting: without testing, it is not clear to which
     extent the model will generalize to unseen data. We split one
     data set because the data are of comparable quality/structure.
   #+end_notes
6) [X] What happens when voting is tied after measuring the distances?
   #+begin_notes
   In ~class::knn~, the class is decided at random. Use odd ~k~ to avoid this.
   #+end_notes
7) [X] If I have 10 features q_{1}...q_{10} and 1 target label
   - How many dimensions does the feature space have?
   - How many terms under the square root of the distance formula?
   - How many variables are used for classification of p?
     #+begin_notes
     - 10 features, hence 10-dim feature space
     - Each point has 10 coordinates. The distance formula has 10 terms
       p_{i}-q_{ij}, one for each i-th coordinate of q_{j}, and 11
       variables are involved: 10 features (predictors) and 1 target
       variable (predicted).
     #+end_notes
8) [X] Is a bigger ~k~ always better? Why or why not?
   #+begin_notes
   Optimal k depends on noise in the data, on the pattern to be
   identified, one starts off with the square root of the number of
   training examples, to find the optimal k, try different values.
   #+end_notes
9) [X] Measuring model performance:
   - What is the confusion matrix? What's its *dimension*?
   - How can you *compute* the confusion matrix for kNN in R?
   - How can you assess the accuracy *numerically*?
     #+begin_notes
     - The confusion matrix is a 2 x 2 cross-table if the target feature
       had 2 labels/categories (e.g. male/female or benign/malignant),
       or a 3 x 3 for 3 labels. It shows, which levels were accurately
       identified and which were not ([[https://github.com/birkenkrahe/ml/blob/main/img/confusion1.png][solution]]).
       #+attr_latex: :width 400px
       #+caption: Confusion matrix (empty) and classification results
       [[../img/confusion.png]]
     - With the ~table~ function and the ~factor~ levels or ~character~
       values as arguments, e.g. ~table(signs_pred,signs_actual)~.
     - With the ~mean~ function by averaging over the vector resulting
       from comparing original and predicted values:
       ~mean(sign_actual==sign_pred)~.
     #+end_notes
10) [X] What does bias-tradeoff refer to?
    #+begin_notes
    #+attr_latex: :width 400px
    [[../img/bias_tradeoff.png]]
    The image shows subsequent classification attempts. Perfect
    prediction corresponds to the bulls-eye. The further away a point
    is, the worse the prediction.
    - Bias leads to a systematic prediction error, e.g. because of
      existing patterns in the data, or other redundancies.
    - Low bias (upper row) means low error and high bias (lower row)
      means large error.
    - Variance corresponds to spread. Low variance (left column)
      leaves the results together, high variance blows them up.
    - The best case is low bias and low variance: low error, points
      close together, the worst is high bias and high variance.
    #+end_notes
** Assignment - submit by March 10
#+attr_latex: :width 400px
#+caption: Photo by Andrew Neel, Unsplash.com
[[../img/assignment.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8405][Next assignment: NAIVE BAYES method - deadline MARCH 7, 11:59 pm]]

** Test - submit by March 10
#+attr_latex: :width 400px
#+caption: Photo by Ben Mullins, Unsplash.com
[[../img/0_test.jpg]]

- [[https://lyon.instructure.com/courses/1021/assignments/8789/edit?quiz_lti][Test 3 is live from later today until Friday, MARCH 10, 11:59pm.]]

** Project - 2nd sprint review by March 17

** Continue: kNN case study
#+attr_latex: :width 600px
#+caption: k-NN workflow (Gavagsaz, 2022)
[[../img/knn.jpg]]

- If you missed the last session, *download* ~5_knn_practice.org~
  fromGitHub and head to the chapter *Intermission* to catch up!

- Lecture: ~5_knn_case.org~ & 2 bonus exercises

- *We hope to cover today:*
  1) Normalizing (rescaling) numeric data
  2) Creating training and test data sets
  3) Training a model on the data
  4) Evaluating model performance
  5) Exercises: improve the model! (bonus)

* Week 9: k-NN improvement & Naive Bayes
#+attr_latex: :width 400px
[[../img/amazing-thomas-bayes-illustration.jpg]]

*REMINDERS* (check your Canvas calendar):
- Complete *test 3* by Friday 11:59 pm this week!
- Complete *DataCamp assignment* "Naive Bayes" by Friday this week!
- [[https://lyon.instructure.com/courses/1021/assignments/9116][Literature review (2nd sprint review) by end of next week]]!

*THIS WEEK*:
- Evaluating kNN performance (practice)
- Improving kNN performance (2 x exercise)
- Lecture and practice: supervised learning with Naive Bayes

*UPCOMING TOPICS*:
- Text mining to build spam filter with Naive Bayes
- Word cloud visualization
- Regression methods: linear and logistic methods
- Regression use case: predicting medical expenses

** DONE Review: kNN case study (continued)
#+attr_latex: :width 400px
[[../img/rapid_review.jpg]]

- [ ] What are we trying to do?

- [ ] What does "training a kNN learner" mean?

- [ ] What have we done so far?
  #+begin_quotes
  1) Data collection and storage (data frame)
  2) Exploration of the class/target label and predictors/features
  3) Cleaning, randomization and normalization of the data
  4) Splitting of the dataset into training and test data
  5) Isolating the training and test labels
  6) Running the ~class::knn~ function with set ~k~ value


  - [ ] What's left?
    #+begin_quotes
    - Evaluating the performance of the model (confusion matrix, accuracy)
    - Improving the performance (standardization, varying k-values)
    - Running the model on new datasets (prepare data, run ~predict~)
  #+end_quotes

** TODO Game programming in Snap! with AI
#+attr_latex: :width 400px
[[../img/snap.png]]

- https://github.com/birkenkrahe/snap (HTML, PDF, Org, md)
- Next fall: [[https://ecraft2learn.github.io/ai/][AI extensions to Snap!]] with [[https://ecraft2learn.github.io/ai/student-projects/][student project examples]]

** TODO Bonus exercise & March assignments
#+attr_latex: :width 400px
[[../img/bonus.png]]

- Remember how to compute the prediction accuracy: as the ~mean~ over
  the values of the original and the predicted ~factor~ vector ([[https://github.com/birkenkrahe/ml/blob/main/org/4_knn_case.org#computing-accuracy-as-an-average][GitHub]])

* Week 10: Naive Bayes algorithm and case study
#+attr_latex: :width 400px
#+caption: Illustration by Peter Eich (2006), Wikimedia (CC BY-SA 2.5)
[[../img/5_Spamfilter.jpg]]

- Naive Bayes review and algorithm
- Naive Bayes case study: spam filter
- Naive Bayes test 5 (due March 31)

** Pie Day!
#+attr_html: :width 300px
[[../img/pi.jpg]] [[../img/limerick.png]]

** Results: Test 3 and Tsunami of assignments
#+attr_latex: :width 400px
[[../img/mlTestBox.png]]

- Feel free to complete test 3 by Friday for partial credit (60%)

- [[https://github.com/birkenkrahe/org/blob/master/pdf/midterm.pdf][How to improve your mid-term grades if you choose to do so]]. The link
  to [[https://www.quora.com/Why-are-computer-science-degrees-so-math-intensive-when-the-field-doesnt-seem-to-use-much-math-at-all][this Quora comment]] is not available in the document, alas.

- Do not forget that there are *three DataCamp lessons* waiting for
  completion by March 31, 2023: Naive Bayes, Logistic Regression, and
  Classification Trees.

- I'll decide over spring break which of these (if any) we'll pick up
  for detailed treatment in class. In any case: more certificates 4U!

** Review: Simple Naive Bayes
#+attr_html: :width 300px
#+caption: Photo by Ben Mullins on Unsplash
[[../img/review.jpg]]

1) Which evidence (in the data) is used for Naive Bayes?
   #+begin_notes
   Answer: NB uses all available data with stochastic weights or
   probabilities attached to individual features.
   #+end_notes
2) For these *events*, what's a suitable *trial* to establish probability?
   - Spam message
   - Lottery win
   - Sunny weather
3) What determines the class our classifier is trying to predict?
   #+begin_notes
   Answer: 1) our observational data (e.g. recorded labels spam
   vs. ham), 2) our problem (e.g. reduce spam)
   #+end_notes
4) Can the same dataset be used to predict different things? For
   example, could a dataset that contains spam vs. ham labels (in
   addition to many other text features) be used for sentiment
   analysis?
5) Why are more trials better for the accuracy of a probabilistic
   prediction?
   #+begin_notes
   Because of the "law of large numbers", which holds for independent
   trials of the same experiment - the expected value (sum of all
   values weighted by their individual probabilities) becomes the
   sample mean (arithmetic average of all values).
   #+end_notes
6) What kind of events are "spam vs. ham", or "win vs. loss",
   "benign vs. malignant".
   #+begin_notes
   Answer: they are mutually exclusive and exhaustive events, i.e. an
   event is either one or the other but never both, the sum of their
   probabilities adds up to one, and their joint probability is zero.
   #+end_notes
7) What's is the joint probability of the events "being home" and
   "being at the office"?
   #+begin_notes
   Answer: it is zero provided that these events are mutually
   exclusive.
   #+end_notes
8) If event B is "being home" and event A is "working", what is
   the conditional probability of working at home, P(A|B)?
   #+begin_notes
   (1) conditional probability: how likely is an event A based on
   occurrence of a previous event B?  (2) P(A|B) = P(A and B)/P(B),
   joint probability of A and B divided by the probability of B,
   (3) P(A|B)=P(B|A) * P(A)/P(B)
   #+end_notes
9) What is the basis of predictive, probabilistic modeling?
   #+begin_notes
   Answer: dependent events - dependency of events (recorded as
   feature values) means that the occurrence of one event is
   conditional on another: for example, clouds are a condition for
   rain. The probability of rain is conditional on the probability of
   clouds appearing - the latter reveals something about the future of
   the former that we can use to predict the weather. Clouds are
   predictors for rain.
   #+end_notes
10) What does independence of two events imply?
    #+begin_notes
    Answer: knowing something about one event reveals nothing about
    the other event. Neither can be used as a predictor for the
    other. Their joint probability (the probability that they occur
    together) is the product of the individual probabilities.
    #+end_notes
11) You're trying to establish the probability that a message with the
    word "URGENT" in upper case letters is a spam message. What else
    do you need to compute this?
    #+begin_notes
    Answer: according to Bayes' theorem, the conditional probability
    P(spam|URGENT) = P(URGENT|spam) * P(spam) / P(URGENT). You need:
    - P(URGENT|spam): likelihood of finding URGENT in a spam message;
    - P(spam): chance of spam in a given sample of messages (trials)
    - P(URGENT): chance of finding "URGENT" in a sample of messages
    #+end_notes
12) Compute P(spam|URGENT) from the following *frequency* table:
    | FREQUENCY | URGENT=YES | URGENT=NO | TOTAL |
    |-----------+------------+-----------+-------|
    | spam      |          9 |        16 |    25 |
    | ham       |          5 |       120 |   125 |
    |-----------+------------+-----------+-------|
    | Total     |         14 |       136 |   150 |
    #+begin_notes
    Answer: create *likelihood* table - the conditional probabilities
    | LIKELIHOOD | URGENT=YES | URGENT=NO | TOTAL |
    |------------+------------+-----------+-------|
    | spam       | 9/25       | 16/25     |    25 |
    | ham        | 5/125      | 120/125   |   125 |
    |------------+------------+-----------+-------|
    | Total      | 14/150     | 136/150   |   150 |

    + ~P(URGENT|spam)~ = 9/25 (likelihood of URGENT in spam)
    + ~P(spam)~ = 25/150 (prior probability - before URGENT)
    + ~P(URGENT)~ = 14/150 (marginal likelihood)
    #+end_notes
    #+begin_src R
      p <- (9/25) * (25/150) / (14/150)
      paste("Posterior probability:",format(p*100,digits=2),"%")
    #+end_src

    #+RESULTS:
    : Posterior probability: 64 %

** Reading therapy for spring break: ELIZA
#+attr_latex: :width 400px
[[../img/eliza.png]]

- You will get a print copy of this seminal article to read and
  (perhaps) discuss.

- Cool online implementation on a DEC VT100 terminal at
  [[https://www.masswerk.at/eliza/][masswerk.at/eliza/]]

- ELIZA - A Computer Program For the Study of Natural Language
  Communication Between Man And Machine - Weizenbaum, CACM 9(1)
  (1966):36-46. URL: [[https://dl.acm.org/doi/pdf/10.1145/365153.365168][dl.acm.org]].

- See also: The computational therapeutic: exploring Weizenbaum’s ELIZA as a
  history of the present - Bassett, AI & Society 34
  (2019):803-812. URL: [[https://link.springer.com/article/10.1007/s00146-018-0825-9][link.springer.com]].

** Review: Spam Filter I
#+attr_html: :width 300px
#+caption: Photo by Ben Mullins on Unsplash
[[../img/review1.jpg]]

1) How is the usual ML workflow altered for a message spam filter?
   #+begin_notes
   Answer: the data cleaning includes text formatting, tokenization,
   stemming, stopwords, corpus creation, and a term frequency table.
   #+end_notes
2) How many features does the raw data set (SMS Spam Collection) have?
   How many will the cleaned data set have?
   #+begin_notes
   Answer: the initial dataset has 2 features - class label (spam,
   ham), and message. The final dataset has as many features as spam
   trigger terms (probably several hundred) - our example: > 6,000.
   #+end_notes
3) You've just loaded the R package "tm". How would you check (1) that
   the package is loaded, (2) which functions and (3) which datasets
   (if any) it contains?
   #+begin_src R :results output
     library(tm)   # load package provided it's been installed
     search()  # check package environments search path
     ls('package:tm') # check methods/functions inside tm
     data(package="tm") # list datasets contained in package
   #+end_src

   #+RESULTS:
   #+begin_example
    [1] ".GlobalEnv"        "package:tm"        "package:NLP"
    [4] "ESSR"              "package:stats"     "package:graphics"
    [7] "package:grDevices" "package:utils"     "package:datasets"
   [10] "package:stringr"   "package:httr"      "package:methods"
   [13] "Autoloads"         "package:base"
    [1] "as.DocumentTermMatrix"   "as.TermDocumentMatrix"
    [3] "as.VCorpus"              "Boost_tokenizer"
    [5] "content_transformer"     "Corpus"
    [7] "DataframeSource"         "DirSource"
    [9] "Docs"                    "DocumentTermMatrix"
   [11] "DublinCore"              "DublinCore<-"
   [13] "eoi"                     "findAssocs"
   [15] "findFreqTerms"           "findMostFreqTerms"
   [17] "FunctionGenerator"       "getElem"
   [19] "getMeta"                 "getReaders"
   [21] "getSources"              "getTokenizers"
   [23] "getTransformations"      "Heaps_plot"
   [25] "inspect"                 "MC_tokenizer"
   [27] "nDocs"                   "nTerms"
   [29] "PCorpus"                 "pGetElem"
   [31] "PlainTextDocument"       "read_dtm_Blei_et_al"
   [33] "read_dtm_MC"             "readDataframe"
   [35] "readDOC"                 "reader"
   [37] "readPDF"                 "readPlain"
   [39] "readRCV1"                "readRCV1asPlain"
   [41] "readReut21578XML"        "readReut21578XMLasPlain"
   [43] "readTagged"              "readXML"
   [45] "removeNumbers"           "removePunctuation"
   [47] "removeSparseTerms"       "removeWords"
   [49] "scan_tokenizer"          "SimpleCorpus"
   [51] "SimpleSource"            "stemCompletion"
   [53] "stemDocument"            "stepNext"
   [55] "stopwords"               "stripWhitespace"
   [57] "TermDocumentMatrix"      "termFreq"
   [59] "Terms"                   "tm_filter"
   [61] "tm_index"                "tm_map"
   [63] "tm_parLapply"            "tm_parLapply_engine"
   [65] "tm_reduce"               "tm_term_score"
   [67] "URISource"               "VCorpus"
   [69] "VectorSource"            "weightBin"
   [71] "WeightFunction"          "weightSMART"
   [73] "weightTf"                "weightTfIdf"
   [75] "writeCorpus"             "XMLSource"
   [77] "XMLTextDocument"         "Zipf_plot"
   [79] "ZipSource"
   Data sets in package 'tm':

   acq                     50 Exemplary News Articles from the
                           Reuters-21578 Data Set of Topic acq
   crude                   20 Exemplary News Articles from the
                           Reuters-21578 Data Set of Topic crude
   #+end_example

Now go to GitHub, save ~5.2.org~ to ~Downloads~ and insert the file to
your practice file ~5_naive_bayes_practice.org~ with ~C-x i~.

Save the file and run all code blocks: ~M-x org-babel-execute-buffer~,
then check with ~ls~ and ~search~ that all is ready to move on.

* Week 11: AI and chatbots / Saving & loading ~.RData~
#+attr_latex: :width 400px
#+caption: G. Washington's teeth, Library of Congress
[[../img/teeth.jpg]]

- [X] I evaluated & graded outstanding exercises (*please check before*)
- [X] The literature reviews were all impressive in their own way. The
  best one even referenced other sources within the review.
- [X] Those who have not submitted reviews yet please do so *by March 31*
  for partial credit.
- [X] Next sprint review: results! *Due on April 21.*
- [X] *Test 4* is live from 2.30 pm 28-March: full points if you submit by
  March 31, partial credit if you miss the deadline but submit by
  April 7.
- [X] *Bonus points* for experience report from the job fair April 4 -
  [[https://lyon.instructure.com/courses/1021/assignments/9611][see Canvas]]

** In the meantime: the charade continues
#+attr_latex: :width 550px
#+caption: Source: Twitter
[[../img/neil_gaiman.png]]
[[../img/leo_pires.png]]

In response to a "Sandman" fan discovering chatbot lies.
#+attr_latex: :width 400px
#+caption: Noam Chomsky, MIT.
[[../img/chomsky.png]]

[[https://drive.google.com/file/d/1lptWLxqzpFbSPKsumU5KsnsKUD6__PyO/view?usp=sharing][Right down this alley: Chomsky's biting critique of ChatGPT in the NYT]]

** DataCamp assignments update
#+attr_latex: :width 400px
[[../img/datacampupdate.png]]

- DataCamp assignments: *due dates* changed to weekly in April
- *Removed* two assignments (we need some time for neural nets)
- The last assignments carry *more points* (you're more on your own)
- If you already completed both courses, contact me for a bonus
- *When you complete these, try to maximize your XP (don't cheat)*

** ELIZA - "Fake Rogerian Therapist"
#+attr_latex: :width 400px
[[../img/weizenbaum-eliza.jpg]]

[[https://www.masswerk.at/elizabot/][TLDR? Here's a summary]].

*20 MINUTES "CLOSE READING" AND DISCUSSION:*
1) Split into pairs or groups of three
2) Discuss your views of ELIZA in the light of both what you have learnt
   about machine learning, and of Weizenbaum's conclusions (5 min)
3) Say in your own words what an "augmented ELIZA program" should be
   able to do and contrast this what you know of state-of-the-art
   chatbots programs (5 min)
5) Highlight at least one hypothesis from [[https://www.reddit.com/r/ChatGPT/comments/11ng6t6/noam_chomsky_the_false_promise_of_chatgpt/][Chomsky's article]] "The False
   Promise of ChatGPT" (he's got a lot of 'em - I counted 10) (5 min)
6) What does all this mean for the future of machine learning? (5 min)

Along the way, write down any QUESTIONS that you may have!

#+begin_quote
*In addition, Weizenbaum wrote:*
- "The whole issue of the credibility (to humans) of machine output
  demands investigation. Important decisions increasingly tend to be
  made in response to computer output. ELIZA shows [...] how easy it
  is to create and maintain the illusion of understanding. A certain
  danger lurks here. [...] An augmented program is a system which
  already has access to a store of information about some aspects of
  the real world which, by means of conversational interaction with
  people, can reveal both what it knows (behave as an information
  retrieval system) and where its knowledge ends and needs to be
  augmented. Hopefully, the augmentation of its knowledge will also be
  a direct consequence of its conversational experience."
#+end_quote
#+begin_quote
*Notes from reading Weizenbaum's article:*
- "Rogerian psychotherapist": empathy-based therapy
- "dyadic conversation": between two people
- MAC time-sharing system: precursor of Unix (MAC -> EMACs)
- Separation of code (scripted, edited) and data (transformed)
- Text data transformation with tokenization functions
- Text interpretation after applying stop words transformation
- "Augmented ELIZA" is not realized yet, ChatGPT is just another
  "translating processor" (without understanding).
#+end_quote
#+begin_quote
*Chomsky's hypotheses:*
1) Machine learning will degrade our science and debase our ethics by
   incorporating into our technology a fundamentally flawed conception
   of language and knowledge.
2) True understanding has not, will not and cannot occur if ML
   programs like ChatGPT dominate the field of AI.
3) The human mind is not a lumbering statistical pattern matching
   engine gorging on Big Data and extrapolating the most probable
   answer by inferring correlations.
4) The human mind operates with small amount of information and
   creates explanations.
5) Chatbots are stuck in a prehuman phase of cognitive evolution
   because they lack the ability to think of counterfactuals.
6) The predictions of ML systems will always be superficial and
   dubious.
7) ML predictions of scientific facts are pseudoscience.
8) True intelligence is demonstrated in the ability to think and
   express improbable but insightful things.
9) True intelligence is capable of moral thinking, i.e. constraining
   creativity with ethical principles.
10) Chatbots are constitutionally unable to balance creativity with
    (moral) constraint.

*Questions you might have had:*
- Who was Jorge Luis Borges? ([[https://en.wikipedia.org/wiki/Jorge_Luis_Borges][Blind Argentinian librarian and author]])
- What is '[[https://en.wikipedia.org/wiki/Universal_grammar][universal grammar]]'? (Chomskian theory of language, 1965)
- What is 'the banality of evil'? (Arendt: "Can one do evil without
  being evil?", in "[[https://en.wikipedia.org/wiki/Eichmann_in_Jerusalem#Banality_of_evil][Eichmann In Jerusalem]]", 1963).
- Who is Jeffrey Watumull? (Chief Philosophy Officer of Oceanit -
  "Intellectual Anarchy through Disruptive Innovation")
#+end_quote

** Lex Fridman (MIT) interviews Sam Altman (OpenAI)
#+attr_latex: :width 400px
#+caption: Lex Fridman (MIT) and Sam Altman (OpenAI) 26-March-2023
[[../img/altman.png]]

- [[https://youtu.be/L_Guz73e6fw][Link to the > 2 hr interview (YouTube)]]

** Bonus points for job fair experience report!
#+attr_latex: :width 400px
[[../img/fair_flickr_color.jpg]]

- Write long paragraph about your job fair experience for 10 points.
- Great opportunity to network, mix and mingle, and show off.
- Bring 1 page resume, a few questions, a story, and dress up.
- Must go: graduating seniors. Should go: everyone else.
- Motivate each other by going as a pair, a group, a team.
- Post your experience report in Canvas.

** Review: text mining
#+attr_latex: :width 400px
[[../img/sparse_dense.gif]]

1) What is the relative proportion of spam vs. ham messages in the
   ~sms_raw~ dataset if the ~type~ feature contains this information, in
   the ~format~ of ~numeric~ output with ~2~ ~digits~ after the decimal point?
   #+begin_src R
     <<load_sms_raw>>
     ## easy: intermediate storage
     spam_ham <- table(sms_raw$type)  # frequencies of spam vs ham
     spam_ham_prop <- prop.table(spam_ham)  # proportions of spam vs ham
     spam_ham_prop_2 <- format(spam_ham_prop,digits=2)
     spam_ham_prop_2 <- as.numeric(spam_ham_prop_2)
     spam_ham_prop_2
                                             #as.numeric(spam_ham_prop_2)

     ## medium: nested/functional call
     as.numeric(format(prop.table(table(sms_raw$type)),digits=2))
   #+end_src

   #+RESULTS:
   : [1] 0.87 0.13
   : [1] 0.87 0.13
   
2) Can you compute the same thing using a pipeline?
   #+begin_src R
     <<load_sms_raw>>
     ## hard: pipeline/shell call
     sms_raw$type |>
       table() |>
       prop.table() |>
       format(digits=2) |>
       as.numeric()
   #+end_src

3) How can you check directly that the ~sms_raw~ dataframe was actually
   loaded into the current R session - with a ~logical~ answer?
   #+begin_src R
     any(ls()=="sms_raw")
   #+end_src

   #+RESULTS:
   : [1] TRUE
   :  [1] "api_key"          "ask_chatgpt"      "ham"              "m"               
   :  [5] "sms_corpus"       "sms_corpus_clean" "sms_dtm"          "sms_dtm_test"    
   :  [9] "sms_dtm_train"    "sms_dtm2"         "sms_raw"          "sms_test_labels" 
   : [13] "sms_train_labels" "spam"             "spam_ham"         "spam_ham_prop"   
   : [17] "spam_ham_prop_2"  "string"           "stringX"          "tokens"

4) Is the ~removeWords~ function part of the ~tm~ package? What are the
   steps to check this?
   #+begin_src R
     library(tm)
     any(ls('package:tm')=="removeWords")
     ls('package:tm')
   #+end_src

   #+RESULTS:
   #+begin_example
   [1] TRUE
    [1] "as.DocumentTermMatrix"   "as.TermDocumentMatrix"  
    [3] "as.VCorpus"              "Boost_tokenizer"        
    [5] "content_transformer"     "Corpus"                 
    [7] "DataframeSource"         "DirSource"              
    [9] "Docs"                    "DocumentTermMatrix"     
   [11] "DublinCore"              "DublinCore<-"           
   [13] "eoi"                     "findAssocs"             
   [15] "findFreqTerms"           "findMostFreqTerms"      
   [17] "FunctionGenerator"       "getElem"                
   [19] "getMeta"                 "getReaders"             
   [21] "getSources"              "getTokenizers"          
   [23] "getTransformations"      "Heaps_plot"             
   [25] "inspect"                 "MC_tokenizer"           
   [27] "nDocs"                   "nTerms"                 
   [29] "PCorpus"                 "pGetElem"               
   [31] "PlainTextDocument"       "read_dtm_Blei_et_al"    
   [33] "read_dtm_MC"             "readDataframe"          
   [35] "readDOC"                 "reader"                 
   [37] "readPDF"                 "readPlain"              
   [39] "readRCV1"                "readRCV1asPlain"        
   [41] "readReut21578XML"        "readReut21578XMLasPlain"
   [43] "readTagged"              "readXML"                
   [45] "removeNumbers"           "removePunctuation"      
   [47] "removeSparseTerms"       "removeWords"            
   [49] "scan_tokenizer"          "SimpleCorpus"           
   [51] "SimpleSource"            "stemCompletion"         
   [53] "stemDocument"            "stepNext"               
   [55] "stopwords"               "stripWhitespace"        
   [57] "TermDocumentMatrix"      "termFreq"               
   [59] "Terms"                   "tm_filter"              
   [61] "tm_index"                "tm_map"                 
   [63] "tm_parLapply"            "tm_parLapply_engine"    
   [65] "tm_reduce"               "tm_term_score"          
   [67] "URISource"               "VCorpus"                
   [69] "VectorSource"            "weightBin"              
   [71] "WeightFunction"          "weightSMART"            
   [73] "weightTf"                "weightTfIdf"            
   [75] "writeCorpus"             "XMLSource"              
   [77] "XMLTextDocument"         "Zipf_plot"              
   [79] "ZipSource"
   #+end_example

5) Display a list of all datasets contained in the ~tm~ package:
   #+begin_src R
     data(package="tm")
   #+end_src

   #+RESULTS:
   : Data sets in package 'tm':
   : 
   : acq                     50 Exemplary News Articles from the
   :                         Reuters-21578 Data Set of Topic acq
   : crude                   20 Exemplary News Articles from the
   :                         Reuters-21578 Data Set of Topic crude

6) After turning a CSV file into a dataframe, then into a source, then
   into a volatile corpus ~list~, what kind of output do you get when
   entering the name of the corpus?
   #+begin_src R
     <<create_sms_corpus>>
     sms_corpus # output is meta data only
   #+end_src

   #+RESULTS:
   : <<VCorpus>>
   : Metadata:  corpus specific: 0, document level (indexed): 0
   : Content:  documents: 5559

7) How many characters does message no. 444 from ~sms_corpus~ have?
   #+begin_src R
     nchar(content(sms_corpus[[444]]))  # nested version
     sms_corpus[[444]] |> content() |> nchar()   # pipeline version
   #+end_src

   #+RESULTS:
   : [1] 47
   : [1] 47

8) What is the average number of characters of all messages from
   ~sms_corpus~? (Use: ~lapply~, ~nchar~ and ~mean~)
   #+begin_src R
     ## apply content to each list element
     all_msg <- lapply(sms_corpus, content)
     ## count number of chars with nchar
     all_msg_chars <- nchar(all_msg)
     ## compute the mean across the vector
     mean(all_msg_chars, na.rm=TRUE)
                                             # nested:
     mean(nchar(lapply(sms_corpus, content)))
                                             # pipeline:
     sms_corpus |>
       lapply(FUN=content) |>
       nchar() |>
       mean(na.rm=TRUE)
   #+end_src

   #+RESULTS:
   : [1] 79.77136
   : [1] 79.77136
   : [1] 79.77136

9) How can you remove all numbers from the message no. 200?
   #+begin_src R
     ## the wrapper tm_map only works for
     content(sms_corpus[[200]])
     content(removeNumbers(sms_corpus[[200]]))
   #+end_src

   #+RESULTS:
   : [1] "WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"
   : [1] "WELL DONE! Your * Costa Del Sol Holiday or £ await collection. Call  Now toClaim. SAE, TCs, POBox, Stockport, SKxh, Cost£./pm, Maxmins"

10) Could you also use the wrapper function ~tm_map~ to remove the
    numbers from an individual message?
    #+begin_src R
      ## tm_map only accepts the whole corpus as data argument!
      tm_map(sms_corpus,removeNumbers) -> sms_corpus_no_numbers
      content(sms_corpus[[200]])
      content(sms_corpus_no_numbers[[200]])
    #+end_src

    #+RESULTS:
    : [1] "WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"
    : [1] "WELL DONE! Your * Costa Del Sol Holiday or £ await collection. Call  Now toClaim. SAE, TCs, POBox, Stockport, SKxh, Cost£./pm, Maxmins"

11) What is ~stopwords("german")~ and how can you look at its end?
    #+begin_src R
      ## stopwords("de") is character vector of small, irrelevant German
      ## words used to clean a German text.
      tail(stopwords("de"))
    #+end_src

    #+RESULTS:
    : [1] "würden"   "zu"       "zum"      "zur"      "zwar"     "zwischen"

12) Is the word "Your" in ~stopwords("en")~? How would you find out?
    #+begin_src R
      any(stopwords("en")=="your")
    #+end_src

    #+RESULTS:
    : [1] TRUE

13) Remove all English stopwords and "Your" from the SMS corpus and
    check the result for SMS message no. 200.
    #+begin_src R
      ## removing English stopwords + "Yours" from sms_corpus
      content(sms_corpus[[200]])   # original msg
      content(tm_map(sms_corpus,
                     removeWords,
                     c(stopwords("en"),"Your"))[[200]])
    #+end_src

    #+RESULTS:
    : [1] "WELL DONE! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"
    : [1] "WELL DONE!  4* Costa Del Sol Holiday  £5000 await collection. Call 09050090044 Now toClaim. SAE, TCs, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins"

14) What does "Porter's algorithm" from the ~SnowballC~ package do?
    #+begin_notes
    Answer: it breaks down all words into root + affixes using a set
    of rules. The algorithm doesn't find the roots of all words
    though! For example it does not recognize that 'learner' comes
    from 'learn'.
    #+end_notes

15) Why does the Naive Bayes algorithm benefit from a clean text corpus?
    #+begin_notes
    In Naive Bayes, probabilities are attached to every feature, and
    conditional probabilities of the predictors are computed for
    classification using Bayes' formula. For a textual dataset, the
    features/predictors are individual words - the fewer words we have
    to account for, the fewer computations need to be carried out!
    #+end_notes

16) Does the order of text mining operations matter for the final
    result? And what is the format of the final result?
   #+begin_notes
    Answer: yes, it does matter. The DocumentTermMatrix function,
    which creates the final result, a frequency table of documents
    (rows) vs. terms (columns/words) uses a different stopwords
    dictionary, and therefore we have a different number of
    features. In Naive Bayes, this is especially relevant since ALL
    information is used that enters the final probabilities
    computation.
    #+end_notes

** What did you learn this week?

Tue:
1. The chatbot narrative has an arc of 60 years (1963-2023)
2. Some think LLMs "degrade science and debase ethics"
3. People who aren't evil can do evil (Arendt's "Banality of Evil")

Thu:
1. How to save and load a complete interactive R session
2. Run an R file in batch mode (R CMD BATCH)
3. How to do text mining (review)

* TODO Week 12: Spam filter & intro to regression
#+attr_latex: :width 400px
#+caption: Photo by Jonathan Borba on Unsplash.
[[../img/regression.jpg]]

- [[https://app.datacamp.com/learn/projects/584][Guided DataCamp HR project "Degrees that pay you back"]]
- [[https://pieriantraining.com/k-means-clustering-machine-learning-in-python/][Tutorial blog post: k-means clustering in Python]] (3/17/2023)

* TODO Naive Bayes: model training and testing
#+attr_latex: :width 300px
#+caption: Testpilot Lt John A. MacReady (Source: Flickr.com/LOC)
[[../img/testing.jpg]]

- Download ~clean.R~ from GitHub (bit.ly/nb_clean)

- Run the file on the shell (~M-x eshell~) as a batch job[fn:1]:
  #+begin_src sh
    R CMD BATCH clean.R
    ls -al .RData
  #+end_src

- Load the ~.RData~ file in your current R session and check that
  packages and user-defined objects were loaded:
  #+begin_src R
    load(".RData")
    search()
    ls()
  #+end_src
- If we don't finish with a session, save your data from now on:
  #+begin_src R
    save.image(file=".RData")
    shell("ls -al .RData")
  #+end_src

- Now: continue with your practice file ~5_naive_bayes_practice.org~
  after adding the next (and final) chapters from here:
  bit.ly/nb_final

* TODO Introduction to regression with examples
#+attr_latex: :width 400px
[[../img/6_linear.png]]
[[../img/6_logistic.png]]

- DataCamp offers *14 different courses on regression techniques* in R
- The basic course is "[[https://app.datacamp.com/learn/courses/introduction-to-regression-in-r][Introduction to Regression in R]]" (w/Tidyverse)
- The best course is "[[https://app.datacamp.com/learn/courses/supervised-learning-in-r-regression][Supervised learning in R: regression]]" - the
  creators also wrote "Practical Data Science with R" (Manning, 2019).

* TODO What did you learn this week?

Tue:


Thu:


* TODO Week 13: Artifical Neural Networks (ANN)
- [[https://app.datacamp.com/learn/projects/552][DataCamp guided project "Clustering Heart Disease Patient Data"]]
** DataCamp review questions: logistic regression
#+attr_latex: :width 400px
#+caption: Source: Wikipedia ROC entry
[[../img/Roc_curve.svg]]

* TODO Week 14: Support Vector Machines (SVM)
** DataCamp review questions: k-means clustering

* TODO Week 15: Deep Learning & Projects I

** DataCamp review questions: hierarchical clustering

* TODO Week 16: Projects II and Closing
* TODO What next?

Simple: 1) Work through some good books, 2) learn Python, too, 3)
complete plenty of fun projects and document them.

- The Art of Machine Learning by Norman Matloff (NoStarch, 2023) is
  shaping up to be the best book for ML with R (and ML altogether):
  #+attr_latex: :width 200px
  #+caption: Cover, The Art of Machine Learning by N Matloff (NoStarch, 2023)
  [[../img/matloffTAOML.png]]

- Deep learning with R -  (Manning, 2022)
  #+attr_latex: :width 200px
  #+caption: Cover, Deep learning with R by Chollet/Allaire (Manning, 2022)
  [[../img/cholletallaire.png]]

- [[https://www.manning.com/books/grokking-machine-learning][Grokking machine learning (Manning, 2022)]]
  #+attr_latex: :width 200px
  #+caption: Cover, Grokking machine learning by Serrano (Manning, 2022)
  [[../img/serrano.png]]

- Learn Python (@DataCamp | CSC 109 @Lyon)
  #+attr_latex: :width 200px
  #+caption: Cover, Automate the boring stuff with Python by Sweigart (Manning, 2022)
  [[../img/python.jpg]]

* Resources

- Import the CSV data and save them to a data frame ~sms_raw~. Do not
  automatically convert ~character~ to ~factor~ vectors. Use the
  appropriate function arguments:
  #+name: load_sms_raw
  #+begin_src R :results silent
    ## save CSV data as data frame sms_raw
    sms_raw <- read.csv(file="https://bit.ly/sms_spam_csv",
                        header=TRUE,
                        stringsAsFactors=FALSE)
  #+end_src
- Three steps lead from a data frame with text to a corpus:
  1) Isolate the text vector
  2) Turn the vector into a source
  3) Turn the source into a corpus
  4) Check that the corpus is there
  #+name: create_sms_corpus
  #+begin_src R
    sms_corpus <- VCorpus(VectorSource(sms_raw$text))
  #+end_src

* References

- Gavagsaz, E (May 2022). Efficient Parallel Processing of
  k-Nearest Neighbor Queries by Using a Centroid-based and
  Hierarchical Clustering Algorithm, DOI: 10.30564/aia.v4i1.4668

- Goldin, S @freeCodeCamp.org (Sep 9, 2023). Google Like a Pro –
  All Advanced Search Operators Tutorial [2023 Tips]. Online:
  [[https://youtu.be/BRiNw490Eq0][youtube.com]].

- Lopp, S (Apr 4, 2019). R for Enterprise: Understanding R's
  Startup. In: R Views. Online: [[https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/][rviews.rstudio.com]].

- Stokes, J (Jan 4, 2023). The Fourth Age Of Programming
  [Blog]. URL: [[https://blog.replit.com/fourth][blog.repolit.com]]

- Waters, R (Feb 19, 2023). Man beats machine at Go in human
  victory over AI. URL: [[https://arstechnica.com/information-technology/2023/02/man-beats-machine-at-go-in-human-victory-over-ai/][arstechnica.com]].

- Worsley, S (Mar 2022). What is R? The Statistical Computing
  Powerhouse. [[https://www.datacamp.com/blog/all-about-r][Online: datacamp.com/blog.]]

- Ying, K @freeCodeCamp.org (Sep 26, 2022). Machine Learning for
  Everybody - Full Course. Online: [[https://youtu.be/i_LwzRVP7bg][youtube.com]].

- Zumel, N and Mount J (2010). Practical Data Science with R
  (2e). Manning. Online: [[https://www.manning.com/books/practical-data-science-with-r-second-edition][manning.com]].

- Photos by: Ben Mullins, Katarzyna Pe, Benjamin Davies, Grovemade,
  Feliphe Schiarolli, Avin CP, Andrew Neel, Jonathan Borba on
  Unsplash.com

* Footnotes

[fn:1]It is possible that the existence of an ~.RData~ file converts
warnings about packages you're trying to load into errors, which stops
the packages from being loaded. In this case /erase/ the ~.RData~ file and
restart R.
